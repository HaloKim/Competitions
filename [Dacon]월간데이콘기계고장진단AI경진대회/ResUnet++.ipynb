{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2bcba5f-002e-4f49-9622-ada6117faf0a",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b0d9b68-7102-4eca-9543-3b9b8acafc6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hist\\miniconda3\\envs\\ml\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import librosa\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba7851e-f369-4b1b-b5e3-b45233535157",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7c0d625-02ae-42bf-b87c-bc60423e2284",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1000\n",
    "LR = 1e-1\n",
    "SR = 16000\n",
    "SEED = 42\n",
    "N_MFCC = 128\n",
    "BATCH = 256\n",
    "device = 'cuda'\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(SEED) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a4172e-5791-446f-9616-35c09d8bf25a",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a62c78cd-4f40-4e98-b8a6-1b6f1d906b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'open/'\n",
    "train_df = pd.read_csv(path+'train.csv') # 모두 정상 Sample\n",
    "test_df = pd.read_csv(path+'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33f37236-257c-44ab-8a9f-b2daf6c50ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAMPLE_ID</th>\n",
       "      <th>SAMPLE_PATH</th>\n",
       "      <th>FAN_TYPE</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>./train/TRAIN_0000.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>./train/TRAIN_0001.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>./train/TRAIN_0002.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>./train/TRAIN_0003.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>./train/TRAIN_0004.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274</th>\n",
       "      <td>TRAIN_1274</td>\n",
       "      <td>./train/TRAIN_1274.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275</th>\n",
       "      <td>TRAIN_1275</td>\n",
       "      <td>./train/TRAIN_1275.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1276</th>\n",
       "      <td>TRAIN_1276</td>\n",
       "      <td>./train/TRAIN_1276.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1277</th>\n",
       "      <td>TRAIN_1277</td>\n",
       "      <td>./train/TRAIN_1277.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1278</th>\n",
       "      <td>TRAIN_1278</td>\n",
       "      <td>./train/TRAIN_1278.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1279 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       SAMPLE_ID             SAMPLE_PATH  FAN_TYPE  LABEL\n",
       "0     TRAIN_0000  ./train/TRAIN_0000.wav         2      0\n",
       "1     TRAIN_0001  ./train/TRAIN_0001.wav         0      0\n",
       "2     TRAIN_0002  ./train/TRAIN_0002.wav         0      0\n",
       "3     TRAIN_0003  ./train/TRAIN_0003.wav         2      0\n",
       "4     TRAIN_0004  ./train/TRAIN_0004.wav         2      0\n",
       "...          ...                     ...       ...    ...\n",
       "1274  TRAIN_1274  ./train/TRAIN_1274.wav         2      0\n",
       "1275  TRAIN_1275  ./train/TRAIN_1275.wav         2      0\n",
       "1276  TRAIN_1276  ./train/TRAIN_1276.wav         2      0\n",
       "1277  TRAIN_1277  ./train/TRAIN_1277.wav         2      0\n",
       "1278  TRAIN_1278  ./train/TRAIN_1278.wav         0      0\n",
       "\n",
       "[1279 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee800463-995c-43ac-a05d-f3988923add9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mfcc_feature(df):\n",
    "    features = []\n",
    "    features2 = []\n",
    "    for path in tqdm(df['SAMPLE_PATH']):\n",
    "        # librosa패키지를 사용하여 wav 파일 load\n",
    "        y, sr = librosa.load(path, sr=SR)\n",
    "        \n",
    "        # melspectrogram\n",
    "        mels = librosa.feature.melspectrogram(y, sr=sr, n_mels=N_MFCC)\n",
    "        mels = librosa.power_to_db(mels, ref=np.max)\n",
    "        \n",
    "        # librosa패키지를 사용하여 mfcc 추출\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=N_MFCC)\n",
    "        \n",
    "        y_feature2 = []\n",
    "        # 추출된 MFCC들의 평균을 Feature로 사용\n",
    "        for e in mels:\n",
    "            y_feature2.append(np.mean(e))\n",
    "        features2.append(y_feature2)\n",
    "        \n",
    "        y_feature = []\n",
    "        # 추출된 MFCC들의 평균을 Feature로 사용\n",
    "        for e in mfcc:\n",
    "            y_feature.append(np.mean(e))\n",
    "        features.append(y_feature)\n",
    "    return features, features2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5b4be22-d948-4407-afe9-cf16c4281826",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1279/1279 [00:26<00:00, 47.38it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1514/1514 [00:32<00:00, 47.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 22s\n",
      "Wall time: 59.2 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_features, train_features2 = get_mfcc_feature(train_df)\n",
    "test_features, test_features2 = get_mfcc_feature(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c86bd05-d873-4943-885c-b7efffe41048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 반복문으로 전체 컬럼명 변경하기\n",
    "def rename(df):\n",
    "    flag = 0\n",
    "    for col_name in df.columns:\n",
    "        if col_name == 0:\n",
    "            flag = 1\n",
    "        if flag == 1:\n",
    "            df.rename(columns = {col_name : 128+col_name}, inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03d6602e-38a0-4a7e-99b4-ba10b6d03d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAMPLE_ID</th>\n",
       "      <th>SAMPLE_PATH</th>\n",
       "      <th>FAN_TYPE</th>\n",
       "      <th>LABEL</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>...</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>./train/TRAIN_0000.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.952273</td>\n",
       "      <td>0.284621</td>\n",
       "      <td>0.295843</td>\n",
       "      <td>0.363586</td>\n",
       "      <td>0.357530</td>\n",
       "      <td>0.294327</td>\n",
       "      <td>...</td>\n",
       "      <td>0.833846</td>\n",
       "      <td>0.842291</td>\n",
       "      <td>0.818590</td>\n",
       "      <td>0.817896</td>\n",
       "      <td>0.799984</td>\n",
       "      <td>0.799645</td>\n",
       "      <td>0.811261</td>\n",
       "      <td>0.816584</td>\n",
       "      <td>0.825692</td>\n",
       "      <td>0.805742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>./train/TRAIN_0001.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.081981</td>\n",
       "      <td>0.935459</td>\n",
       "      <td>0.514880</td>\n",
       "      <td>0.613406</td>\n",
       "      <td>0.691659</td>\n",
       "      <td>0.825306</td>\n",
       "      <td>...</td>\n",
       "      <td>0.148846</td>\n",
       "      <td>0.144862</td>\n",
       "      <td>0.149996</td>\n",
       "      <td>0.165012</td>\n",
       "      <td>0.156853</td>\n",
       "      <td>0.160930</td>\n",
       "      <td>0.163661</td>\n",
       "      <td>0.162874</td>\n",
       "      <td>0.158788</td>\n",
       "      <td>0.084692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>./train/TRAIN_0002.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.240260</td>\n",
       "      <td>0.664368</td>\n",
       "      <td>0.724483</td>\n",
       "      <td>0.354632</td>\n",
       "      <td>0.625930</td>\n",
       "      <td>0.695975</td>\n",
       "      <td>...</td>\n",
       "      <td>0.454258</td>\n",
       "      <td>0.520886</td>\n",
       "      <td>0.454087</td>\n",
       "      <td>0.465858</td>\n",
       "      <td>0.423989</td>\n",
       "      <td>0.497721</td>\n",
       "      <td>0.504808</td>\n",
       "      <td>0.502295</td>\n",
       "      <td>0.487844</td>\n",
       "      <td>0.409240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>./train/TRAIN_0003.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.943729</td>\n",
       "      <td>0.295295</td>\n",
       "      <td>0.312391</td>\n",
       "      <td>0.371455</td>\n",
       "      <td>0.329344</td>\n",
       "      <td>0.268686</td>\n",
       "      <td>...</td>\n",
       "      <td>0.808817</td>\n",
       "      <td>0.827032</td>\n",
       "      <td>0.815166</td>\n",
       "      <td>0.787913</td>\n",
       "      <td>0.779337</td>\n",
       "      <td>0.781981</td>\n",
       "      <td>0.771998</td>\n",
       "      <td>0.779565</td>\n",
       "      <td>0.796365</td>\n",
       "      <td>0.798277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>./train/TRAIN_0004.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.949608</td>\n",
       "      <td>0.188729</td>\n",
       "      <td>0.179787</td>\n",
       "      <td>0.154144</td>\n",
       "      <td>0.007228</td>\n",
       "      <td>0.055841</td>\n",
       "      <td>...</td>\n",
       "      <td>0.877542</td>\n",
       "      <td>0.909439</td>\n",
       "      <td>0.894097</td>\n",
       "      <td>0.863824</td>\n",
       "      <td>0.849838</td>\n",
       "      <td>0.867351</td>\n",
       "      <td>0.865360</td>\n",
       "      <td>0.861068</td>\n",
       "      <td>0.868706</td>\n",
       "      <td>0.875541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274</th>\n",
       "      <td>TRAIN_1274</td>\n",
       "      <td>./train/TRAIN_1274.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.964989</td>\n",
       "      <td>0.096119</td>\n",
       "      <td>0.129592</td>\n",
       "      <td>0.222531</td>\n",
       "      <td>0.039493</td>\n",
       "      <td>0.075616</td>\n",
       "      <td>...</td>\n",
       "      <td>0.857574</td>\n",
       "      <td>0.875432</td>\n",
       "      <td>0.862135</td>\n",
       "      <td>0.853636</td>\n",
       "      <td>0.852105</td>\n",
       "      <td>0.854758</td>\n",
       "      <td>0.852087</td>\n",
       "      <td>0.851688</td>\n",
       "      <td>0.860304</td>\n",
       "      <td>0.881918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275</th>\n",
       "      <td>TRAIN_1275</td>\n",
       "      <td>./train/TRAIN_1275.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.959506</td>\n",
       "      <td>0.283203</td>\n",
       "      <td>0.293961</td>\n",
       "      <td>0.390142</td>\n",
       "      <td>0.322176</td>\n",
       "      <td>0.239990</td>\n",
       "      <td>...</td>\n",
       "      <td>0.842018</td>\n",
       "      <td>0.853391</td>\n",
       "      <td>0.822730</td>\n",
       "      <td>0.825484</td>\n",
       "      <td>0.822317</td>\n",
       "      <td>0.809976</td>\n",
       "      <td>0.820805</td>\n",
       "      <td>0.835689</td>\n",
       "      <td>0.836974</td>\n",
       "      <td>0.819587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1276</th>\n",
       "      <td>TRAIN_1276</td>\n",
       "      <td>./train/TRAIN_1276.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.930908</td>\n",
       "      <td>0.223856</td>\n",
       "      <td>0.210020</td>\n",
       "      <td>0.151861</td>\n",
       "      <td>0.134018</td>\n",
       "      <td>0.149416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.918239</td>\n",
       "      <td>0.932157</td>\n",
       "      <td>0.898576</td>\n",
       "      <td>0.905386</td>\n",
       "      <td>0.894843</td>\n",
       "      <td>0.879353</td>\n",
       "      <td>0.900221</td>\n",
       "      <td>0.910439</td>\n",
       "      <td>0.903620</td>\n",
       "      <td>0.895840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1277</th>\n",
       "      <td>TRAIN_1277</td>\n",
       "      <td>./train/TRAIN_1277.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.932890</td>\n",
       "      <td>0.247222</td>\n",
       "      <td>0.265837</td>\n",
       "      <td>0.223214</td>\n",
       "      <td>0.170546</td>\n",
       "      <td>0.141445</td>\n",
       "      <td>...</td>\n",
       "      <td>0.922436</td>\n",
       "      <td>0.946884</td>\n",
       "      <td>0.920099</td>\n",
       "      <td>0.916849</td>\n",
       "      <td>0.900980</td>\n",
       "      <td>0.903257</td>\n",
       "      <td>0.919087</td>\n",
       "      <td>0.922190</td>\n",
       "      <td>0.920950</td>\n",
       "      <td>0.932029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1278</th>\n",
       "      <td>TRAIN_1278</td>\n",
       "      <td>./train/TRAIN_1278.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.407188</td>\n",
       "      <td>0.620077</td>\n",
       "      <td>0.356668</td>\n",
       "      <td>0.769474</td>\n",
       "      <td>0.548411</td>\n",
       "      <td>0.806605</td>\n",
       "      <td>...</td>\n",
       "      <td>0.306191</td>\n",
       "      <td>0.317135</td>\n",
       "      <td>0.329551</td>\n",
       "      <td>0.308712</td>\n",
       "      <td>0.285871</td>\n",
       "      <td>0.289114</td>\n",
       "      <td>0.280076</td>\n",
       "      <td>0.270611</td>\n",
       "      <td>0.257248</td>\n",
       "      <td>0.177652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1279 rows × 260 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       SAMPLE_ID             SAMPLE_PATH  FAN_TYPE  LABEL       128       129  \\\n",
       "0     TRAIN_0000  ./train/TRAIN_0000.wav         2      0  0.952273  0.284621   \n",
       "1     TRAIN_0001  ./train/TRAIN_0001.wav         0      0  0.081981  0.935459   \n",
       "2     TRAIN_0002  ./train/TRAIN_0002.wav         0      0  0.240260  0.664368   \n",
       "3     TRAIN_0003  ./train/TRAIN_0003.wav         2      0  0.943729  0.295295   \n",
       "4     TRAIN_0004  ./train/TRAIN_0004.wav         2      0  0.949608  0.188729   \n",
       "...          ...                     ...       ...    ...       ...       ...   \n",
       "1274  TRAIN_1274  ./train/TRAIN_1274.wav         2      0  0.964989  0.096119   \n",
       "1275  TRAIN_1275  ./train/TRAIN_1275.wav         2      0  0.959506  0.283203   \n",
       "1276  TRAIN_1276  ./train/TRAIN_1276.wav         2      0  0.930908  0.223856   \n",
       "1277  TRAIN_1277  ./train/TRAIN_1277.wav         2      0  0.932890  0.247222   \n",
       "1278  TRAIN_1278  ./train/TRAIN_1278.wav         0      0  0.407188  0.620077   \n",
       "\n",
       "           130       131       132       133  ...       118       119  \\\n",
       "0     0.295843  0.363586  0.357530  0.294327  ...  0.833846  0.842291   \n",
       "1     0.514880  0.613406  0.691659  0.825306  ...  0.148846  0.144862   \n",
       "2     0.724483  0.354632  0.625930  0.695975  ...  0.454258  0.520886   \n",
       "3     0.312391  0.371455  0.329344  0.268686  ...  0.808817  0.827032   \n",
       "4     0.179787  0.154144  0.007228  0.055841  ...  0.877542  0.909439   \n",
       "...        ...       ...       ...       ...  ...       ...       ...   \n",
       "1274  0.129592  0.222531  0.039493  0.075616  ...  0.857574  0.875432   \n",
       "1275  0.293961  0.390142  0.322176  0.239990  ...  0.842018  0.853391   \n",
       "1276  0.210020  0.151861  0.134018  0.149416  ...  0.918239  0.932157   \n",
       "1277  0.265837  0.223214  0.170546  0.141445  ...  0.922436  0.946884   \n",
       "1278  0.356668  0.769474  0.548411  0.806605  ...  0.306191  0.317135   \n",
       "\n",
       "           120       121       122       123       124       125       126  \\\n",
       "0     0.818590  0.817896  0.799984  0.799645  0.811261  0.816584  0.825692   \n",
       "1     0.149996  0.165012  0.156853  0.160930  0.163661  0.162874  0.158788   \n",
       "2     0.454087  0.465858  0.423989  0.497721  0.504808  0.502295  0.487844   \n",
       "3     0.815166  0.787913  0.779337  0.781981  0.771998  0.779565  0.796365   \n",
       "4     0.894097  0.863824  0.849838  0.867351  0.865360  0.861068  0.868706   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1274  0.862135  0.853636  0.852105  0.854758  0.852087  0.851688  0.860304   \n",
       "1275  0.822730  0.825484  0.822317  0.809976  0.820805  0.835689  0.836974   \n",
       "1276  0.898576  0.905386  0.894843  0.879353  0.900221  0.910439  0.903620   \n",
       "1277  0.920099  0.916849  0.900980  0.903257  0.919087  0.922190  0.920950   \n",
       "1278  0.329551  0.308712  0.285871  0.289114  0.280076  0.270611  0.257248   \n",
       "\n",
       "           127  \n",
       "0     0.805742  \n",
       "1     0.084692  \n",
       "2     0.409240  \n",
       "3     0.798277  \n",
       "4     0.875541  \n",
       "...        ...  \n",
       "1274  0.881918  \n",
       "1275  0.819587  \n",
       "1276  0.895840  \n",
       "1277  0.932029  \n",
       "1278  0.177652  \n",
       "\n",
       "[1279 rows x 260 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = pd.concat([train_df, pd.DataFrame(train_features)], axis=1)\n",
    "tmp = rename(tmp)\n",
    "tmp = pd.concat([tmp, pd.DataFrame(train_features2)], axis=1)\n",
    "\n",
    "test = pd.concat([test_df, pd.DataFrame(test_features)], axis=1)\n",
    "test = rename(test)\n",
    "test = pd.concat([test, pd.DataFrame(test_features2)], axis=1)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "tmp.iloc[:,range(4,len(tmp.columns))] = scaler.fit_transform(tmp.iloc[:,range(4,len(tmp.columns))])\n",
    "test.iloc[:,range(3,len(test.columns))] = scaler.transform(test.iloc[:,range(3,len(test.columns))])\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09100fa5-d563-42aa-9575-1798d8ebbd40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAMPLE_ID</th>\n",
       "      <th>SAMPLE_PATH</th>\n",
       "      <th>FAN_TYPE</th>\n",
       "      <th>LABEL</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>...</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>./train/TRAIN_0000.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.952273</td>\n",
       "      <td>0.284621</td>\n",
       "      <td>0.295843</td>\n",
       "      <td>0.363586</td>\n",
       "      <td>0.357530</td>\n",
       "      <td>0.294327</td>\n",
       "      <td>...</td>\n",
       "      <td>0.833846</td>\n",
       "      <td>0.842291</td>\n",
       "      <td>0.818590</td>\n",
       "      <td>0.817896</td>\n",
       "      <td>0.799984</td>\n",
       "      <td>0.799645</td>\n",
       "      <td>0.811261</td>\n",
       "      <td>0.816584</td>\n",
       "      <td>0.825692</td>\n",
       "      <td>0.805742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>./train/TRAIN_0001.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.081981</td>\n",
       "      <td>0.935459</td>\n",
       "      <td>0.514880</td>\n",
       "      <td>0.613406</td>\n",
       "      <td>0.691659</td>\n",
       "      <td>0.825306</td>\n",
       "      <td>...</td>\n",
       "      <td>0.148846</td>\n",
       "      <td>0.144862</td>\n",
       "      <td>0.149996</td>\n",
       "      <td>0.165012</td>\n",
       "      <td>0.156853</td>\n",
       "      <td>0.160930</td>\n",
       "      <td>0.163661</td>\n",
       "      <td>0.162874</td>\n",
       "      <td>0.158788</td>\n",
       "      <td>0.084692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>./train/TRAIN_0002.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.240260</td>\n",
       "      <td>0.664368</td>\n",
       "      <td>0.724483</td>\n",
       "      <td>0.354632</td>\n",
       "      <td>0.625930</td>\n",
       "      <td>0.695975</td>\n",
       "      <td>...</td>\n",
       "      <td>0.454258</td>\n",
       "      <td>0.520886</td>\n",
       "      <td>0.454087</td>\n",
       "      <td>0.465858</td>\n",
       "      <td>0.423989</td>\n",
       "      <td>0.497721</td>\n",
       "      <td>0.504808</td>\n",
       "      <td>0.502295</td>\n",
       "      <td>0.487844</td>\n",
       "      <td>0.409240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>./train/TRAIN_0003.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.943729</td>\n",
       "      <td>0.295295</td>\n",
       "      <td>0.312391</td>\n",
       "      <td>0.371455</td>\n",
       "      <td>0.329344</td>\n",
       "      <td>0.268686</td>\n",
       "      <td>...</td>\n",
       "      <td>0.808817</td>\n",
       "      <td>0.827032</td>\n",
       "      <td>0.815166</td>\n",
       "      <td>0.787913</td>\n",
       "      <td>0.779337</td>\n",
       "      <td>0.781981</td>\n",
       "      <td>0.771998</td>\n",
       "      <td>0.779565</td>\n",
       "      <td>0.796365</td>\n",
       "      <td>0.798277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>./train/TRAIN_0004.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.949608</td>\n",
       "      <td>0.188729</td>\n",
       "      <td>0.179787</td>\n",
       "      <td>0.154144</td>\n",
       "      <td>0.007228</td>\n",
       "      <td>0.055841</td>\n",
       "      <td>...</td>\n",
       "      <td>0.877542</td>\n",
       "      <td>0.909439</td>\n",
       "      <td>0.894097</td>\n",
       "      <td>0.863824</td>\n",
       "      <td>0.849838</td>\n",
       "      <td>0.867351</td>\n",
       "      <td>0.865360</td>\n",
       "      <td>0.861068</td>\n",
       "      <td>0.868706</td>\n",
       "      <td>0.875541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274</th>\n",
       "      <td>TRAIN_1274</td>\n",
       "      <td>./train/TRAIN_1274.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.964989</td>\n",
       "      <td>0.096119</td>\n",
       "      <td>0.129592</td>\n",
       "      <td>0.222531</td>\n",
       "      <td>0.039493</td>\n",
       "      <td>0.075616</td>\n",
       "      <td>...</td>\n",
       "      <td>0.857574</td>\n",
       "      <td>0.875432</td>\n",
       "      <td>0.862135</td>\n",
       "      <td>0.853636</td>\n",
       "      <td>0.852105</td>\n",
       "      <td>0.854758</td>\n",
       "      <td>0.852087</td>\n",
       "      <td>0.851688</td>\n",
       "      <td>0.860304</td>\n",
       "      <td>0.881918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275</th>\n",
       "      <td>TRAIN_1275</td>\n",
       "      <td>./train/TRAIN_1275.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.959506</td>\n",
       "      <td>0.283203</td>\n",
       "      <td>0.293961</td>\n",
       "      <td>0.390142</td>\n",
       "      <td>0.322176</td>\n",
       "      <td>0.239990</td>\n",
       "      <td>...</td>\n",
       "      <td>0.842018</td>\n",
       "      <td>0.853391</td>\n",
       "      <td>0.822730</td>\n",
       "      <td>0.825484</td>\n",
       "      <td>0.822317</td>\n",
       "      <td>0.809976</td>\n",
       "      <td>0.820805</td>\n",
       "      <td>0.835689</td>\n",
       "      <td>0.836974</td>\n",
       "      <td>0.819587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1276</th>\n",
       "      <td>TRAIN_1276</td>\n",
       "      <td>./train/TRAIN_1276.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.930908</td>\n",
       "      <td>0.223856</td>\n",
       "      <td>0.210020</td>\n",
       "      <td>0.151861</td>\n",
       "      <td>0.134018</td>\n",
       "      <td>0.149416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.918239</td>\n",
       "      <td>0.932157</td>\n",
       "      <td>0.898576</td>\n",
       "      <td>0.905386</td>\n",
       "      <td>0.894843</td>\n",
       "      <td>0.879353</td>\n",
       "      <td>0.900221</td>\n",
       "      <td>0.910439</td>\n",
       "      <td>0.903620</td>\n",
       "      <td>0.895840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1277</th>\n",
       "      <td>TRAIN_1277</td>\n",
       "      <td>./train/TRAIN_1277.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.932890</td>\n",
       "      <td>0.247222</td>\n",
       "      <td>0.265837</td>\n",
       "      <td>0.223214</td>\n",
       "      <td>0.170546</td>\n",
       "      <td>0.141445</td>\n",
       "      <td>...</td>\n",
       "      <td>0.922436</td>\n",
       "      <td>0.946884</td>\n",
       "      <td>0.920099</td>\n",
       "      <td>0.916849</td>\n",
       "      <td>0.900980</td>\n",
       "      <td>0.903257</td>\n",
       "      <td>0.919087</td>\n",
       "      <td>0.922190</td>\n",
       "      <td>0.920950</td>\n",
       "      <td>0.932029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1278</th>\n",
       "      <td>TRAIN_1278</td>\n",
       "      <td>./train/TRAIN_1278.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.407188</td>\n",
       "      <td>0.620077</td>\n",
       "      <td>0.356668</td>\n",
       "      <td>0.769474</td>\n",
       "      <td>0.548411</td>\n",
       "      <td>0.806605</td>\n",
       "      <td>...</td>\n",
       "      <td>0.306191</td>\n",
       "      <td>0.317135</td>\n",
       "      <td>0.329551</td>\n",
       "      <td>0.308712</td>\n",
       "      <td>0.285871</td>\n",
       "      <td>0.289114</td>\n",
       "      <td>0.280076</td>\n",
       "      <td>0.270611</td>\n",
       "      <td>0.257248</td>\n",
       "      <td>0.177652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1279 rows × 260 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       SAMPLE_ID             SAMPLE_PATH  FAN_TYPE  LABEL       128       129  \\\n",
       "0     TRAIN_0000  ./train/TRAIN_0000.wav         2      0  0.952273  0.284621   \n",
       "1     TRAIN_0001  ./train/TRAIN_0001.wav         0      0  0.081981  0.935459   \n",
       "2     TRAIN_0002  ./train/TRAIN_0002.wav         0      0  0.240260  0.664368   \n",
       "3     TRAIN_0003  ./train/TRAIN_0003.wav         2      0  0.943729  0.295295   \n",
       "4     TRAIN_0004  ./train/TRAIN_0004.wav         2      0  0.949608  0.188729   \n",
       "...          ...                     ...       ...    ...       ...       ...   \n",
       "1274  TRAIN_1274  ./train/TRAIN_1274.wav         2      0  0.964989  0.096119   \n",
       "1275  TRAIN_1275  ./train/TRAIN_1275.wav         2      0  0.959506  0.283203   \n",
       "1276  TRAIN_1276  ./train/TRAIN_1276.wav         2      0  0.930908  0.223856   \n",
       "1277  TRAIN_1277  ./train/TRAIN_1277.wav         2      0  0.932890  0.247222   \n",
       "1278  TRAIN_1278  ./train/TRAIN_1278.wav         0      0  0.407188  0.620077   \n",
       "\n",
       "           130       131       132       133  ...       118       119  \\\n",
       "0     0.295843  0.363586  0.357530  0.294327  ...  0.833846  0.842291   \n",
       "1     0.514880  0.613406  0.691659  0.825306  ...  0.148846  0.144862   \n",
       "2     0.724483  0.354632  0.625930  0.695975  ...  0.454258  0.520886   \n",
       "3     0.312391  0.371455  0.329344  0.268686  ...  0.808817  0.827032   \n",
       "4     0.179787  0.154144  0.007228  0.055841  ...  0.877542  0.909439   \n",
       "...        ...       ...       ...       ...  ...       ...       ...   \n",
       "1274  0.129592  0.222531  0.039493  0.075616  ...  0.857574  0.875432   \n",
       "1275  0.293961  0.390142  0.322176  0.239990  ...  0.842018  0.853391   \n",
       "1276  0.210020  0.151861  0.134018  0.149416  ...  0.918239  0.932157   \n",
       "1277  0.265837  0.223214  0.170546  0.141445  ...  0.922436  0.946884   \n",
       "1278  0.356668  0.769474  0.548411  0.806605  ...  0.306191  0.317135   \n",
       "\n",
       "           120       121       122       123       124       125       126  \\\n",
       "0     0.818590  0.817896  0.799984  0.799645  0.811261  0.816584  0.825692   \n",
       "1     0.149996  0.165012  0.156853  0.160930  0.163661  0.162874  0.158788   \n",
       "2     0.454087  0.465858  0.423989  0.497721  0.504808  0.502295  0.487844   \n",
       "3     0.815166  0.787913  0.779337  0.781981  0.771998  0.779565  0.796365   \n",
       "4     0.894097  0.863824  0.849838  0.867351  0.865360  0.861068  0.868706   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1274  0.862135  0.853636  0.852105  0.854758  0.852087  0.851688  0.860304   \n",
       "1275  0.822730  0.825484  0.822317  0.809976  0.820805  0.835689  0.836974   \n",
       "1276  0.898576  0.905386  0.894843  0.879353  0.900221  0.910439  0.903620   \n",
       "1277  0.920099  0.916849  0.900980  0.903257  0.919087  0.922190  0.920950   \n",
       "1278  0.329551  0.308712  0.285871  0.289114  0.280076  0.270611  0.257248   \n",
       "\n",
       "           127  \n",
       "0     0.805742  \n",
       "1     0.084692  \n",
       "2     0.409240  \n",
       "3     0.798277  \n",
       "4     0.875541  \n",
       "...        ...  \n",
       "1274  0.881918  \n",
       "1275  0.819587  \n",
       "1276  0.895840  \n",
       "1277  0.932029  \n",
       "1278  0.177652  \n",
       "\n",
       "[1279 rows x 260 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a8fecd3-b50a-4c2a-9cb2-301a69345620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['FAN_TYPE',        128,        129,        130,        131,        132,\n",
       "              133,        134,        135,        136,\n",
       "       ...\n",
       "              118,        119,        120,        121,        122,        123,\n",
       "              124,        125,        126,        127],\n",
       "      dtype='object', length=257)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = tmp.columns.drop(['SAMPLE_ID', 'SAMPLE_PATH', 'LABEL'])\n",
    "cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e967190-c87e-458a-b9cb-4399574fa696",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0bdc4b0-54e4-45c1-8bca-57c252dc591c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, eval_mode):\n",
    "        self.eval_mode = eval_mode\n",
    "        if self.eval_mode:\n",
    "            self.labels = df['LABEL'].values\n",
    "        self.df = df[cols].values\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if self.eval_mode:\n",
    "            self.x = self.df[index]\n",
    "            self.y = self.labels[index]\n",
    "            return torch.Tensor(self.x).reshape(-1,1), self.y\n",
    "        else:\n",
    "            self.x = self.df[index]\n",
    "            return torch.Tensor(self.x).reshape(-1,1)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2afb059-203b-4ac7-aaec-79a0380abadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, tolerance=5, min_delta=0):\n",
    "        self.tolerance = tolerance\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.min_loss = np.inf\n",
    "\n",
    "    def __call__(self, train_loss, validation_loss=None):\n",
    "        if train_loss < self.min_loss:\n",
    "            self.counter = 0\n",
    "            self.min_loss = train_loss\n",
    "            print(f'counter : set 0 min loss : {self.min_loss}')\n",
    "        elif train_loss > self.min_loss:\n",
    "            self.counter += 1\n",
    "            print(f'counter : {self.counter}')\n",
    "        if self.counter >= self.tolerance:  \n",
    "            self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20032097-fc78-4ae7-bf1a-bcd831ecca78",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(tmp, test_size=0.1, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d381b860-662d-4cc1-a144-5410e9554877",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(df=train_df, eval_mode=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH, shuffle=True)\n",
    "\n",
    "val_dataset = CustomDataset(df = val_df, eval_mode=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b146980-5d90-4639-ac2c-df6ea0e4fd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, model, optimizer, train_loader, val_loader, scheduler, device):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.scheduler = scheduler\n",
    "        self.device = device\n",
    "        self.early_stopping = EarlyStopping(tolerance=100, min_delta=10)\n",
    "        \n",
    "        # Loss Function\n",
    "        self.criterion = nn.KLDivLoss(reduction='batchmean', log_target=True).to(self.device)\n",
    "        # self.criterion = nn.L1Loss().to(self.device)\n",
    "        \n",
    "    def fit(self):\n",
    "        self.model.to(self.device)\n",
    "        best_score = float('-inf')\n",
    "        for epoch in range(EPOCHS):\n",
    "            self.model.train()\n",
    "            train_loss = []\n",
    "            for x in iter(self.train_loader):\n",
    "                self.optimizer.zero_grad()\n",
    "                x = x.float().to(self.device)\n",
    "                _x = self.model(x)\n",
    "\n",
    "                log_target = F.log_softmax(_x, dim=1)\n",
    "                log_input = F.log_softmax(x.reshape(-1,257), dim=1)\n",
    "                \n",
    "                loss = self.criterion(log_input, log_target)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                train_loss.append(loss.item())\n",
    "            \n",
    "            score = self.validation(self.model)\n",
    "            print(f'Epoch : [{epoch}] Train loss : [{np.mean(train_loss)}] Val Score : [{score}])')\n",
    "            \n",
    "            if best_score < score:\n",
    "                best_score = score\n",
    "                torch.save(model.state_dict(), './best_model.pth', _use_new_zipfile_serialization=False)\n",
    "            # torch.save(model.state_dict(), './model.pth', _use_new_zipfile_serialization=False)\n",
    "            # early stopping\n",
    "            self.early_stopping(np.mean(train_loss))\n",
    "            if self.early_stopping.early_stop:\n",
    "                print(\"early_stopping:\", epoch)\n",
    "                break\n",
    "            \n",
    "            if self.scheduler is not None:\n",
    "                self.scheduler.step(score)\n",
    "                            \n",
    "    def validation(self, eval_model, thr=0.999):\n",
    "        cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        eval_model.eval()\n",
    "        pred = []\n",
    "        true = []\n",
    "        with torch.no_grad():\n",
    "            for x, y in iter(self.val_loader):\n",
    "                x = x.float().to(self.device)\n",
    "                _x = self.model(x)\n",
    "                \n",
    "                log_target = F.log_softmax(_x, dim=1)\n",
    "                log_input = F.log_softmax(x.reshape(-1,257), dim=1)\n",
    "                \n",
    "                diff = cos(log_input, log_target).cpu().tolist()\n",
    "                # print(diff)\n",
    "                batch_pred = np.where(np.array(diff) > thr, 0, 1).tolist()\n",
    "\n",
    "                pred += batch_pred\n",
    "                true += y.tolist()\n",
    "\n",
    "        return f1_score(true, pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "638207fa-027e-4b11-ab15-38ea0a9222df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResConv, self).__init__()\n",
    "\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = in_channels, out_channels = out_channels, \n",
    "                                 kernel_size = 1, stride = 1),\n",
    "            nn.BatchNorm1d(num_features = out_channels),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "        self.conv_skip = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = in_channels, out_channels = out_channels, \n",
    "                                 kernel_size = 1, stride = 1),\n",
    "            nn.BatchNorm1d(num_features = out_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_block(x) + self.conv_skip(x)\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Upsample, self).__init__()\n",
    "\n",
    "        self.upsample = nn.ConvTranspose1d(\n",
    "            in_channels, out_channels, kernel_size=1, stride=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.upsample(x)\n",
    "    \n",
    "class Squeeze_Excite_Block(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(Squeeze_Excite_Block, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "class ASPP(nn.Module):\n",
    "    def __init__(self, in_dims, out_dims, rate=[6, 12, 18]):\n",
    "        super(ASPP, self).__init__()\n",
    "\n",
    "        self.aspp_block1 = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_dims, out_dims, kernel_size = 1, stride = 1\n",
    "            ),\n",
    "            nn.GELU(),\n",
    "            nn.BatchNorm1d(out_dims),\n",
    "        )\n",
    "        self.aspp_block2 = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_dims, out_dims, kernel_size = 1, stride = 1\n",
    "            ),\n",
    "            nn.GELU(),\n",
    "            nn.BatchNorm1d(out_dims),\n",
    "        )\n",
    "        self.aspp_block3 = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_dims, out_dims, kernel_size = 1, stride = 1\n",
    "            ),\n",
    "            nn.GELU(),\n",
    "            nn.BatchNorm1d(out_dims),\n",
    "        )\n",
    "\n",
    "        self.output = nn.Conv1d(len(rate) * out_dims, out_dims, 1)\n",
    "        self._init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.aspp_block1(x)\n",
    "        x2 = self.aspp_block2(x)\n",
    "        x3 = self.aspp_block3(x)\n",
    "        out = torch.cat([x1, x2, x3], dim=1)\n",
    "        return self.output(out)\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "class Upsample_(nn.Module):\n",
    "    def __init__(self, scale=2):\n",
    "        super(Upsample_, self).__init__()\n",
    "\n",
    "        self.upsample = nn.Upsample(mode=\"bilinear\", scale_factor=scale)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.upsample(x)\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, input_encoder, input_decoder, output_dim):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "\n",
    "        self.conv_encoder = nn.Sequential(\n",
    "            nn.BatchNorm1d(input_encoder),\n",
    "            nn.GELU(),\n",
    "            nn.Conv1d(input_encoder, output_dim, 3, padding=1),\n",
    "            nn.MaxPool1d(1, 1),\n",
    "        )\n",
    "\n",
    "        self.conv_decoder = nn.Sequential(\n",
    "            nn.BatchNorm1d(input_decoder),\n",
    "            nn.GELU(),\n",
    "            nn.Conv1d(input_decoder, output_dim, 3, padding=1),\n",
    "        )\n",
    "\n",
    "        self.conv_attn = nn.Sequential(\n",
    "            nn.BatchNorm1d(output_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(output_dim, 1, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        out = self.conv_encoder(x1) + self.conv_decoder(x2)\n",
    "        out = self.conv_attn(out)\n",
    "        return out * x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74688a39-2c30-44e2-ad64-498bbf1e5317",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.input_layer = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 257, out_channels = 512, \n",
    "                                 kernel_size = 1, stride = 1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.GELU(),\n",
    "            nn.Conv1d(in_channels = 512, out_channels = 512, \n",
    "                                 kernel_size = 1, stride = 1),\n",
    "        )\n",
    "        \n",
    "        self.input_skip = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 257, out_channels = 512, \n",
    "                                 kernel_size = 1, stride = 1)\n",
    "        )\n",
    "        self.squeeze_excite1 = Squeeze_Excite_Block(512)\n",
    "\n",
    "        self.residual_conv1 = ResConv(512, 1024)\n",
    "\n",
    "        self.squeeze_excite2 = Squeeze_Excite_Block(1024)\n",
    "\n",
    "        self.residual_conv2 = ResConv(1024, 2048)\n",
    "\n",
    "        self.squeeze_excite3 = Squeeze_Excite_Block(2048)\n",
    "\n",
    "        self.residual_conv3 = ResConv(2048, 4096)\n",
    "\n",
    "        self.aspp_bridge = ASPP(4096, 4096)\n",
    "\n",
    "        self.attn1 = AttentionBlock(2048, 4096, 4096)\n",
    "        self.upsample1 = Upsample_(1)\n",
    "        self.up_residual_conv1 = ResConv(4096 + 2048, 2048)\n",
    "\n",
    "        self.attn2 = AttentionBlock(1024, 2048, 2048)\n",
    "        self.upsample2 = Upsample_(1)\n",
    "        self.up_residual_conv2 = ResConv(2048 + 1024, 2048)\n",
    "\n",
    "        self.attn3 = AttentionBlock(512, 2048, 2048)\n",
    "        self.upsample3 = Upsample_(1)\n",
    "        self.up_residual_conv3 = ResConv(2048 + 512, 1024)\n",
    "\n",
    "        self.aspp_out = ASPP(2048, 512)\n",
    "\n",
    "        self.flat = nn.Flatten()\n",
    "        self.fc = nn.Conv1d(in_channels = 512, out_channels = 257, kernel_size = 1, stride = 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encode\n",
    "        x1 = self.input_layer(x) + self.input_skip(x)\n",
    "\n",
    "        # x2 = self.squeeze_excite1(x1)\n",
    "        x2 = self.residual_conv1(x1)\n",
    "\n",
    "        # x3 = self.squeeze_excite2(x2)\n",
    "        x3 = self.residual_conv2(x2)\n",
    "\n",
    "        # x4 = self.squeeze_excite3(x3)\n",
    "        x4 = self.residual_conv3(x3)\n",
    "        x5 = self.aspp_bridge(x4)\n",
    "        \n",
    "        x6 = self.attn1(x3, x5)\n",
    "        # x6 = self.upsample1(x6)\n",
    "        x6 = torch.cat([x6, x3], dim=1)\n",
    "        x6 = self.up_residual_conv1(x6)\n",
    "        \n",
    "        x7 = self.attn2(x2, x6)\n",
    "        # x7 = self.upsample2(x7)\n",
    "        x7 = torch.cat([x7, x2], dim=1)\n",
    "        x7 = self.up_residual_conv2(x7)\n",
    "        \n",
    "        # x8 = self.attn3(x1, x7)\n",
    "        # # x8 = self.upsample3(x8)\n",
    "        # x8 = torch.cat([x8, x1], dim=1)\n",
    "        # x8 = self.up_residual_conv3(x8)\n",
    "\n",
    "        x9 = self.aspp_out(x7)\n",
    "        output = self.fc(x9)\n",
    "        return self.flat(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c4ed34d-cf4f-46ac-a7dc-e0af29698759",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [0] Train loss : [0.06406954005360603] Val Score : [0.4410480349344978])\n",
      "counter : set 0 min loss : 0.06406954005360603\n",
      "Epoch : [1] Train loss : [0.05592035204172134] Val Score : [0.45064377682403434])\n",
      "counter : set 0 min loss : 0.05592035204172134\n",
      "Epoch : [2] Train loss : [0.047380104660987854] Val Score : [0.46218487394957986])\n",
      "counter : set 0 min loss : 0.047380104660987854\n",
      "Epoch : [3] Train loss : [0.04523633345961571] Val Score : [0.4796747967479675])\n",
      "counter : set 0 min loss : 0.04523633345961571\n",
      "Epoch : [4] Train loss : [0.043872532248497007] Val Score : [0.4817813765182186])\n",
      "counter : set 0 min loss : 0.043872532248497007\n",
      "Epoch : [5] Train loss : [0.03346921056509018] Val Score : [0.4980392156862745])\n",
      "counter : set 0 min loss : 0.03346921056509018\n",
      "Epoch : [6] Train loss : [0.032176388427615166] Val Score : [1.0])\n",
      "counter : set 0 min loss : 0.032176388427615166\n",
      "Epoch : [7] Train loss : [0.025649922713637353] Val Score : [1.0])\n",
      "counter : set 0 min loss : 0.025649922713637353\n",
      "Epoch : [8] Train loss : [0.026059689372777937] Val Score : [1.0])\n",
      "counter : 1\n",
      "Epoch : [9] Train loss : [0.018618908524513245] Val Score : [0.49407114624505927])\n",
      "counter : set 0 min loss : 0.018618908524513245\n",
      "Epoch : [10] Train loss : [0.020378730818629266] Val Score : [0.49407114624505927])\n",
      "counter : 1\n",
      "Epoch : [11] Train loss : [0.014156931638717651] Val Score : [0.49407114624505927])\n",
      "counter : set 0 min loss : 0.014156931638717651\n",
      "Epoch : [12] Train loss : [0.01179269291460514] Val Score : [0.49407114624505927])\n",
      "counter : set 0 min loss : 0.01179269291460514\n",
      "Epoch : [13] Train loss : [0.010648105293512344] Val Score : [0.49407114624505927])\n",
      "counter : set 0 min loss : 0.010648105293512344\n",
      "Epoch : [14] Train loss : [0.009688622504472732] Val Score : [0.49206349206349204])\n",
      "counter : set 0 min loss : 0.009688622504472732\n",
      "Epoch : [15] Train loss : [0.010251574777066707] Val Score : [0.49206349206349204])\n",
      "counter : 1\n",
      "Epoch : [16] Train loss : [0.009032742865383626] Val Score : [0.49407114624505927])\n",
      "counter : set 0 min loss : 0.009032742865383626\n",
      "Epoch : [17] Train loss : [0.008645924180746079] Val Score : [0.49407114624505927])\n",
      "counter : set 0 min loss : 0.008645924180746079\n",
      "Epoch 00018: reducing learning rate of group 0 to 5.0000e-02.\n",
      "Epoch : [18] Train loss : [0.011233981512486935] Val Score : [0.49407114624505927])\n",
      "counter : 1\n",
      "Epoch : [19] Train loss : [0.008240446820855141] Val Score : [0.49407114624505927])\n",
      "counter : set 0 min loss : 0.008240446820855141\n",
      "Epoch : [20] Train loss : [0.008271874114871024] Val Score : [0.49407114624505927])\n",
      "counter : 1\n",
      "Epoch : [21] Train loss : [0.008003797475248575] Val Score : [0.49407114624505927])\n",
      "counter : set 0 min loss : 0.008003797475248575\n",
      "Epoch : [22] Train loss : [0.00825897790491581] Val Score : [0.49407114624505927])\n",
      "counter : 1\n",
      "Epoch : [23] Train loss : [0.00804339637979865] Val Score : [0.49407114624505927])\n",
      "counter : 2\n",
      "Epoch : [24] Train loss : [0.007619321811944246] Val Score : [0.49407114624505927])\n",
      "counter : set 0 min loss : 0.007619321811944246\n",
      "Epoch : [25] Train loss : [0.009129379875957966] Val Score : [0.49407114624505927])\n",
      "counter : 1\n",
      "Epoch : [26] Train loss : [0.008148565329611302] Val Score : [0.49407114624505927])\n",
      "counter : 2\n",
      "Epoch : [27] Train loss : [0.007634114660322666] Val Score : [0.49407114624505927])\n",
      "counter : 3\n",
      "Epoch : [28] Train loss : [0.00756367789581418] Val Score : [0.49606299212598426])\n",
      "counter : set 0 min loss : 0.00756367789581418\n",
      "Epoch 00029: reducing learning rate of group 0 to 2.5000e-02.\n",
      "Epoch : [29] Train loss : [0.0075536355376243595] Val Score : [0.49606299212598426])\n",
      "counter : set 0 min loss : 0.0075536355376243595\n",
      "Epoch : [30] Train loss : [0.009346811845898629] Val Score : [0.49407114624505927])\n",
      "counter : 1\n",
      "Epoch : [31] Train loss : [0.007223120797425509] Val Score : [0.49407114624505927])\n",
      "counter : set 0 min loss : 0.007223120797425509\n",
      "Epoch : [32] Train loss : [0.007239349186420441] Val Score : [0.49606299212598426])\n",
      "counter : 1\n",
      "Epoch : [33] Train loss : [0.00738304341211915] Val Score : [0.49606299212598426])\n",
      "counter : 2\n",
      "Epoch : [34] Train loss : [0.007441646419465542] Val Score : [0.49606299212598426])\n",
      "counter : 3\n",
      "Epoch : [35] Train loss : [0.007371839880943298] Val Score : [0.4980392156862745])\n",
      "counter : 4\n",
      "Epoch : [36] Train loss : [0.007064930163323879] Val Score : [0.4980392156862745])\n",
      "counter : set 0 min loss : 0.007064930163323879\n",
      "Epoch : [37] Train loss : [0.008588814083486796] Val Score : [0.49407114624505927])\n",
      "counter : 1\n",
      "Epoch : [38] Train loss : [0.00717695513740182] Val Score : [0.4980392156862745])\n",
      "counter : 2\n",
      "Epoch : [39] Train loss : [0.007070479728281498] Val Score : [0.4980392156862745])\n",
      "counter : 3\n",
      "Epoch 00040: reducing learning rate of group 0 to 1.2500e-02.\n",
      "Epoch : [40] Train loss : [0.007177530135959387] Val Score : [0.4980392156862745])\n",
      "counter : 4\n",
      "Epoch : [41] Train loss : [0.00705450726673007] Val Score : [0.4980392156862745])\n",
      "counter : set 0 min loss : 0.00705450726673007\n",
      "Epoch : [42] Train loss : [0.006951445061713457] Val Score : [0.4980392156862745])\n",
      "counter : set 0 min loss : 0.006951445061713457\n",
      "Epoch : [43] Train loss : [0.006955065857619047] Val Score : [0.4980392156862745])\n",
      "counter : 1\n",
      "Epoch : [44] Train loss : [0.008944181818515062] Val Score : [0.49407114624505927])\n",
      "counter : 2\n",
      "Epoch : [45] Train loss : [0.007006011810153723] Val Score : [0.49407114624505927])\n",
      "counter : 3\n",
      "Epoch : [46] Train loss : [0.007963519915938378] Val Score : [0.49407114624505927])\n",
      "counter : 4\n",
      "Epoch : [47] Train loss : [0.007126664184033871] Val Score : [0.4980392156862745])\n",
      "counter : 5\n",
      "Epoch : [48] Train loss : [0.006993067637085915] Val Score : [0.4980392156862745])\n",
      "counter : 6\n",
      "Epoch : [49] Train loss : [0.006959841679781675] Val Score : [0.4980392156862745])\n",
      "counter : 7\n",
      "Epoch : [50] Train loss : [0.008211072534322739] Val Score : [0.49606299212598426])\n",
      "counter : 8\n",
      "Epoch 00051: reducing learning rate of group 0 to 6.2500e-03.\n",
      "Epoch : [51] Train loss : [0.0068102614022791386] Val Score : [0.4980392156862745])\n",
      "counter : set 0 min loss : 0.0068102614022791386\n",
      "Epoch : [52] Train loss : [0.007065331283956766] Val Score : [0.4980392156862745])\n",
      "counter : 1\n",
      "Epoch : [53] Train loss : [0.007104447670280933] Val Score : [0.4980392156862745])\n",
      "counter : 2\n",
      "Epoch : [54] Train loss : [0.007516828272491694] Val Score : [0.49407114624505927])\n",
      "counter : 3\n",
      "Epoch : [55] Train loss : [0.006819158885627985] Val Score : [0.4980392156862745])\n",
      "counter : 4\n",
      "Epoch : [56] Train loss : [0.007787153031677008] Val Score : [0.49407114624505927])\n",
      "counter : 5\n",
      "Epoch : [57] Train loss : [0.006834001932293177] Val Score : [0.49407114624505927])\n",
      "counter : 6\n",
      "Epoch : [58] Train loss : [0.006733710411936045] Val Score : [0.4980392156862745])\n",
      "counter : set 0 min loss : 0.006733710411936045\n",
      "Epoch : [59] Train loss : [0.00686580240726471] Val Score : [0.4980392156862745])\n",
      "counter : 1\n",
      "Epoch : [60] Train loss : [0.007027304265648127] Val Score : [0.4980392156862745])\n",
      "counter : 2\n",
      "Epoch : [61] Train loss : [0.006837980449199676] Val Score : [0.4980392156862745])\n",
      "counter : 3\n",
      "Epoch 00062: reducing learning rate of group 0 to 3.1250e-03.\n",
      "Epoch : [62] Train loss : [0.0069845541380345825] Val Score : [0.4980392156862745])\n",
      "counter : 4\n",
      "Epoch : [63] Train loss : [0.006784800998866558] Val Score : [0.4980392156862745])\n",
      "counter : 5\n",
      "Epoch : [64] Train loss : [0.006794637255370617] Val Score : [0.4980392156862745])\n",
      "counter : 6\n",
      "Epoch : [65] Train loss : [0.00682688932865858] Val Score : [1.0])\n",
      "counter : 7\n",
      "Epoch : [66] Train loss : [0.00900740185752511] Val Score : [0.49606299212598426])\n",
      "counter : 8\n",
      "Epoch : [67] Train loss : [0.007430907338857651] Val Score : [0.49606299212598426])\n",
      "counter : 9\n",
      "Epoch : [68] Train loss : [0.0071097870357334616] Val Score : [0.4980392156862745])\n",
      "counter : 10\n",
      "Epoch : [69] Train loss : [0.006732430122792721] Val Score : [0.4980392156862745])\n",
      "counter : set 0 min loss : 0.006732430122792721\n",
      "Epoch : [70] Train loss : [0.006997609511017799] Val Score : [1.0])\n",
      "counter : 1\n",
      "Epoch : [71] Train loss : [0.006894244998693466] Val Score : [1.0])\n",
      "counter : 2\n",
      "Epoch : [72] Train loss : [0.006700189225375653] Val Score : [1.0])\n",
      "counter : set 0 min loss : 0.006700189225375653\n",
      "Epoch 00073: reducing learning rate of group 0 to 1.5625e-03.\n",
      "Epoch : [73] Train loss : [0.006796479597687722] Val Score : [1.0])\n",
      "counter : 1\n",
      "Epoch : [74] Train loss : [0.006980734318494797] Val Score : [1.0])\n",
      "counter : 2\n",
      "Epoch : [75] Train loss : [0.006818267237395048] Val Score : [0.4980392156862745])\n",
      "counter : 3\n",
      "Epoch : [76] Train loss : [0.006797819770872593] Val Score : [0.4980392156862745])\n",
      "counter : 4\n",
      "Epoch : [77] Train loss : [0.006920494977384806] Val Score : [0.4980392156862745])\n",
      "counter : 5\n",
      "Epoch : [78] Train loss : [0.007073410600423813] Val Score : [1.0])\n",
      "counter : 6\n",
      "Epoch : [79] Train loss : [0.007118295319378376] Val Score : [0.4980392156862745])\n",
      "counter : 7\n",
      "Epoch : [80] Train loss : [0.006863692123442888] Val Score : [0.4980392156862745])\n",
      "counter : 8\n",
      "Epoch : [81] Train loss : [0.00671567814424634] Val Score : [0.4980392156862745])\n",
      "counter : 9\n",
      "Epoch : [82] Train loss : [0.008531185518950223] Val Score : [0.49407114624505927])\n",
      "counter : 10\n",
      "Epoch : [83] Train loss : [0.0068123298697173595] Val Score : [0.4980392156862745])\n",
      "counter : 11\n",
      "Epoch 00084: reducing learning rate of group 0 to 7.8125e-04.\n",
      "Epoch : [84] Train loss : [0.006812912505120039] Val Score : [0.4980392156862745])\n",
      "counter : 12\n",
      "Epoch : [85] Train loss : [0.0076763717457652095] Val Score : [0.49407114624505927])\n",
      "counter : 13\n",
      "Epoch : [86] Train loss : [0.007529692351818084] Val Score : [0.49606299212598426])\n",
      "counter : 14\n",
      "Epoch : [87] Train loss : [0.00692750122398138] Val Score : [0.4980392156862745])\n",
      "counter : 15\n",
      "Epoch : [88] Train loss : [0.006869125738739967] Val Score : [0.4980392156862745])\n",
      "counter : 16\n",
      "Epoch : [89] Train loss : [0.0067504940554499624] Val Score : [0.4980392156862745])\n",
      "counter : 17\n",
      "Epoch : [90] Train loss : [0.008964713010936976] Val Score : [0.49606299212598426])\n",
      "counter : 18\n",
      "Epoch : [91] Train loss : [0.006894085928797722] Val Score : [0.49606299212598426])\n",
      "counter : 19\n",
      "Epoch : [92] Train loss : [0.007102502882480622] Val Score : [0.4980392156862745])\n",
      "counter : 20\n",
      "Epoch : [93] Train loss : [0.006663644220679998] Val Score : [0.4980392156862745])\n",
      "counter : set 0 min loss : 0.006663644220679998\n",
      "Epoch : [94] Train loss : [0.006699061859399081] Val Score : [0.4980392156862745])\n",
      "counter : 1\n",
      "Epoch 00095: reducing learning rate of group 0 to 3.9063e-04.\n",
      "Epoch : [95] Train loss : [0.006702828407287598] Val Score : [0.4980392156862745])\n",
      "counter : 2\n",
      "Epoch : [96] Train loss : [0.006732844095677137] Val Score : [0.4980392156862745])\n",
      "counter : 3\n",
      "Epoch : [97] Train loss : [0.006722324062138796] Val Score : [0.4980392156862745])\n",
      "counter : 4\n",
      "Epoch : [98] Train loss : [0.006673192791640759] Val Score : [0.4980392156862745])\n",
      "counter : 5\n",
      "Epoch : [99] Train loss : [0.008124894183129071] Val Score : [0.49407114624505927])\n",
      "counter : 6\n",
      "Epoch : [100] Train loss : [0.006870855297893286] Val Score : [0.4980392156862745])\n",
      "counter : 7\n",
      "Epoch : [101] Train loss : [0.006816031318157912] Val Score : [0.4980392156862745])\n",
      "counter : 8\n",
      "Epoch : [102] Train loss : [0.006761224288493395] Val Score : [0.4980392156862745])\n",
      "counter : 9\n",
      "Epoch : [103] Train loss : [0.006604235526174307] Val Score : [0.4980392156862745])\n",
      "counter : set 0 min loss : 0.006604235526174307\n",
      "Epoch : [104] Train loss : [0.006684065330773592] Val Score : [0.4980392156862745])\n",
      "counter : 1\n",
      "Epoch : [105] Train loss : [0.007028154470026493] Val Score : [0.4980392156862745])\n",
      "counter : 2\n",
      "Epoch 00106: reducing learning rate of group 0 to 1.9531e-04.\n",
      "Epoch : [106] Train loss : [0.006735091377049684] Val Score : [0.4980392156862745])\n",
      "counter : 3\n",
      "Epoch : [107] Train loss : [0.006805320642888546] Val Score : [0.4980392156862745])\n",
      "counter : 4\n",
      "Epoch : [108] Train loss : [0.006923131830990315] Val Score : [0.4980392156862745])\n",
      "counter : 5\n",
      "Epoch : [109] Train loss : [0.006791076716035604] Val Score : [0.4980392156862745])\n",
      "counter : 6\n",
      "Epoch : [110] Train loss : [0.0066291804425418375] Val Score : [0.4980392156862745])\n",
      "counter : 7\n",
      "Epoch : [111] Train loss : [0.008664040826261044] Val Score : [0.49407114624505927])\n",
      "counter : 8\n",
      "Epoch : [112] Train loss : [0.006817393098026514] Val Score : [0.49407114624505927])\n",
      "counter : 9\n",
      "Epoch : [113] Train loss : [0.006629961449652911] Val Score : [0.4980392156862745])\n",
      "counter : 10\n",
      "Epoch : [114] Train loss : [0.007043030858039856] Val Score : [0.4980392156862745])\n",
      "counter : 11\n",
      "Epoch : [115] Train loss : [0.006912229489535094] Val Score : [0.4980392156862745])\n",
      "counter : 12\n",
      "Epoch : [116] Train loss : [0.0073548958636820315] Val Score : [0.4980392156862745])\n",
      "counter : 13\n",
      "Epoch 00117: reducing learning rate of group 0 to 9.7656e-05.\n",
      "Epoch : [117] Train loss : [0.006697351112961769] Val Score : [1.0])\n",
      "counter : 14\n",
      "Epoch : [118] Train loss : [0.006781934294849634] Val Score : [1.0])\n",
      "counter : 15\n",
      "Epoch : [119] Train loss : [0.006946920696645975] Val Score : [1.0])\n",
      "counter : 16\n",
      "Epoch : [120] Train loss : [0.006765821110457182] Val Score : [1.0])\n",
      "counter : 17\n",
      "Epoch : [121] Train loss : [0.00683095483109355] Val Score : [1.0])\n",
      "counter : 18\n",
      "Epoch : [122] Train loss : [0.0066091475076973435] Val Score : [1.0])\n",
      "counter : 19\n",
      "Epoch : [123] Train loss : [0.006886547617614269] Val Score : [1.0])\n",
      "counter : 20\n",
      "Epoch : [124] Train loss : [0.006732817087322473] Val Score : [0.4980392156862745])\n",
      "counter : 21\n",
      "Epoch : [125] Train loss : [0.00675341272726655] Val Score : [1.0])\n",
      "counter : 22\n",
      "Epoch : [126] Train loss : [0.0069394964724779126] Val Score : [1.0])\n",
      "counter : 23\n",
      "Epoch : [127] Train loss : [0.006696317810565233] Val Score : [1.0])\n",
      "counter : 24\n",
      "Epoch 00128: reducing learning rate of group 0 to 4.8828e-05.\n",
      "Epoch : [128] Train loss : [0.006729492451995611] Val Score : [1.0])\n",
      "counter : 25\n",
      "Epoch : [129] Train loss : [0.006919973343610763] Val Score : [0.4980392156862745])\n",
      "counter : 26\n",
      "Epoch : [130] Train loss : [0.0096042912453413] Val Score : [0.49407114624505927])\n",
      "counter : 27\n",
      "Epoch : [131] Train loss : [0.007012333534657955] Val Score : [0.49407114624505927])\n",
      "counter : 28\n",
      "Epoch : [132] Train loss : [0.00686079990118742] Val Score : [0.49606299212598426])\n",
      "counter : 29\n",
      "Epoch : [133] Train loss : [0.006934210285544395] Val Score : [0.4980392156862745])\n",
      "counter : 30\n",
      "Epoch : [134] Train loss : [0.006675539910793305] Val Score : [0.4980392156862745])\n",
      "counter : 31\n",
      "Epoch : [135] Train loss : [0.006930009555071592] Val Score : [0.4980392156862745])\n",
      "counter : 32\n",
      "Epoch : [136] Train loss : [0.007370926160365343] Val Score : [0.49407114624505927])\n",
      "counter : 33\n",
      "Epoch : [137] Train loss : [0.007019073329865932] Val Score : [0.4980392156862745])\n",
      "counter : 34\n",
      "Epoch : [138] Train loss : [0.006721160747110844] Val Score : [0.4980392156862745])\n",
      "counter : 35\n",
      "Epoch 00139: reducing learning rate of group 0 to 2.4414e-05.\n",
      "Epoch : [139] Train loss : [0.006834586057811976] Val Score : [0.4980392156862745])\n",
      "counter : 36\n",
      "Epoch : [140] Train loss : [0.00937090553343296] Val Score : [0.49407114624505927])\n",
      "counter : 37\n",
      "Epoch : [141] Train loss : [0.006712245475500822] Val Score : [0.4980392156862745])\n",
      "counter : 38\n",
      "Epoch : [142] Train loss : [0.0068610810674726965] Val Score : [0.49606299212598426])\n",
      "counter : 39\n",
      "Epoch : [143] Train loss : [0.006917476002126932] Val Score : [0.4980392156862745])\n",
      "counter : 40\n",
      "Epoch : [144] Train loss : [0.006917575560510159] Val Score : [0.4980392156862745])\n",
      "counter : 41\n",
      "Epoch : [145] Train loss : [0.006667227856814862] Val Score : [0.4980392156862745])\n",
      "counter : 42\n",
      "Epoch : [146] Train loss : [0.006624656915664673] Val Score : [0.4980392156862745])\n",
      "counter : 43\n",
      "Epoch : [147] Train loss : [0.008044721558690071] Val Score : [0.49407114624505927])\n",
      "counter : 44\n",
      "Epoch : [148] Train loss : [0.006862062774598598] Val Score : [0.4980392156862745])\n",
      "counter : 45\n",
      "Epoch : [149] Train loss : [0.0070476895198225975] Val Score : [0.4980392156862745])\n",
      "counter : 46\n",
      "Epoch 00150: reducing learning rate of group 0 to 1.2207e-05.\n",
      "Epoch : [150] Train loss : [0.006703021842986346] Val Score : [0.4980392156862745])\n",
      "counter : 47\n",
      "Epoch : [151] Train loss : [0.006785548850893974] Val Score : [0.4980392156862745])\n",
      "counter : 48\n",
      "Epoch : [152] Train loss : [0.0069456400349736215] Val Score : [0.4980392156862745])\n",
      "counter : 49\n",
      "Epoch : [153] Train loss : [0.006659834366291762] Val Score : [0.4980392156862745])\n",
      "counter : 50\n",
      "Epoch : [154] Train loss : [0.007116068340837956] Val Score : [0.4980392156862745])\n",
      "counter : 51\n",
      "Epoch : [155] Train loss : [0.007052951026707888] Val Score : [0.4980392156862745])\n",
      "counter : 52\n",
      "Epoch : [156] Train loss : [0.006820078939199448] Val Score : [0.4980392156862745])\n",
      "counter : 53\n",
      "Epoch : [157] Train loss : [0.0067773665301501754] Val Score : [0.4980392156862745])\n",
      "counter : 54\n",
      "Epoch : [158] Train loss : [0.007050400599837303] Val Score : [0.4980392156862745])\n",
      "counter : 55\n",
      "Epoch : [159] Train loss : [0.007185614015907049] Val Score : [0.4980392156862745])\n",
      "counter : 56\n",
      "Epoch : [160] Train loss : [0.008604303747415543] Val Score : [0.49407114624505927])\n",
      "counter : 57\n",
      "Epoch 00161: reducing learning rate of group 0 to 6.1035e-06.\n",
      "Epoch : [161] Train loss : [0.007039043866097927] Val Score : [0.49407114624505927])\n",
      "counter : 58\n",
      "Epoch : [162] Train loss : [0.006785472948104143] Val Score : [0.4980392156862745])\n",
      "counter : 59\n",
      "Epoch : [163] Train loss : [0.006795391254127025] Val Score : [0.4980392156862745])\n",
      "counter : 60\n",
      "Epoch : [164] Train loss : [0.006934532430022955] Val Score : [0.4980392156862745])\n",
      "counter : 61\n",
      "Epoch : [165] Train loss : [0.006688235979527235] Val Score : [0.4980392156862745])\n",
      "counter : 62\n",
      "Epoch : [166] Train loss : [0.006842527631670236] Val Score : [0.4980392156862745])\n",
      "counter : 63\n",
      "Epoch : [167] Train loss : [0.007107801828533411] Val Score : [0.4980392156862745])\n",
      "counter : 64\n",
      "Epoch : [168] Train loss : [0.007476213295012713] Val Score : [0.4980392156862745])\n",
      "counter : 65\n",
      "Epoch : [169] Train loss : [0.00684378519654274] Val Score : [0.4980392156862745])\n",
      "counter : 66\n",
      "Epoch : [170] Train loss : [0.007299159839749336] Val Score : [0.4980392156862745])\n",
      "counter : 67\n",
      "Epoch : [171] Train loss : [0.0068645304068923] Val Score : [0.4980392156862745])\n",
      "counter : 68\n",
      "Epoch 00172: reducing learning rate of group 0 to 3.0518e-06.\n",
      "Epoch : [172] Train loss : [0.0066985909827053545] Val Score : [1.0])\n",
      "counter : 69\n",
      "Epoch : [173] Train loss : [0.00899150427430868] Val Score : [0.49407114624505927])\n",
      "counter : 70\n",
      "Epoch : [174] Train loss : [0.006670017261058092] Val Score : [0.4980392156862745])\n",
      "counter : 71\n",
      "Epoch : [175] Train loss : [0.006905969697982073] Val Score : [0.4980392156862745])\n",
      "counter : 72\n",
      "Epoch : [176] Train loss : [0.006657253485172987] Val Score : [0.4980392156862745])\n",
      "counter : 73\n",
      "Epoch : [177] Train loss : [0.006857124529778958] Val Score : [0.4980392156862745])\n",
      "counter : 74\n",
      "Epoch : [178] Train loss : [0.006896512303501368] Val Score : [0.4980392156862745])\n",
      "counter : 75\n",
      "Epoch : [179] Train loss : [0.006803749594837427] Val Score : [0.4980392156862745])\n",
      "counter : 76\n",
      "Epoch : [180] Train loss : [0.006788927037268877] Val Score : [0.4980392156862745])\n",
      "counter : 77\n",
      "Epoch : [181] Train loss : [0.0074512801133096215] Val Score : [0.49606299212598426])\n",
      "counter : 78\n",
      "Epoch : [182] Train loss : [0.006696325354278087] Val Score : [0.4980392156862745])\n",
      "counter : 79\n",
      "Epoch 00183: reducing learning rate of group 0 to 1.5259e-06.\n",
      "Epoch : [183] Train loss : [0.006762142106890678] Val Score : [0.4980392156862745])\n",
      "counter : 80\n",
      "Epoch : [184] Train loss : [0.00868397979065776] Val Score : [0.49407114624505927])\n",
      "counter : 81\n",
      "Epoch : [185] Train loss : [0.006899334583431482] Val Score : [0.4980392156862745])\n",
      "counter : 82\n",
      "Epoch : [186] Train loss : [0.007076111156493425] Val Score : [0.4980392156862745])\n",
      "counter : 83\n",
      "Epoch : [187] Train loss : [0.007591699995100498] Val Score : [0.49407114624505927])\n",
      "counter : 84\n",
      "Epoch : [188] Train loss : [0.006824566144496202] Val Score : [0.4980392156862745])\n",
      "counter : 85\n",
      "Epoch : [189] Train loss : [0.006907470058649778] Val Score : [0.4980392156862745])\n",
      "counter : 86\n",
      "Epoch : [190] Train loss : [0.006847779452800751] Val Score : [0.4980392156862745])\n",
      "counter : 87\n",
      "Epoch : [191] Train loss : [0.006745457928627729] Val Score : [0.4980392156862745])\n",
      "counter : 88\n",
      "Epoch : [192] Train loss : [0.007027275674045086] Val Score : [0.4980392156862745])\n",
      "counter : 89\n",
      "Epoch : [193] Train loss : [0.007064173836261034] Val Score : [0.4980392156862745])\n",
      "counter : 90\n",
      "Epoch 00194: reducing learning rate of group 0 to 7.6294e-07.\n",
      "Epoch : [194] Train loss : [0.006993363611400127] Val Score : [0.4980392156862745])\n",
      "counter : 91\n",
      "Epoch : [195] Train loss : [0.007261044532060623] Val Score : [0.4980392156862745])\n",
      "counter : 92\n",
      "Epoch : [196] Train loss : [0.006925668008625508] Val Score : [0.4980392156862745])\n",
      "counter : 93\n",
      "Epoch : [197] Train loss : [0.006677258666604758] Val Score : [0.4980392156862745])\n",
      "counter : 94\n",
      "Epoch : [198] Train loss : [0.006656610127538443] Val Score : [1.0])\n",
      "counter : 95\n",
      "Epoch : [199] Train loss : [0.007117371540516615] Val Score : [1.0])\n",
      "counter : 96\n",
      "Epoch : [200] Train loss : [0.00676245242357254] Val Score : [1.0])\n",
      "counter : 97\n",
      "Epoch : [201] Train loss : [0.006911879871040583] Val Score : [0.4980392156862745])\n",
      "counter : 98\n",
      "Epoch : [202] Train loss : [0.007018590345978737] Val Score : [1.0])\n",
      "counter : 99\n",
      "Epoch : [203] Train loss : [0.006646628305315971] Val Score : [1.0])\n",
      "counter : 100\n",
      "early_stopping: 203\n"
     ]
    }
   ],
   "source": [
    "model = CNN()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=10, threshold_mode='abs', min_lr=1e-12, verbose=True)\n",
    "\n",
    "trainer = Trainer(model, optimizer, train_loader, val_loader, scheduler, device)\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96ca587b-6af7-444c-8396-0387d8b8f450",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (input_layer): Sequential(\n",
       "    (0): Conv1d(257, 512, kernel_size=(1,), stride=(1,))\n",
       "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): GELU(approximate='none')\n",
       "    (3): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (input_skip): Sequential(\n",
       "    (0): Conv1d(257, 512, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (squeeze_excite1): Squeeze_Excite_Block(\n",
       "    (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
       "    (fc): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=32, bias=False)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=32, out_features=512, bias=False)\n",
       "      (3): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (residual_conv1): ResConv(\n",
       "    (conv_block): Sequential(\n",
       "      (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): GELU(approximate='none')\n",
       "    )\n",
       "    (conv_skip): Sequential(\n",
       "      (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (squeeze_excite2): Squeeze_Excite_Block(\n",
       "    (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
       "    (fc): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=64, bias=False)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=64, out_features=1024, bias=False)\n",
       "      (3): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (residual_conv2): ResConv(\n",
       "    (conv_block): Sequential(\n",
       "      (0): Conv1d(1024, 2048, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): GELU(approximate='none')\n",
       "    )\n",
       "    (conv_skip): Sequential(\n",
       "      (0): Conv1d(1024, 2048, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (squeeze_excite3): Squeeze_Excite_Block(\n",
       "    (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
       "    (fc): Sequential(\n",
       "      (0): Linear(in_features=2048, out_features=128, bias=False)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=2048, bias=False)\n",
       "      (3): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (residual_conv3): ResConv(\n",
       "    (conv_block): Sequential(\n",
       "      (0): Conv1d(2048, 4096, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): GELU(approximate='none')\n",
       "    )\n",
       "    (conv_skip): Sequential(\n",
       "      (0): Conv1d(2048, 4096, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (aspp_bridge): ASPP(\n",
       "    (aspp_block1): Sequential(\n",
       "      (0): Conv1d(4096, 4096, kernel_size=(1,), stride=(1,))\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (aspp_block2): Sequential(\n",
       "      (0): Conv1d(4096, 4096, kernel_size=(1,), stride=(1,))\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (aspp_block3): Sequential(\n",
       "      (0): Conv1d(4096, 4096, kernel_size=(1,), stride=(1,))\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (output): Conv1d(12288, 4096, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (attn1): AttentionBlock(\n",
       "    (conv_encoder): Sequential(\n",
       "      (0): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Conv1d(2048, 4096, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (3): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (conv_decoder): Sequential(\n",
       "      (0): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Conv1d(4096, 4096, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    )\n",
       "    (conv_attn): Sequential(\n",
       "      (0): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): ReLU()\n",
       "      (2): Conv1d(4096, 1, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "  )\n",
       "  (upsample1): Upsample_(\n",
       "    (upsample): Upsample(scale_factor=1.0, mode=bilinear)\n",
       "  )\n",
       "  (up_residual_conv1): ResConv(\n",
       "    (conv_block): Sequential(\n",
       "      (0): Conv1d(6144, 2048, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): GELU(approximate='none')\n",
       "    )\n",
       "    (conv_skip): Sequential(\n",
       "      (0): Conv1d(6144, 2048, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (attn2): AttentionBlock(\n",
       "    (conv_encoder): Sequential(\n",
       "      (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Conv1d(1024, 2048, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (3): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (conv_decoder): Sequential(\n",
       "      (0): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Conv1d(2048, 2048, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    )\n",
       "    (conv_attn): Sequential(\n",
       "      (0): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): ReLU()\n",
       "      (2): Conv1d(2048, 1, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "  )\n",
       "  (upsample2): Upsample_(\n",
       "    (upsample): Upsample(scale_factor=1.0, mode=bilinear)\n",
       "  )\n",
       "  (up_residual_conv2): ResConv(\n",
       "    (conv_block): Sequential(\n",
       "      (0): Conv1d(3072, 2048, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): GELU(approximate='none')\n",
       "    )\n",
       "    (conv_skip): Sequential(\n",
       "      (0): Conv1d(3072, 2048, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (attn3): AttentionBlock(\n",
       "    (conv_encoder): Sequential(\n",
       "      (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Conv1d(512, 2048, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (3): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (conv_decoder): Sequential(\n",
       "      (0): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Conv1d(2048, 2048, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    )\n",
       "    (conv_attn): Sequential(\n",
       "      (0): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): ReLU()\n",
       "      (2): Conv1d(2048, 1, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "  )\n",
       "  (upsample3): Upsample_(\n",
       "    (upsample): Upsample(scale_factor=1.0, mode=bilinear)\n",
       "  )\n",
       "  (up_residual_conv3): ResConv(\n",
       "    (conv_block): Sequential(\n",
       "      (0): Conv1d(2560, 1024, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): GELU(approximate='none')\n",
       "    )\n",
       "    (conv_skip): Sequential(\n",
       "      (0): Conv1d(2560, 1024, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (aspp_out): ASPP(\n",
       "    (aspp_block1): Sequential(\n",
       "      (0): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (aspp_block2): Sequential(\n",
       "      (0): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (aspp_block3): Sequential(\n",
       "      (0): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (output): Conv1d(1536, 512, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc): Conv1d(512, 257, kernel_size=(1,), stride=(1,))\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNN()\n",
    "model.load_state_dict(torch.load('./best_model.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a634dee-9e6d-4332-898e-c564866d09c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "762db097-20e2-47db-a5b0-21e2b97b1553",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(test, False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e6872caa-8a20-45a4-8ded-76124adae2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(model, test_loader, device, thr=0.9993):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    pred = []\n",
    "    with torch.no_grad():\n",
    "        for x in iter(test_loader):\n",
    "            x = x.float().to(device)\n",
    "            _x = model(x)\n",
    "            log_target = F.log_softmax(_x, dim=1)\n",
    "            log_input = F.log_softmax(x.reshape(-1,257), dim=1)\n",
    "                \n",
    "            diff = cos(log_input, log_target).cpu().tolist()\n",
    "            print(diff)\n",
    "            batch_pred = np.where(np.array(diff) > thr, 0, 1).tolist()\n",
    "            pred += batch_pred\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cf0f701d-7862-42bc-a8df-a235785dd876",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9992483258247375, 0.9993578791618347, 0.9994314908981323, 0.9983837604522705, 0.9993669986724854, 0.9996156692504883, 0.9995800852775574, 0.9994066953659058, 0.9989098906517029, 0.9995006918907166, 0.9993021488189697, 0.9992926120758057, 0.9990189075469971, 0.9988436698913574, 0.9995005130767822, 0.9991111159324646, 0.9994137287139893, 0.9992807507514954, 0.9992237091064453, 0.9983579516410828, 0.9985300302505493, 0.9991097450256348, 0.9994000196456909, 0.9986343383789062, 0.9989681839942932, 0.9990826845169067, 0.9989393949508667, 0.99910569190979, 0.9992388486862183, 0.9995608329772949, 0.9995982050895691, 0.9990841150283813, 0.9995185136795044, 0.9990791082382202, 0.999056339263916, 0.9994670152664185, 0.9995079040527344, 0.999281644821167, 0.9993642568588257, 0.9994605779647827, 0.9992635250091553, 0.999529242515564, 0.9992015361785889, 0.9994982481002808, 0.9990350604057312, 0.9997432231903076, 0.9995969533920288, 0.9992828369140625, 0.9994145035743713, 0.9986430406570435, 0.9995566606521606, 0.9995201230049133, 0.9994684457778931, 0.9992061853408813, 0.9988783001899719, 0.9993371963500977, 0.9990410208702087, 0.9993731379508972, 0.9994633197784424, 0.9995309710502625, 0.999087929725647, 0.9990677833557129, 0.9991060495376587, 0.9993852376937866, 0.9991061687469482, 0.9995732307434082, 0.9992508292198181, 0.9993826746940613, 0.9994691610336304, 0.9992654919624329, 0.9993816018104553, 0.9993811845779419, 0.9993325471878052, 0.999603807926178, 0.9993537664413452, 0.9993919134140015, 0.9992245435714722, 0.9994335770606995, 0.9990123510360718, 0.9994043111801147, 0.9988850355148315, 0.99928218126297, 0.9993211627006531, 0.9990883469581604, 0.9989995956420898, 0.9992651343345642, 0.9993109703063965, 0.9993917942047119, 0.9993965029716492, 0.999643087387085, 0.9995105266571045, 0.9992372989654541, 0.998936653137207, 0.9995438456535339, 0.9992662668228149, 0.9982562065124512, 0.998687744140625, 0.9993777871131897, 0.9991201758384705, 0.9991469383239746, 0.9994834065437317, 0.9995285272598267, 0.9994163513183594, 0.9985557794570923, 0.9990804195404053, 0.9992342591285706, 0.9992886781692505, 0.9995266199111938, 0.9993001818656921, 0.9996052384376526, 0.999042272567749, 0.9992761611938477, 0.9995397329330444, 0.9989746809005737, 0.9986650943756104, 0.9991626143455505, 0.9996170997619629, 0.9990055561065674, 0.9992188811302185, 0.9992028474807739, 0.9993799328804016, 0.9989933371543884, 0.9991300106048584, 0.9987015724182129, 0.999474287033081, 0.9994573593139648, 0.999320387840271, 0.9996142387390137, 0.999498188495636, 0.9991769790649414, 0.9993095397949219, 0.9991461038589478, 0.999179482460022, 0.99921053647995, 0.9992989301681519, 0.999376118183136, 0.9987627267837524, 0.9993096590042114, 0.9992430210113525, 0.9992339015007019, 0.9994457364082336, 0.9989192485809326, 0.9992054104804993, 0.9994960427284241, 0.9987635612487793, 0.9991388320922852, 0.9991123676300049, 0.9992836117744446, 0.9989510774612427, 0.9990459084510803, 0.9989601373672485, 0.9993326663970947, 0.9991846084594727, 0.9996453523635864, 0.9996395707130432, 0.9992591142654419, 0.9993571639060974, 0.9991836547851562, 0.9990487098693848, 0.9991104602813721, 0.9991921186447144, 0.9995065927505493, 0.9993650913238525, 0.9989694356918335, 0.9993962645530701, 0.9990160465240479, 0.9978618025779724, 0.9988267421722412, 0.9985284805297852, 0.9995911717414856, 0.9991336464881897, 0.9992755055427551, 0.9989439249038696, 0.999001681804657, 0.9993764162063599, 0.9989064931869507, 0.9993919730186462, 0.9991116523742676, 0.9996537566184998, 0.9985489845275879, 0.9992610216140747, 0.9994040727615356, 0.9988675117492676, 0.999165415763855, 0.9996216297149658, 0.9992841482162476, 0.9996680021286011, 0.9990142583847046, 0.9994251132011414, 0.9994964003562927, 0.9988584518432617, 0.9993177652359009, 0.9991493225097656, 0.9986470937728882, 0.999220073223114, 0.9992907047271729, 0.9989755749702454, 0.9991068243980408, 0.9990390539169312, 0.9996414184570312, 0.9994903802871704, 0.9991494417190552, 0.9989831447601318, 0.9995871186256409, 0.9992839694023132, 0.9992730617523193, 0.9991745948791504, 0.9994207620620728, 0.9995688199996948, 0.999366819858551, 0.9990821480751038, 0.9992082118988037, 0.9993544816970825, 0.9991043210029602, 0.9994101524353027, 0.9996440410614014, 0.9986392259597778, 0.9990887641906738, 0.9992814064025879, 0.9990674257278442, 0.9993019104003906, 0.9993066787719727, 0.9994254112243652, 0.9992505311965942, 0.9996808767318726, 0.9985018372535706, 0.9989772439002991, 0.9989604949951172, 0.9996118545532227, 0.9996402859687805, 0.9993593692779541, 0.9987212419509888, 0.9992456436157227, 0.9993866086006165, 0.9994456768035889, 0.9991825819015503, 0.9996635317802429, 0.9995709657669067, 0.999444842338562, 0.9991755485534668, 0.9995291829109192, 0.9990471005439758, 0.9994935393333435, 0.9995932579040527, 0.9989667534828186, 0.9995065331459045, 0.9992836117744446, 0.9993553757667542, 0.9990173578262329, 0.9994860887527466, 0.9992066025733948, 0.9987443089485168, 0.9993448853492737, 0.9990973472595215, 0.9987500905990601, 0.9991224408149719]\n",
      "[0.9987818002700806, 0.9989721775054932, 0.9992413520812988, 0.9988083839416504, 0.9997052550315857, 0.9996179342269897, 0.9990106821060181, 0.9994922280311584, 0.9993653297424316, 0.9992436170578003, 0.9990147352218628, 0.9995403289794922, 0.9991283416748047, 0.9991774559020996, 0.9992663860321045, 0.9991704821586609, 0.9996300935745239, 0.9994854927062988, 0.9986572265625, 0.9994232058525085, 0.9984180331230164, 0.999225378036499, 0.9994310140609741, 0.9990593194961548, 0.9992142915725708, 0.9995322227478027, 0.9991558790206909, 0.9990140199661255, 0.9992631673812866, 0.9990435838699341, 0.9991576075553894, 0.9990898370742798, 0.9993922710418701, 0.9995561838150024, 0.9991891980171204, 0.9994068145751953, 0.9991820454597473, 0.9990726113319397, 0.9982743859291077, 0.9987000823020935, 0.9987406134605408, 0.9993906617164612, 0.99942547082901, 0.9993599057197571, 0.9991848468780518, 0.999258279800415, 0.9990342259407043, 0.9992809295654297, 0.9991316199302673, 0.9994510412216187, 0.9996148347854614, 0.9992542266845703, 0.9993020296096802, 0.9986786246299744, 0.9990514516830444, 0.9992777109146118, 0.9991781115531921, 0.9988225102424622, 0.9990594387054443, 0.9994500279426575, 0.9991031289100647, 0.9989899396896362, 0.9993345141410828, 0.9991934299468994, 0.999255895614624, 0.9989185333251953, 0.9995720386505127, 0.999132513999939, 0.9982356429100037, 0.9992459416389465, 0.9992234110832214, 0.9992913603782654, 0.9996143579483032, 0.9988043904304504, 0.999336302280426, 0.999198317527771, 0.9995832443237305, 0.9991665482521057, 0.9996102452278137, 0.9993356466293335, 0.9992901086807251, 0.9995547533035278, 0.9989314079284668, 0.999326765537262, 0.9995563626289368, 0.9994834661483765, 0.9993271827697754, 0.9990159273147583, 0.9991143941879272, 0.999248206615448, 0.9990161657333374, 0.9991517066955566, 0.999445915222168, 0.9992971420288086, 0.9990473389625549, 0.9988483786582947, 0.9986331462860107, 0.9994558095932007, 0.9991841316223145, 0.9985643029212952, 0.9994699954986572, 0.9995466470718384, 0.9994226694107056, 0.9996724128723145, 0.9995665550231934, 0.9988129138946533, 0.9994255304336548, 0.999355137348175, 0.9993070960044861, 0.9994890093803406, 0.9996220469474792, 0.9992328882217407, 0.9993674755096436, 0.9993613362312317, 0.9990167021751404, 0.9996079206466675, 0.9994693398475647, 0.9995278120040894, 0.9992729425430298, 0.9993515014648438, 0.9993801116943359, 0.9996565580368042, 0.9992156028747559, 0.9992332458496094, 0.9990586638450623, 0.9991669058799744, 0.9990950226783752, 0.9992541670799255, 0.9990232586860657, 0.9991424083709717, 0.9994395971298218, 0.9983125925064087, 0.9989577531814575, 0.9988362789154053, 0.9990746974945068, 0.9990016222000122, 0.9989128112792969, 0.9990556240081787, 0.9989988803863525, 0.9993184804916382, 0.9990869164466858, 0.9993820190429688, 0.9990975260734558, 0.9995408058166504, 0.9991415739059448, 0.9992730617523193, 0.9993855953216553, 0.9986730217933655, 0.9985835552215576, 0.9995018243789673, 0.9991682767868042, 0.999652624130249, 0.9996193051338196, 0.9993296265602112, 0.9995805025100708, 0.9988575577735901, 0.998571515083313, 0.9982249140739441, 0.9987055063247681, 0.9991661310195923, 0.9993994235992432, 0.9988943934440613, 0.9987058639526367, 0.9988136291503906, 0.9991047978401184, 0.9995338916778564, 0.9994009137153625, 0.9989688992500305, 0.9990351796150208, 0.9990106821060181, 0.9990895986557007, 0.9991896152496338, 0.999611496925354, 0.9995054602622986, 0.9993577003479004, 0.9994561672210693, 0.9993749260902405, 0.9988682866096497, 0.9993507266044617, 0.9994866251945496, 0.9994574785232544, 0.9997589588165283, 0.9989175200462341, 0.999126672744751, 0.9991455078125, 0.999121904373169, 0.9990171790122986, 0.9994117021560669, 0.9991917610168457, 0.9994480609893799, 0.9991837739944458, 0.999064564704895, 0.9995375871658325, 0.9991949200630188, 0.9984709024429321, 0.9988553524017334, 0.999143123626709, 0.9989895820617676, 0.9994876384735107, 0.9990620017051697, 0.9993730783462524, 0.9986745119094849, 0.9994581937789917, 0.9989752769470215, 0.9990629553794861, 0.9995464086532593, 0.9993104934692383, 0.9992936253547668, 0.9994583129882812, 0.9994325637817383, 0.9990167021751404, 0.9989805221557617, 0.9992583394050598, 0.9992009401321411, 0.9996109008789062, 0.9994266629219055, 0.999324381351471, 0.999375581741333, 0.9995142221450806, 0.9992866516113281, 0.9987863302230835, 0.9991757273674011, 0.9992783069610596, 0.9983543157577515, 0.9991832971572876, 0.9996640682220459, 0.9994984865188599, 0.998738169670105, 0.9994893074035645, 0.9984885454177856, 0.9997197389602661, 0.999342679977417, 0.9991695880889893, 0.9992485046386719, 0.9996708035469055, 0.9994921684265137, 0.9994418621063232, 0.9996020793914795, 0.9991704225540161, 0.9995169639587402, 0.9988030195236206, 0.9996000528335571, 0.9993046522140503, 0.9996351003646851, 0.999532163143158, 0.9991203546524048, 0.9990912675857544, 0.9986953735351562, 0.9989034533500671, 0.999358057975769, 0.9990732073783875, 0.9987642765045166, 0.9995713829994202, 0.9994398951530457, 0.999367356300354, 0.9993126392364502]\n",
      "[0.9996033310890198, 0.9994528889656067, 0.9990020990371704, 0.9986234903335571, 0.9993811845779419, 0.9995127320289612, 0.9994573593139648, 0.9991872906684875, 0.9993720650672913, 0.9994401931762695, 0.999396800994873, 0.9991288185119629, 0.9992517828941345, 0.9991800785064697, 0.998684823513031, 0.9987413883209229, 0.9991490244865417, 0.9989332556724548, 0.9991042613983154, 0.9984584450721741, 0.9995013475418091, 0.9993544816970825, 0.9989795684814453, 0.9994037747383118, 0.9995893239974976, 0.9987107515335083, 0.998884916305542, 0.9992400407791138, 0.9996159076690674, 0.9988390207290649, 0.9996259808540344, 0.9994669556617737, 0.9989227056503296, 0.9992421865463257, 0.9983446002006531, 0.9993760585784912, 0.999381422996521, 0.9991431832313538, 0.9992607235908508, 0.9982844591140747, 0.9994848370552063, 0.9994941353797913, 0.9995914697647095, 0.9995472431182861, 0.9992765188217163, 0.999487578868866, 0.9995996952056885, 0.9994069337844849, 0.9984140396118164, 0.9992660284042358, 0.9990027546882629, 0.9993244409561157, 0.9993046522140503, 0.9995517730712891, 0.9993978142738342, 0.9992474317550659, 0.9983471632003784, 0.9994988441467285, 0.9993472099304199, 0.9993149638175964, 0.9990835785865784, 0.9993366599082947, 0.9989436864852905, 0.9993972778320312, 0.9993080496788025, 0.9994279146194458, 0.9987235069274902, 0.9984287023544312, 0.9992759227752686, 0.9993181228637695, 0.9989883899688721, 0.9985888004302979, 0.998979926109314, 0.9996784925460815, 0.9991575479507446, 0.9992544054985046, 0.999458372592926, 0.9992035627365112, 0.9995114803314209, 0.9995288848876953, 0.9992355108261108, 0.9992412328720093, 0.9991962909698486, 0.9992477297782898, 0.9989116787910461, 0.9992491006851196, 0.9993304014205933, 0.9990445375442505, 0.9992352724075317, 0.9996805787086487, 0.9992783069610596, 0.999601423740387, 0.9992121458053589, 0.9986275434494019, 0.9989149570465088, 0.9993464946746826, 0.9995816946029663, 0.9992963075637817, 0.9991969466209412, 0.9995486736297607, 0.9990899562835693, 0.9990513324737549, 0.9988765120506287, 0.9991201162338257, 0.9992023706436157, 0.9993419051170349, 0.9995633363723755, 0.9989187717437744, 0.9996732473373413, 0.9987918734550476, 0.9995381832122803, 0.9992396831512451, 0.999027669429779, 0.9994760751724243, 0.9993873834609985, 0.9995716214179993, 0.9996188282966614, 0.9994398355484009, 0.9995085000991821, 0.9997106194496155, 0.9990695714950562, 0.9994645118713379, 0.9992687106132507, 0.9991885423660278, 0.9995351433753967, 0.9995931386947632, 0.999055027961731, 0.9989480376243591, 0.9994863271713257, 0.9990395903587341, 0.9995048642158508, 0.9996335506439209, 0.9992973208427429, 0.9993281364440918, 0.9992859363555908, 0.9990781545639038, 0.998774528503418, 0.9991981983184814, 0.9991904497146606, 0.9995518922805786, 0.9989768266677856, 0.9993494749069214, 0.9992524981498718, 0.999109148979187, 0.9990165829658508, 0.9993480443954468, 0.9992641806602478, 0.9988086223602295, 0.9993209838867188, 0.9989243745803833, 0.9993237257003784, 0.9995934367179871, 0.9995212554931641, 0.9997231364250183, 0.998924732208252, 0.9992330074310303, 0.9989110231399536, 0.999440610408783, 0.9978384971618652, 0.9991461634635925, 0.9992404580116272, 0.9983959197998047, 0.9992485642433167, 0.9995990991592407, 0.9995105266571045, 0.9992534518241882, 0.9987499713897705, 0.9991259574890137, 0.9993506669998169, 0.9990228414535522, 0.9992865324020386, 0.9992625713348389, 0.9992362260818481, 0.999464750289917, 0.9989256858825684, 0.9994022250175476, 0.9994338750839233, 0.9988235235214233, 0.999163806438446, 0.9993550777435303, 0.9994266033172607, 0.99913489818573, 0.999123215675354, 0.9993501305580139, 0.9993138313293457, 0.999403715133667, 0.9991551637649536, 0.9989122748374939, 0.9991124272346497, 0.9992370009422302, 0.9991085529327393, 0.9995577335357666, 0.9991735219955444, 0.998817503452301, 0.999669075012207, 0.9987894296646118, 0.998988151550293, 0.9988625049591064, 0.9994556307792664, 0.998994767665863, 0.9986462593078613, 0.9991490840911865, 0.9993939399719238, 0.9992161989212036, 0.9996434450149536, 0.9993418455123901, 0.999506413936615, 0.9992150068283081, 0.9992835521697998, 0.9994980096817017, 0.9993385076522827, 0.9992825984954834, 0.9989656209945679, 0.9995288848876953, 0.999062180519104, 0.9991929531097412, 0.9994736313819885, 0.999632716178894, 0.9990754723548889, 0.998950183391571, 0.9995978474617004, 0.9990977644920349, 0.9989356398582458, 0.9991803765296936, 0.999113917350769, 0.9995777010917664, 0.9987213015556335, 0.9991053938865662, 0.9993568062782288, 0.9991713762283325, 0.9992895722389221, 0.9994843006134033, 0.9989445805549622, 0.9990075826644897, 0.999392032623291, 0.9993381500244141, 0.9985455274581909, 0.9992419481277466, 0.9991148710250854, 0.9994057416915894, 0.999428391456604, 0.9990774393081665, 0.9992724657058716, 0.9992929697036743, 0.9992564916610718, 0.9988333582878113, 0.9993224143981934, 0.9985583424568176, 0.9994372129440308, 0.9995012879371643, 0.9994195103645325, 0.9993245601654053, 0.9992735385894775, 0.9994238018989563, 0.9994233250617981, 0.9989959597587585]\n",
      "[0.999127984046936, 0.9993054270744324, 0.9992561340332031, 0.9992407560348511, 0.9984282851219177, 0.9993038177490234, 0.9982097148895264, 0.9992142915725708, 0.9993396401405334, 0.9995603561401367, 0.9987817406654358, 0.9993326663970947, 0.9990009069442749, 0.9994001388549805, 0.9992066621780396, 0.9993627667427063, 0.999181866645813, 0.9985007047653198, 0.9992231130599976, 0.9994708299636841, 0.9989742040634155, 0.9995087385177612, 0.9992913603782654, 0.9991050958633423, 0.9992973804473877, 0.9986221194267273, 0.999595046043396, 0.999443531036377, 0.9993991255760193, 0.9995875358581543, 0.9991099834442139, 0.9995388984680176, 0.9990733861923218, 0.9992476105690002, 0.9995346665382385, 0.9993363618850708, 0.9989507794380188, 0.9994148015975952, 0.9993017911911011, 0.9995211362838745, 0.9994509220123291, 0.9995824694633484, 0.9992750883102417, 0.9993743896484375, 0.9991512894630432, 0.9993225932121277, 0.9990149140357971, 0.9990877509117126, 0.9988936185836792, 0.9993656873703003, 0.9995302557945251, 0.9996230006217957, 0.9995761513710022, 0.9994105696678162, 0.9988341927528381, 0.999366044998169, 0.9993048906326294, 0.9991448521614075, 0.99955815076828, 0.999117374420166, 0.9990307092666626, 0.9992941617965698, 0.9991912841796875, 0.9991899728775024, 0.9990006685256958, 0.9991320371627808, 0.9996536374092102, 0.999579906463623, 0.998524010181427, 0.9986899495124817, 0.9989827871322632, 0.9992961883544922, 0.9992470741271973, 0.999556303024292, 0.9995258450508118, 0.9993262887001038, 0.9990699291229248, 0.9991468191146851, 0.9983354806900024, 0.9993586540222168, 0.9982410073280334, 0.9985689520835876, 0.9990987181663513, 0.9996255040168762, 0.9996507167816162, 0.9990363717079163, 0.9992685914039612, 0.9992979764938354, 0.9991553425788879, 0.9996230602264404, 0.9995051622390747, 0.9993225336074829, 0.9996563196182251, 0.9991697669029236, 0.9993261098861694, 0.9986993074417114, 0.9993640184402466, 0.9994124174118042, 0.9994302988052368, 0.9993711709976196, 0.9994795918464661, 0.9985039234161377, 0.9985563158988953, 0.9992927312850952, 0.9996142387390137, 0.9990747570991516, 0.9989105463027954, 0.9996412992477417, 0.998715877532959, 0.9992835521697998, 0.9994652271270752, 0.9993444681167603, 0.9991944432258606, 0.9996426105499268, 0.9995095133781433, 0.9990746974945068, 0.9988843202590942, 0.9993880391120911, 0.9994795322418213, 0.9991762042045593, 0.9993347525596619, 0.9987331628799438, 0.9994618892669678, 0.999539852142334, 0.99940425157547, 0.9993278980255127, 0.9993293881416321, 0.9992551803588867, 0.9995338320732117, 0.9990953207015991, 0.9993878602981567, 0.9993298053741455, 0.9989924430847168, 0.9994083642959595, 0.9995700716972351, 0.9986282587051392, 0.9990061521530151, 0.999308705329895, 0.9995846748352051, 0.9992626309394836, 0.9991070032119751, 0.9991110563278198, 0.9991588592529297, 0.999398410320282, 0.9994722008705139, 0.9989528656005859, 0.9991151094436646, 0.9996170997619629, 0.9995885491371155, 0.998466968536377, 0.9992457628250122, 0.9993199110031128, 0.9985944628715515, 0.9994710087776184, 0.9990813136100769, 0.9990307688713074, 0.9987291693687439, 0.9988079071044922, 0.998775839805603, 0.9996418952941895, 0.9984152913093567, 0.9995316863059998, 0.9987695217132568, 0.9993999004364014, 0.999078094959259, 0.9992982745170593, 0.9988933205604553, 0.9987174272537231, 0.9991188049316406, 0.9988541007041931, 0.9994103312492371, 0.9992567300796509, 0.9985593557357788, 0.9987621903419495, 0.9985926151275635, 0.9996553659439087, 0.9995572566986084, 0.9995009303092957, 0.9993069767951965, 0.9982959032058716, 0.9994440078735352, 0.9993453025817871, 0.9990572333335876, 0.9993553757667542, 0.999433159828186, 0.9991893768310547, 0.9991200566291809, 0.999255359172821, 0.9995467662811279, 0.9988535046577454, 0.9991691708564758, 0.9986633658409119, 0.9995589852333069, 0.9996135234832764, 0.9984338283538818, 0.9985922574996948, 0.9992256164550781, 0.9990154504776001, 0.998871922492981, 0.9993411302566528, 0.99949049949646, 0.9992876052856445, 0.9990546703338623, 0.9993137717247009, 0.9987539649009705, 0.9991239309310913, 0.999241828918457, 0.9992997050285339, 0.998942494392395, 0.9992572069168091, 0.9993707537651062, 0.9993082284927368, 0.9993419647216797, 0.9992497563362122, 0.9993670582771301, 0.9991190433502197, 0.9994089603424072, 0.9993991851806641, 0.9988517165184021, 0.9992912411689758, 0.9994174242019653, 0.9995566606521606, 0.9991139769554138, 0.9991540908813477, 0.9986876845359802, 0.9993346333503723, 0.9992538094520569, 0.999530553817749, 0.999420166015625, 0.9996654987335205, 0.9991388320922852, 0.9994664192199707, 0.9989569187164307, 0.9993857741355896, 0.9996123313903809, 0.9994256496429443, 0.999293327331543, 0.9993253946304321, 0.9992966651916504, 0.9992551207542419, 0.9990447759628296, 0.9987523555755615, 0.9989991188049316, 0.9990293979644775, 0.9990721344947815, 0.9994345903396606, 0.9992800951004028, 0.9993869066238403, 0.9990312457084656, 0.9994116425514221, 0.999649167060852, 0.9990664720535278, 0.9995694756507874, 0.997967004776001, 0.999165415763855, 0.9996825456619263]\n",
      "[0.9994633197784424, 0.9994964003562927, 0.9981068968772888, 0.9994953870773315, 0.9992666244506836, 0.9988784193992615, 0.9986172318458557, 0.9993956685066223, 0.9992403984069824, 0.9986605048179626, 0.9992908239364624, 0.9993269443511963, 0.9994120597839355, 0.9984577298164368, 0.9990759491920471, 0.9989484548568726, 0.9990441799163818, 0.9994757175445557, 0.9990500807762146, 0.9990219473838806, 0.9991710186004639, 0.9992700815200806, 0.999157726764679, 0.9991435408592224, 0.9992668628692627, 0.9996551275253296, 0.9994211196899414, 0.9989908933639526, 0.9996740818023682, 0.9992872476577759, 0.9992477893829346, 0.9994910359382629, 0.998671293258667, 0.9994891881942749, 0.9992021918296814, 0.9994538426399231, 0.9995532035827637, 0.9988651871681213, 0.9986105561256409, 0.9990256428718567, 0.9995931386947632, 0.9993943572044373, 0.999140202999115, 0.9994831085205078, 0.999549150466919, 0.9993772506713867, 0.9991046190261841, 0.9996066689491272, 0.9995699524879456, 0.9990087151527405, 0.9992302656173706, 0.9987607002258301, 0.9996271133422852, 0.9994080662727356, 0.999320387840271, 0.9991987943649292, 0.9992948770523071, 0.9994029998779297, 0.999596357345581, 0.9991081953048706, 0.9992283582687378, 0.9994755387306213, 0.999252200126648, 0.9991949796676636, 0.9991508722305298, 0.9990668296813965, 0.9993595480918884, 0.9990204572677612, 0.9995160102844238, 0.9990479350090027, 0.9994687438011169, 0.9988596439361572, 0.999437153339386, 0.9990519881248474, 0.9994404911994934, 0.99836266040802, 0.9992250204086304, 0.9992936253547668, 0.9985891580581665, 0.9991235136985779, 0.9986624717712402, 0.999366283416748, 0.9991533756256104, 0.9994773268699646, 0.9988653659820557, 0.9993581175804138, 0.9993107318878174, 0.9992356300354004, 0.999068021774292, 0.9994996786117554, 0.9989254474639893, 0.9995827674865723, 0.9991359114646912, 0.9993716478347778, 0.9996128678321838, 0.9992947578430176, 0.9993245005607605, 0.9987401962280273, 0.9987069368362427, 0.9992679357528687, 0.9991747140884399, 0.9991169571876526, 0.9994842410087585, 0.9989194273948669, 0.999699056148529, 0.9993107914924622, 0.9990777969360352, 0.9993900060653687, 0.9989827871322632, 0.9994037747383118, 0.9994191527366638, 0.9985654354095459, 0.9995991587638855, 0.9988090991973877, 0.9994319081306458, 0.9994446039199829, 0.9991912245750427, 0.9992842674255371, 0.9985166788101196, 0.9983847141265869, 0.9994800686836243, 0.9995006918907166, 0.999156653881073, 0.9993605017662048, 0.9992192983627319, 0.998928427696228, 0.9992639422416687, 0.9988387823104858, 0.9992753267288208, 0.9990559220314026, 0.999343752861023, 0.9995230436325073, 0.9993552565574646, 0.9995940327644348, 0.9990051984786987, 0.9991319179534912, 0.9993535876274109, 0.9991428852081299, 0.9994516968727112, 0.9995415210723877, 0.9994414448738098, 0.9995934963226318, 0.9996068477630615, 0.9988565444946289, 0.9992051124572754, 0.9994127750396729, 0.9991532564163208, 0.9993168115615845, 0.9992548227310181, 0.9989311695098877, 0.99833083152771, 0.9993509650230408, 0.9994841814041138, 0.999535322189331, 0.9995409846305847, 0.9994261264801025, 0.9993051290512085, 0.9983962774276733, 0.99884033203125, 0.9991459846496582, 0.9990990161895752, 0.9986987113952637, 0.9988167881965637, 0.9992217421531677, 0.999541163444519, 0.999457836151123, 0.999016523361206, 0.9995133876800537, 0.9994848966598511, 0.9992340803146362, 0.9992997050285339, 0.9987586736679077, 0.9987891912460327, 0.9991580247879028, 0.9985179305076599, 0.9995806813240051, 0.9992383718490601, 0.9994957447052002, 0.9992952346801758, 0.9992942214012146, 0.9990290999412537, 0.9990003108978271, 0.9995925426483154, 0.9991551637649536, 0.9992614984512329, 0.9987145662307739, 0.9993479251861572, 0.9990971088409424, 0.999576985836029, 0.9994961023330688, 0.9990888237953186, 0.999488353729248, 0.9994910359382629, 0.9992846250534058, 0.9990372657775879, 0.9994006156921387, 0.9997317790985107, 0.9993412494659424, 0.998827338218689, 0.9993985891342163, 0.9987125992774963, 0.9994409680366516, 0.9996623992919922, 0.9994305372238159, 0.9995691776275635, 0.9993448853492737, 0.9994592070579529, 0.9994453191757202, 0.9989621043205261, 0.9990429878234863, 0.9987470507621765, 0.9990119934082031, 0.9991764426231384, 0.9993933439254761, 0.9988093376159668, 0.9991426467895508, 0.9991506338119507, 0.9993230700492859, 0.9990633726119995, 0.9995402097702026, 0.9989699125289917, 0.9988683462142944, 0.9990018010139465, 0.9991194009780884, 0.9994111657142639, 0.9991614818572998, 0.9993873238563538, 0.9986981153488159, 0.9991157054901123, 0.9995958209037781, 0.9986210465431213, 0.9992986917495728, 0.9991388320922852, 0.9994933605194092, 0.9988741278648376, 0.9989194869995117, 0.9987996816635132, 0.9991413950920105, 0.9991560578346252, 0.999180018901825, 0.9990134239196777, 0.9995943903923035, 0.9988377690315247, 0.9991833567619324, 0.9991607666015625, 0.999440610408783, 0.9995721578598022, 0.9996817111968994, 0.9991754293441772, 0.9994282126426697, 0.9990816116333008, 0.9992362856864929, 0.9992871284484863, 0.9995191097259521, 0.9993735551834106, 0.9992911219596863]\n",
      "[0.9994001388549805, 0.9981476068496704, 0.9992320537567139, 0.9992575645446777, 0.9990608096122742, 0.9988484978675842, 0.9988695383071899, 0.9991272687911987, 0.9996554851531982, 0.9992830157279968, 0.9995943307876587, 0.9992702603340149, 0.9990342855453491, 0.9991763234138489, 0.9991065859794617, 0.9994632005691528, 0.9993970394134521, 0.9995792508125305, 0.99892258644104, 0.9986005425453186, 0.9990003108978271, 0.9986148476600647, 0.9992320537567139, 0.999301552772522, 0.9991353154182434, 0.9990813136100769, 0.9993420243263245, 0.9996007084846497, 0.9993473291397095, 0.9985408782958984, 0.9995452761650085, 0.9991941452026367, 0.999221682548523, 0.9994235038757324, 0.9991757869720459, 0.998611569404602, 0.9983357191085815, 0.9987500905990601, 0.9994264841079712, 0.9992910027503967, 0.9993212223052979, 0.9992073178291321, 0.9995251893997192, 0.9992984533309937, 0.9990257024765015, 0.999454140663147, 0.9994527697563171, 0.9995547533035278, 0.9995180368423462, 0.9992923140525818, 0.9989410638809204, 0.9994285702705383, 0.9996538162231445, 0.9993501901626587, 0.9990437030792236, 0.9995567798614502, 0.9995092153549194, 0.999004065990448, 0.9996418952941895, 0.9987772703170776, 0.999261736869812, 0.9989743828773499, 0.9988071322441101, 0.999472439289093, 0.9986405372619629, 0.9987897872924805, 0.998622477054596, 0.9991104006767273, 0.9990314245223999, 0.9990972876548767, 0.9993893504142761, 0.9993914365768433, 0.9994849562644958, 0.9994146823883057, 0.9991047382354736, 0.9996194243431091, 0.9993671178817749, 0.9987558722496033, 0.9987195134162903, 0.9996721148490906, 0.9991343021392822, 0.9990425705909729, 0.9991016983985901, 0.9990673065185547, 0.999613881111145, 0.9993333220481873, 0.999225378036499, 0.9989843964576721, 0.9992421269416809, 0.9992716312408447, 0.9988393187522888, 0.9995847940444946, 0.9994503855705261, 0.9994232654571533, 0.9996896982192993, 0.9995357990264893, 0.9995760917663574, 0.9987097978591919, 0.9988638758659363, 0.9991484880447388, 0.9990985989570618, 0.9989646673202515, 0.9992090463638306, 0.9994193315505981, 0.999488115310669, 0.9995554685592651, 0.9990217685699463, 0.9990760087966919, 0.9994531869888306, 0.9993034601211548, 0.999140739440918, 0.999612033367157, 0.9994379281997681, 0.9996771812438965, 0.9995644092559814, 0.9995783567428589, 0.9987152814865112, 0.9995965957641602, 0.9996459484100342, 0.9996012449264526, 0.9985801577568054, 0.9994065761566162, 0.9992067217826843, 0.9995593428611755, 0.9992091059684753, 0.999413251876831, 0.9986613988876343, 0.9996626377105713, 0.9990720748901367, 0.9994170665740967, 0.9994308948516846, 0.9995302557945251, 0.9988775849342346, 0.9986670017242432, 0.9988631010055542, 0.9990097284317017, 0.9991036653518677, 0.9993505477905273, 0.9994565844535828, 0.9991579651832581, 0.999386727809906, 0.9991776943206787, 0.9989843368530273, 0.9995278716087341, 0.9995108246803284, 0.9993133544921875, 0.9991084933280945, 0.9990879893302917, 0.9991105794906616, 0.9986189603805542, 0.9996563196182251, 0.9992218017578125, 0.9990870356559753, 0.9992265105247498, 0.9993711709976196, 0.9989490509033203, 0.9990736246109009, 0.9993500709533691, 0.9992905259132385, 0.9994218349456787, 0.9991121292114258, 0.9986597299575806, 0.9985933303833008, 0.9995232224464417, 0.9993525147438049, 0.9989931583404541, 0.9993346333503723, 0.9989346265792847, 0.9992157816886902, 0.9993798732757568, 0.9991493225097656, 0.9992457628250122, 0.9991763830184937, 0.999321460723877, 0.9992740154266357, 0.9988853931427002, 0.9995747804641724, 0.9989821910858154, 0.9991826415061951, 0.9992272853851318, 0.9994456171989441, 0.9994178414344788, 0.9992970824241638, 0.9988535642623901, 0.9991726875305176, 0.999300479888916, 0.9995890855789185, 0.9995778203010559, 0.9996360540390015, 0.9996569156646729, 0.998967170715332, 0.998670220375061, 0.9990828633308411, 0.9994289875030518, 0.9994981288909912, 0.9990368485450745, 0.9992281794548035, 0.9996251463890076, 0.9991116523742676, 0.9991570711135864, 0.9993204474449158, 0.9993956089019775, 0.9985405802726746, 0.9994750618934631, 0.9995721578598022, 0.9995632171630859, 0.9996398687362671, 0.9993695020675659, 0.9996324777603149, 0.9990260601043701, 0.9992746710777283, 0.9995522499084473, 0.9994500875473022, 0.9993263483047485, 0.9992323517799377, 0.9992513656616211, 0.9990169405937195, 0.9982208013534546, 0.9992976784706116, 0.9992364048957825, 0.9993540644645691, 0.9990507364273071, 0.9993916749954224, 0.9994811415672302, 0.9997050166130066, 0.9992770552635193, 0.9994975924491882, 0.9992408156394958, 0.9995073676109314, 0.9984829425811768, 0.9990190267562866, 0.9995696544647217, 0.9995797872543335, 0.998640775680542]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "850"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = prediction(model, test_loader, device)\n",
    "sum(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fecf184-57e8-4969-bce1-46da090bdd32",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "05157612-68bf-4102-a723-8c0ad0f76755",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv(path+'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "38b5a428-5c17-4056-a3fe-4f50e30412ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAMPLE_ID</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_0001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_0002</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_0003</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_0004</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1509</th>\n",
       "      <td>TEST_1509</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1510</th>\n",
       "      <td>TEST_1510</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1511</th>\n",
       "      <td>TEST_1511</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1512</th>\n",
       "      <td>TEST_1512</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1513</th>\n",
       "      <td>TEST_1513</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1514 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      SAMPLE_ID  LABEL\n",
       "0     TEST_0000      1\n",
       "1     TEST_0001      0\n",
       "2     TEST_0002      0\n",
       "3     TEST_0003      1\n",
       "4     TEST_0004      0\n",
       "...         ...    ...\n",
       "1509  TEST_1509      1\n",
       "1510  TEST_1510      1\n",
       "1511  TEST_1511      0\n",
       "1512  TEST_1512      0\n",
       "1513  TEST_1513      1\n",
       "\n",
       "[1514 rows x 2 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit['LABEL'] = preds\n",
    "submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6c1c8567-eacd-4de3-b5e2-490102224edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('submit.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2fe3fa16-acc3-4628-a401-bc752544e41b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "850"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit['LABEL'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ae6f08-b18f-4a6c-a756-7c73c6b2c75d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
