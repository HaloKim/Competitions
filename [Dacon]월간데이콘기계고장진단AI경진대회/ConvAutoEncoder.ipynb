{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2bcba5f-002e-4f49-9622-ada6117faf0a",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b0d9b68-7102-4eca-9543-3b9b8acafc6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hist\\miniconda3\\envs\\ml\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import librosa\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba7851e-f369-4b1b-b5e3-b45233535157",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f7c0d625-02ae-42bf-b87c-bc60423e2284",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1000\n",
    "LR = 1e-1\n",
    "SR = 16000\n",
    "SEED = 42\n",
    "N_MFCC = 128\n",
    "BATCH = 256\n",
    "device = 'cuda'\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(SEED) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a4172e-5791-446f-9616-35c09d8bf25a",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a62c78cd-4f40-4e98-b8a6-1b6f1d906b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'open/'\n",
    "train_df = pd.read_csv(path+'train.csv') # 모두 정상 Sample\n",
    "test_df = pd.read_csv(path+'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33f37236-257c-44ab-8a9f-b2daf6c50ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAMPLE_ID</th>\n",
       "      <th>SAMPLE_PATH</th>\n",
       "      <th>FAN_TYPE</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>./train/TRAIN_0000.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>./train/TRAIN_0001.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>./train/TRAIN_0002.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>./train/TRAIN_0003.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>./train/TRAIN_0004.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274</th>\n",
       "      <td>TRAIN_1274</td>\n",
       "      <td>./train/TRAIN_1274.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275</th>\n",
       "      <td>TRAIN_1275</td>\n",
       "      <td>./train/TRAIN_1275.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1276</th>\n",
       "      <td>TRAIN_1276</td>\n",
       "      <td>./train/TRAIN_1276.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1277</th>\n",
       "      <td>TRAIN_1277</td>\n",
       "      <td>./train/TRAIN_1277.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1278</th>\n",
       "      <td>TRAIN_1278</td>\n",
       "      <td>./train/TRAIN_1278.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1279 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       SAMPLE_ID             SAMPLE_PATH  FAN_TYPE  LABEL\n",
       "0     TRAIN_0000  ./train/TRAIN_0000.wav         2      0\n",
       "1     TRAIN_0001  ./train/TRAIN_0001.wav         0      0\n",
       "2     TRAIN_0002  ./train/TRAIN_0002.wav         0      0\n",
       "3     TRAIN_0003  ./train/TRAIN_0003.wav         2      0\n",
       "4     TRAIN_0004  ./train/TRAIN_0004.wav         2      0\n",
       "...          ...                     ...       ...    ...\n",
       "1274  TRAIN_1274  ./train/TRAIN_1274.wav         2      0\n",
       "1275  TRAIN_1275  ./train/TRAIN_1275.wav         2      0\n",
       "1276  TRAIN_1276  ./train/TRAIN_1276.wav         2      0\n",
       "1277  TRAIN_1277  ./train/TRAIN_1277.wav         2      0\n",
       "1278  TRAIN_1278  ./train/TRAIN_1278.wav         0      0\n",
       "\n",
       "[1279 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee800463-995c-43ac-a05d-f3988923add9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mfcc_feature(df):\n",
    "    features = []\n",
    "    features2 = []\n",
    "    for path in tqdm(df['SAMPLE_PATH']):\n",
    "        # librosa패키지를 사용하여 wav 파일 load\n",
    "        y, sr = librosa.load(path, sr=SR)\n",
    "        \n",
    "        # melspectrogram\n",
    "        mels = librosa.feature.melspectrogram(y, sr=sr, n_mels=N_MFCC)\n",
    "        mels = librosa.power_to_db(mels, ref=np.max)\n",
    "        \n",
    "        # librosa패키지를 사용하여 mfcc 추출\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=N_MFCC)\n",
    "        \n",
    "        y_feature2 = []\n",
    "        # 추출된 MFCC들의 평균을 Feature로 사용\n",
    "        for e in mels:\n",
    "            y_feature2.append(np.mean(e))\n",
    "        features2.append(y_feature2)\n",
    "        \n",
    "        y_feature = []\n",
    "        # 추출된 MFCC들의 평균을 Feature로 사용\n",
    "        for e in mfcc:\n",
    "            y_feature.append(np.mean(e))\n",
    "        features.append(y_feature)\n",
    "    return features, features2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5b4be22-d948-4407-afe9-cf16c4281826",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1279/1279 [00:37<00:00, 34.54it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1514/1514 [00:43<00:00, 34.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 26s\n",
      "Wall time: 1min 20s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_features, train_features2 = get_mfcc_feature(train_df)\n",
    "test_features, test_features2 = get_mfcc_feature(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c86bd05-d873-4943-885c-b7efffe41048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 반복문으로 전체 컬럼명 변경하기\n",
    "def rename(df):\n",
    "    flag = 0\n",
    "    for col_name in df.columns:\n",
    "        if col_name == 0:\n",
    "            flag = 1\n",
    "        if flag == 1:\n",
    "            df.rename(columns = {col_name : 128+col_name}, inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03d6602e-38a0-4a7e-99b4-ba10b6d03d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAMPLE_ID</th>\n",
       "      <th>SAMPLE_PATH</th>\n",
       "      <th>FAN_TYPE</th>\n",
       "      <th>LABEL</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>...</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>./train/TRAIN_0000.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.952273</td>\n",
       "      <td>0.284621</td>\n",
       "      <td>0.295843</td>\n",
       "      <td>0.363586</td>\n",
       "      <td>0.357530</td>\n",
       "      <td>0.294327</td>\n",
       "      <td>...</td>\n",
       "      <td>0.833846</td>\n",
       "      <td>0.842291</td>\n",
       "      <td>0.818590</td>\n",
       "      <td>0.817896</td>\n",
       "      <td>0.799984</td>\n",
       "      <td>0.799645</td>\n",
       "      <td>0.811261</td>\n",
       "      <td>0.816584</td>\n",
       "      <td>0.825692</td>\n",
       "      <td>0.805742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>./train/TRAIN_0001.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.081981</td>\n",
       "      <td>0.935459</td>\n",
       "      <td>0.514880</td>\n",
       "      <td>0.613406</td>\n",
       "      <td>0.691659</td>\n",
       "      <td>0.825306</td>\n",
       "      <td>...</td>\n",
       "      <td>0.148846</td>\n",
       "      <td>0.144862</td>\n",
       "      <td>0.149996</td>\n",
       "      <td>0.165012</td>\n",
       "      <td>0.156853</td>\n",
       "      <td>0.160930</td>\n",
       "      <td>0.163661</td>\n",
       "      <td>0.162874</td>\n",
       "      <td>0.158788</td>\n",
       "      <td>0.084692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>./train/TRAIN_0002.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.240260</td>\n",
       "      <td>0.664368</td>\n",
       "      <td>0.724483</td>\n",
       "      <td>0.354632</td>\n",
       "      <td>0.625930</td>\n",
       "      <td>0.695975</td>\n",
       "      <td>...</td>\n",
       "      <td>0.454258</td>\n",
       "      <td>0.520886</td>\n",
       "      <td>0.454087</td>\n",
       "      <td>0.465858</td>\n",
       "      <td>0.423989</td>\n",
       "      <td>0.497721</td>\n",
       "      <td>0.504808</td>\n",
       "      <td>0.502295</td>\n",
       "      <td>0.487844</td>\n",
       "      <td>0.409240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>./train/TRAIN_0003.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.943729</td>\n",
       "      <td>0.295295</td>\n",
       "      <td>0.312391</td>\n",
       "      <td>0.371455</td>\n",
       "      <td>0.329344</td>\n",
       "      <td>0.268686</td>\n",
       "      <td>...</td>\n",
       "      <td>0.808817</td>\n",
       "      <td>0.827032</td>\n",
       "      <td>0.815166</td>\n",
       "      <td>0.787913</td>\n",
       "      <td>0.779337</td>\n",
       "      <td>0.781981</td>\n",
       "      <td>0.771998</td>\n",
       "      <td>0.779565</td>\n",
       "      <td>0.796365</td>\n",
       "      <td>0.798277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>./train/TRAIN_0004.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.949608</td>\n",
       "      <td>0.188729</td>\n",
       "      <td>0.179787</td>\n",
       "      <td>0.154144</td>\n",
       "      <td>0.007228</td>\n",
       "      <td>0.055841</td>\n",
       "      <td>...</td>\n",
       "      <td>0.877542</td>\n",
       "      <td>0.909439</td>\n",
       "      <td>0.894097</td>\n",
       "      <td>0.863824</td>\n",
       "      <td>0.849838</td>\n",
       "      <td>0.867351</td>\n",
       "      <td>0.865360</td>\n",
       "      <td>0.861068</td>\n",
       "      <td>0.868706</td>\n",
       "      <td>0.875541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274</th>\n",
       "      <td>TRAIN_1274</td>\n",
       "      <td>./train/TRAIN_1274.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.964989</td>\n",
       "      <td>0.096119</td>\n",
       "      <td>0.129592</td>\n",
       "      <td>0.222531</td>\n",
       "      <td>0.039493</td>\n",
       "      <td>0.075616</td>\n",
       "      <td>...</td>\n",
       "      <td>0.857574</td>\n",
       "      <td>0.875432</td>\n",
       "      <td>0.862135</td>\n",
       "      <td>0.853636</td>\n",
       "      <td>0.852105</td>\n",
       "      <td>0.854758</td>\n",
       "      <td>0.852087</td>\n",
       "      <td>0.851688</td>\n",
       "      <td>0.860304</td>\n",
       "      <td>0.881918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275</th>\n",
       "      <td>TRAIN_1275</td>\n",
       "      <td>./train/TRAIN_1275.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.959506</td>\n",
       "      <td>0.283203</td>\n",
       "      <td>0.293961</td>\n",
       "      <td>0.390142</td>\n",
       "      <td>0.322176</td>\n",
       "      <td>0.239990</td>\n",
       "      <td>...</td>\n",
       "      <td>0.842018</td>\n",
       "      <td>0.853391</td>\n",
       "      <td>0.822730</td>\n",
       "      <td>0.825484</td>\n",
       "      <td>0.822317</td>\n",
       "      <td>0.809976</td>\n",
       "      <td>0.820805</td>\n",
       "      <td>0.835689</td>\n",
       "      <td>0.836974</td>\n",
       "      <td>0.819587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1276</th>\n",
       "      <td>TRAIN_1276</td>\n",
       "      <td>./train/TRAIN_1276.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.930908</td>\n",
       "      <td>0.223856</td>\n",
       "      <td>0.210020</td>\n",
       "      <td>0.151861</td>\n",
       "      <td>0.134018</td>\n",
       "      <td>0.149416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.918239</td>\n",
       "      <td>0.932157</td>\n",
       "      <td>0.898576</td>\n",
       "      <td>0.905386</td>\n",
       "      <td>0.894843</td>\n",
       "      <td>0.879353</td>\n",
       "      <td>0.900221</td>\n",
       "      <td>0.910439</td>\n",
       "      <td>0.903620</td>\n",
       "      <td>0.895840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1277</th>\n",
       "      <td>TRAIN_1277</td>\n",
       "      <td>./train/TRAIN_1277.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.932890</td>\n",
       "      <td>0.247222</td>\n",
       "      <td>0.265837</td>\n",
       "      <td>0.223214</td>\n",
       "      <td>0.170546</td>\n",
       "      <td>0.141445</td>\n",
       "      <td>...</td>\n",
       "      <td>0.922436</td>\n",
       "      <td>0.946884</td>\n",
       "      <td>0.920099</td>\n",
       "      <td>0.916849</td>\n",
       "      <td>0.900980</td>\n",
       "      <td>0.903257</td>\n",
       "      <td>0.919087</td>\n",
       "      <td>0.922190</td>\n",
       "      <td>0.920950</td>\n",
       "      <td>0.932029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1278</th>\n",
       "      <td>TRAIN_1278</td>\n",
       "      <td>./train/TRAIN_1278.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.407188</td>\n",
       "      <td>0.620077</td>\n",
       "      <td>0.356668</td>\n",
       "      <td>0.769474</td>\n",
       "      <td>0.548411</td>\n",
       "      <td>0.806605</td>\n",
       "      <td>...</td>\n",
       "      <td>0.306191</td>\n",
       "      <td>0.317135</td>\n",
       "      <td>0.329551</td>\n",
       "      <td>0.308712</td>\n",
       "      <td>0.285871</td>\n",
       "      <td>0.289114</td>\n",
       "      <td>0.280076</td>\n",
       "      <td>0.270611</td>\n",
       "      <td>0.257248</td>\n",
       "      <td>0.177652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1279 rows × 260 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       SAMPLE_ID             SAMPLE_PATH  FAN_TYPE  LABEL       128       129  \\\n",
       "0     TRAIN_0000  ./train/TRAIN_0000.wav         2      0  0.952273  0.284621   \n",
       "1     TRAIN_0001  ./train/TRAIN_0001.wav         0      0  0.081981  0.935459   \n",
       "2     TRAIN_0002  ./train/TRAIN_0002.wav         0      0  0.240260  0.664368   \n",
       "3     TRAIN_0003  ./train/TRAIN_0003.wav         2      0  0.943729  0.295295   \n",
       "4     TRAIN_0004  ./train/TRAIN_0004.wav         2      0  0.949608  0.188729   \n",
       "...          ...                     ...       ...    ...       ...       ...   \n",
       "1274  TRAIN_1274  ./train/TRAIN_1274.wav         2      0  0.964989  0.096119   \n",
       "1275  TRAIN_1275  ./train/TRAIN_1275.wav         2      0  0.959506  0.283203   \n",
       "1276  TRAIN_1276  ./train/TRAIN_1276.wav         2      0  0.930908  0.223856   \n",
       "1277  TRAIN_1277  ./train/TRAIN_1277.wav         2      0  0.932890  0.247222   \n",
       "1278  TRAIN_1278  ./train/TRAIN_1278.wav         0      0  0.407188  0.620077   \n",
       "\n",
       "           130       131       132       133  ...       118       119  \\\n",
       "0     0.295843  0.363586  0.357530  0.294327  ...  0.833846  0.842291   \n",
       "1     0.514880  0.613406  0.691659  0.825306  ...  0.148846  0.144862   \n",
       "2     0.724483  0.354632  0.625930  0.695975  ...  0.454258  0.520886   \n",
       "3     0.312391  0.371455  0.329344  0.268686  ...  0.808817  0.827032   \n",
       "4     0.179787  0.154144  0.007228  0.055841  ...  0.877542  0.909439   \n",
       "...        ...       ...       ...       ...  ...       ...       ...   \n",
       "1274  0.129592  0.222531  0.039493  0.075616  ...  0.857574  0.875432   \n",
       "1275  0.293961  0.390142  0.322176  0.239990  ...  0.842018  0.853391   \n",
       "1276  0.210020  0.151861  0.134018  0.149416  ...  0.918239  0.932157   \n",
       "1277  0.265837  0.223214  0.170546  0.141445  ...  0.922436  0.946884   \n",
       "1278  0.356668  0.769474  0.548411  0.806605  ...  0.306191  0.317135   \n",
       "\n",
       "           120       121       122       123       124       125       126  \\\n",
       "0     0.818590  0.817896  0.799984  0.799645  0.811261  0.816584  0.825692   \n",
       "1     0.149996  0.165012  0.156853  0.160930  0.163661  0.162874  0.158788   \n",
       "2     0.454087  0.465858  0.423989  0.497721  0.504808  0.502295  0.487844   \n",
       "3     0.815166  0.787913  0.779337  0.781981  0.771998  0.779565  0.796365   \n",
       "4     0.894097  0.863824  0.849838  0.867351  0.865360  0.861068  0.868706   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1274  0.862135  0.853636  0.852105  0.854758  0.852087  0.851688  0.860304   \n",
       "1275  0.822730  0.825484  0.822317  0.809976  0.820805  0.835689  0.836974   \n",
       "1276  0.898576  0.905386  0.894843  0.879353  0.900221  0.910439  0.903620   \n",
       "1277  0.920099  0.916849  0.900980  0.903257  0.919087  0.922190  0.920950   \n",
       "1278  0.329551  0.308712  0.285871  0.289114  0.280076  0.270611  0.257248   \n",
       "\n",
       "           127  \n",
       "0     0.805742  \n",
       "1     0.084692  \n",
       "2     0.409240  \n",
       "3     0.798277  \n",
       "4     0.875541  \n",
       "...        ...  \n",
       "1274  0.881918  \n",
       "1275  0.819587  \n",
       "1276  0.895840  \n",
       "1277  0.932029  \n",
       "1278  0.177652  \n",
       "\n",
       "[1279 rows x 260 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = pd.concat([train_df, pd.DataFrame(train_features)], axis=1)\n",
    "tmp = rename(tmp)\n",
    "tmp = pd.concat([tmp, pd.DataFrame(train_features2)], axis=1)\n",
    "\n",
    "test = pd.concat([test_df, pd.DataFrame(test_features)], axis=1)\n",
    "test = rename(test)\n",
    "test = pd.concat([test, pd.DataFrame(test_features2)], axis=1)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "tmp.iloc[:,range(4,len(tmp.columns))] = scaler.fit_transform(tmp.iloc[:,range(4,len(tmp.columns))])\n",
    "test.iloc[:,range(3,len(test.columns))] = scaler.transform(test.iloc[:,range(3,len(test.columns))])\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09100fa5-d563-42aa-9575-1798d8ebbd40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAMPLE_ID</th>\n",
       "      <th>SAMPLE_PATH</th>\n",
       "      <th>FAN_TYPE</th>\n",
       "      <th>LABEL</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>...</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>./train/TRAIN_0000.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.952273</td>\n",
       "      <td>0.284621</td>\n",
       "      <td>0.295843</td>\n",
       "      <td>0.363586</td>\n",
       "      <td>0.357530</td>\n",
       "      <td>0.294327</td>\n",
       "      <td>...</td>\n",
       "      <td>0.833846</td>\n",
       "      <td>0.842291</td>\n",
       "      <td>0.818590</td>\n",
       "      <td>0.817896</td>\n",
       "      <td>0.799984</td>\n",
       "      <td>0.799645</td>\n",
       "      <td>0.811261</td>\n",
       "      <td>0.816584</td>\n",
       "      <td>0.825692</td>\n",
       "      <td>0.805742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>./train/TRAIN_0001.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.081981</td>\n",
       "      <td>0.935459</td>\n",
       "      <td>0.514880</td>\n",
       "      <td>0.613406</td>\n",
       "      <td>0.691659</td>\n",
       "      <td>0.825306</td>\n",
       "      <td>...</td>\n",
       "      <td>0.148846</td>\n",
       "      <td>0.144862</td>\n",
       "      <td>0.149996</td>\n",
       "      <td>0.165012</td>\n",
       "      <td>0.156853</td>\n",
       "      <td>0.160930</td>\n",
       "      <td>0.163661</td>\n",
       "      <td>0.162874</td>\n",
       "      <td>0.158788</td>\n",
       "      <td>0.084692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>./train/TRAIN_0002.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.240260</td>\n",
       "      <td>0.664368</td>\n",
       "      <td>0.724483</td>\n",
       "      <td>0.354632</td>\n",
       "      <td>0.625930</td>\n",
       "      <td>0.695975</td>\n",
       "      <td>...</td>\n",
       "      <td>0.454258</td>\n",
       "      <td>0.520886</td>\n",
       "      <td>0.454087</td>\n",
       "      <td>0.465858</td>\n",
       "      <td>0.423989</td>\n",
       "      <td>0.497721</td>\n",
       "      <td>0.504808</td>\n",
       "      <td>0.502295</td>\n",
       "      <td>0.487844</td>\n",
       "      <td>0.409240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>./train/TRAIN_0003.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.943729</td>\n",
       "      <td>0.295295</td>\n",
       "      <td>0.312391</td>\n",
       "      <td>0.371455</td>\n",
       "      <td>0.329344</td>\n",
       "      <td>0.268686</td>\n",
       "      <td>...</td>\n",
       "      <td>0.808817</td>\n",
       "      <td>0.827032</td>\n",
       "      <td>0.815166</td>\n",
       "      <td>0.787913</td>\n",
       "      <td>0.779337</td>\n",
       "      <td>0.781981</td>\n",
       "      <td>0.771998</td>\n",
       "      <td>0.779565</td>\n",
       "      <td>0.796365</td>\n",
       "      <td>0.798277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>./train/TRAIN_0004.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.949608</td>\n",
       "      <td>0.188729</td>\n",
       "      <td>0.179787</td>\n",
       "      <td>0.154144</td>\n",
       "      <td>0.007228</td>\n",
       "      <td>0.055841</td>\n",
       "      <td>...</td>\n",
       "      <td>0.877542</td>\n",
       "      <td>0.909439</td>\n",
       "      <td>0.894097</td>\n",
       "      <td>0.863824</td>\n",
       "      <td>0.849838</td>\n",
       "      <td>0.867351</td>\n",
       "      <td>0.865360</td>\n",
       "      <td>0.861068</td>\n",
       "      <td>0.868706</td>\n",
       "      <td>0.875541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274</th>\n",
       "      <td>TRAIN_1274</td>\n",
       "      <td>./train/TRAIN_1274.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.964989</td>\n",
       "      <td>0.096119</td>\n",
       "      <td>0.129592</td>\n",
       "      <td>0.222531</td>\n",
       "      <td>0.039493</td>\n",
       "      <td>0.075616</td>\n",
       "      <td>...</td>\n",
       "      <td>0.857574</td>\n",
       "      <td>0.875432</td>\n",
       "      <td>0.862135</td>\n",
       "      <td>0.853636</td>\n",
       "      <td>0.852105</td>\n",
       "      <td>0.854758</td>\n",
       "      <td>0.852087</td>\n",
       "      <td>0.851688</td>\n",
       "      <td>0.860304</td>\n",
       "      <td>0.881918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275</th>\n",
       "      <td>TRAIN_1275</td>\n",
       "      <td>./train/TRAIN_1275.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.959506</td>\n",
       "      <td>0.283203</td>\n",
       "      <td>0.293961</td>\n",
       "      <td>0.390142</td>\n",
       "      <td>0.322176</td>\n",
       "      <td>0.239990</td>\n",
       "      <td>...</td>\n",
       "      <td>0.842018</td>\n",
       "      <td>0.853391</td>\n",
       "      <td>0.822730</td>\n",
       "      <td>0.825484</td>\n",
       "      <td>0.822317</td>\n",
       "      <td>0.809976</td>\n",
       "      <td>0.820805</td>\n",
       "      <td>0.835689</td>\n",
       "      <td>0.836974</td>\n",
       "      <td>0.819587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1276</th>\n",
       "      <td>TRAIN_1276</td>\n",
       "      <td>./train/TRAIN_1276.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.930908</td>\n",
       "      <td>0.223856</td>\n",
       "      <td>0.210020</td>\n",
       "      <td>0.151861</td>\n",
       "      <td>0.134018</td>\n",
       "      <td>0.149416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.918239</td>\n",
       "      <td>0.932157</td>\n",
       "      <td>0.898576</td>\n",
       "      <td>0.905386</td>\n",
       "      <td>0.894843</td>\n",
       "      <td>0.879353</td>\n",
       "      <td>0.900221</td>\n",
       "      <td>0.910439</td>\n",
       "      <td>0.903620</td>\n",
       "      <td>0.895840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1277</th>\n",
       "      <td>TRAIN_1277</td>\n",
       "      <td>./train/TRAIN_1277.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.932890</td>\n",
       "      <td>0.247222</td>\n",
       "      <td>0.265837</td>\n",
       "      <td>0.223214</td>\n",
       "      <td>0.170546</td>\n",
       "      <td>0.141445</td>\n",
       "      <td>...</td>\n",
       "      <td>0.922436</td>\n",
       "      <td>0.946884</td>\n",
       "      <td>0.920099</td>\n",
       "      <td>0.916849</td>\n",
       "      <td>0.900980</td>\n",
       "      <td>0.903257</td>\n",
       "      <td>0.919087</td>\n",
       "      <td>0.922190</td>\n",
       "      <td>0.920950</td>\n",
       "      <td>0.932029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1278</th>\n",
       "      <td>TRAIN_1278</td>\n",
       "      <td>./train/TRAIN_1278.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.407188</td>\n",
       "      <td>0.620077</td>\n",
       "      <td>0.356668</td>\n",
       "      <td>0.769474</td>\n",
       "      <td>0.548411</td>\n",
       "      <td>0.806605</td>\n",
       "      <td>...</td>\n",
       "      <td>0.306191</td>\n",
       "      <td>0.317135</td>\n",
       "      <td>0.329551</td>\n",
       "      <td>0.308712</td>\n",
       "      <td>0.285871</td>\n",
       "      <td>0.289114</td>\n",
       "      <td>0.280076</td>\n",
       "      <td>0.270611</td>\n",
       "      <td>0.257248</td>\n",
       "      <td>0.177652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1279 rows × 260 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       SAMPLE_ID             SAMPLE_PATH  FAN_TYPE  LABEL       128       129  \\\n",
       "0     TRAIN_0000  ./train/TRAIN_0000.wav         2      0  0.952273  0.284621   \n",
       "1     TRAIN_0001  ./train/TRAIN_0001.wav         0      0  0.081981  0.935459   \n",
       "2     TRAIN_0002  ./train/TRAIN_0002.wav         0      0  0.240260  0.664368   \n",
       "3     TRAIN_0003  ./train/TRAIN_0003.wav         2      0  0.943729  0.295295   \n",
       "4     TRAIN_0004  ./train/TRAIN_0004.wav         2      0  0.949608  0.188729   \n",
       "...          ...                     ...       ...    ...       ...       ...   \n",
       "1274  TRAIN_1274  ./train/TRAIN_1274.wav         2      0  0.964989  0.096119   \n",
       "1275  TRAIN_1275  ./train/TRAIN_1275.wav         2      0  0.959506  0.283203   \n",
       "1276  TRAIN_1276  ./train/TRAIN_1276.wav         2      0  0.930908  0.223856   \n",
       "1277  TRAIN_1277  ./train/TRAIN_1277.wav         2      0  0.932890  0.247222   \n",
       "1278  TRAIN_1278  ./train/TRAIN_1278.wav         0      0  0.407188  0.620077   \n",
       "\n",
       "           130       131       132       133  ...       118       119  \\\n",
       "0     0.295843  0.363586  0.357530  0.294327  ...  0.833846  0.842291   \n",
       "1     0.514880  0.613406  0.691659  0.825306  ...  0.148846  0.144862   \n",
       "2     0.724483  0.354632  0.625930  0.695975  ...  0.454258  0.520886   \n",
       "3     0.312391  0.371455  0.329344  0.268686  ...  0.808817  0.827032   \n",
       "4     0.179787  0.154144  0.007228  0.055841  ...  0.877542  0.909439   \n",
       "...        ...       ...       ...       ...  ...       ...       ...   \n",
       "1274  0.129592  0.222531  0.039493  0.075616  ...  0.857574  0.875432   \n",
       "1275  0.293961  0.390142  0.322176  0.239990  ...  0.842018  0.853391   \n",
       "1276  0.210020  0.151861  0.134018  0.149416  ...  0.918239  0.932157   \n",
       "1277  0.265837  0.223214  0.170546  0.141445  ...  0.922436  0.946884   \n",
       "1278  0.356668  0.769474  0.548411  0.806605  ...  0.306191  0.317135   \n",
       "\n",
       "           120       121       122       123       124       125       126  \\\n",
       "0     0.818590  0.817896  0.799984  0.799645  0.811261  0.816584  0.825692   \n",
       "1     0.149996  0.165012  0.156853  0.160930  0.163661  0.162874  0.158788   \n",
       "2     0.454087  0.465858  0.423989  0.497721  0.504808  0.502295  0.487844   \n",
       "3     0.815166  0.787913  0.779337  0.781981  0.771998  0.779565  0.796365   \n",
       "4     0.894097  0.863824  0.849838  0.867351  0.865360  0.861068  0.868706   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1274  0.862135  0.853636  0.852105  0.854758  0.852087  0.851688  0.860304   \n",
       "1275  0.822730  0.825484  0.822317  0.809976  0.820805  0.835689  0.836974   \n",
       "1276  0.898576  0.905386  0.894843  0.879353  0.900221  0.910439  0.903620   \n",
       "1277  0.920099  0.916849  0.900980  0.903257  0.919087  0.922190  0.920950   \n",
       "1278  0.329551  0.308712  0.285871  0.289114  0.280076  0.270611  0.257248   \n",
       "\n",
       "           127  \n",
       "0     0.805742  \n",
       "1     0.084692  \n",
       "2     0.409240  \n",
       "3     0.798277  \n",
       "4     0.875541  \n",
       "...        ...  \n",
       "1274  0.881918  \n",
       "1275  0.819587  \n",
       "1276  0.895840  \n",
       "1277  0.932029  \n",
       "1278  0.177652  \n",
       "\n",
       "[1279 rows x 260 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a8fecd3-b50a-4c2a-9cb2-301a69345620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['FAN_TYPE',        128,        129,        130,        131,        132,\n",
       "              133,        134,        135,        136,\n",
       "       ...\n",
       "              118,        119,        120,        121,        122,        123,\n",
       "              124,        125,        126,        127],\n",
       "      dtype='object', length=257)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = tmp.columns.drop(['SAMPLE_ID', 'SAMPLE_PATH', 'LABEL'])\n",
    "cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e967190-c87e-458a-b9cb-4399574fa696",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a0bdc4b0-54e4-45c1-8bca-57c252dc591c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, eval_mode):\n",
    "        self.eval_mode = eval_mode\n",
    "        if self.eval_mode:\n",
    "            self.labels = df['LABEL'].values\n",
    "        self.df = df[cols].values\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if self.eval_mode:\n",
    "            self.x = self.df[index]\n",
    "            self.y = self.labels[index]\n",
    "            return torch.Tensor(self.x).reshape(-1,1), self.y\n",
    "        else:\n",
    "            self.x = self.df[index]\n",
    "            return torch.Tensor(self.x).reshape(-1,1)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f2afb059-203b-4ac7-aaec-79a0380abadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, tolerance=5, min_delta=0):\n",
    "        self.tolerance = tolerance\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.min_loss = np.inf\n",
    "\n",
    "    def __call__(self, train_loss, validation_loss=None):\n",
    "        if train_loss < self.min_loss:\n",
    "            self.counter = 0\n",
    "            self.min_loss = train_loss\n",
    "            print(f'counter : set 0 min loss : {self.min_loss}')\n",
    "        elif train_loss > self.min_loss:\n",
    "            self.counter += 1\n",
    "            print(f'counter : {self.counter}')\n",
    "        if self.counter >= self.tolerance:  \n",
    "            self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "20032097-fc78-4ae7-bf1a-bcd831ecca78",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(tmp, test_size=0.1, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "d381b860-662d-4cc1-a144-5410e9554877",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(df=train_df, eval_mode=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH, shuffle=True)\n",
    "\n",
    "val_dataset = CustomDataset(df = val_df, eval_mode=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "638207fa-027e-4b11-ab15-38ea0a9222df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, block):\n",
    "        super().__init__()\n",
    "        self.block = block\n",
    "    def forward(self, x):\n",
    "        return self.block(x) + x #f(x) + x\n",
    "    \n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.Encoder = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 257, out_channels = 512, kernel_size = 1, stride = 1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool1d(kernel_size=1, stride=1),\n",
    "            \n",
    "            nn.Conv1d(in_channels = 512, out_channels = 1024, kernel_size = 1, stride = 1),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool1d(kernel_size=1, stride=1),\n",
    "            \n",
    "            nn.Conv1d(in_channels = 1024, out_channels = 2048, kernel_size = 1, stride = 1),\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool1d(kernel_size=1, stride=1),\n",
    "            \n",
    "            ResBlock(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv1d(in_channels = 2048, out_channels = 2048, kernel_size = 1, stride = 1),\n",
    "                    nn.BatchNorm1d(2048),\n",
    "                    nn.GELU(),\n",
    "                    nn.MaxPool1d(kernel_size=1, stride=1),\n",
    "                    \n",
    "                    nn.Conv1d(in_channels = 2048, out_channels = 2048, kernel_size = 1, stride = 1),\n",
    "                    nn.BatchNorm1d(2048),\n",
    "                    nn.GELU(),\n",
    "                    nn.MaxPool1d(kernel_size=1, stride=1),\n",
    "                    \n",
    "                    nn.Conv1d(in_channels = 2048, out_channels = 2048, kernel_size = 1, stride = 1),\n",
    "                    nn.BatchNorm1d(2048),\n",
    "                    nn.GELU(),\n",
    "                    nn.MaxPool1d(kernel_size=1, stride=1),\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "        self.Decoder = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 2048, out_channels = 2048, kernel_size = 1, stride = 1),\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool1d(kernel_size=1, stride=1),\n",
    "            \n",
    "            ResBlock(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv1d(in_channels = 2048, out_channels = 2048, kernel_size = 1, stride = 1),\n",
    "                    nn.BatchNorm1d(2048),\n",
    "                    nn.GELU(),\n",
    "                    nn.MaxPool1d(kernel_size=1, stride=1),\n",
    "                    \n",
    "                    nn.Conv1d(in_channels = 2048, out_channels = 2048, kernel_size = 1, stride = 1),\n",
    "                    nn.BatchNorm1d(2048),\n",
    "                    nn.GELU(),\n",
    "                    nn.MaxPool1d(kernel_size=1, stride=1),\n",
    "                )\n",
    "            ),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        self.linear1 = nn.Sequential(\n",
    "            nn.Linear(2048,257),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.Encoder(x)\n",
    "        x = self.Decoder(x)\n",
    "        x = self.linear1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "2b146980-5d90-4639-ac2c-df6ea0e4fd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, model, optimizer, train_loader, val_loader, scheduler, device):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.scheduler = scheduler\n",
    "        self.device = device\n",
    "        self.early_stopping = EarlyStopping(tolerance=100, min_delta=10)\n",
    "        \n",
    "        # Loss Function\n",
    "        self.criterion = nn.KLDivLoss(reduction='batchmean', log_target=True).to(self.device)\n",
    "        # self.criterion = nn.L1Loss().to(self.device)\n",
    "        \n",
    "    def fit(self):\n",
    "        self.model.to(self.device)\n",
    "        best_score = 0\n",
    "        for epoch in range(EPOCHS):\n",
    "            self.model.train()\n",
    "            train_loss = []\n",
    "            for x in iter(self.train_loader):\n",
    "                self.optimizer.zero_grad()\n",
    "                x = x.float().to(self.device)\n",
    "                _x = self.model(x)\n",
    "\n",
    "                log_target = F.log_softmax(_x, dim=1)\n",
    "                log_input = F.log_softmax(x.reshape(-1,257), dim=1)\n",
    "                \n",
    "                loss = self.criterion(log_input, log_target)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                train_loss.append(loss.item())\n",
    "            \n",
    "            score = self.validation(self.model)\n",
    "            print(f'Epoch : [{epoch}] Train loss : [{np.mean(train_loss)}] Val Score : [{score}])')\n",
    "            \n",
    "            if best_score < score:\n",
    "                best_score = score\n",
    "                torch.save(model.state_dict(), './best_model.pth', _use_new_zipfile_serialization=False)\n",
    "                \n",
    "            # early stopping\n",
    "            self.early_stopping(np.mean(train_loss))\n",
    "            if self.early_stopping.early_stop:\n",
    "                print(\"early_stopping:\", epoch)\n",
    "                break\n",
    "            \n",
    "            if self.scheduler is not None:\n",
    "                self.scheduler.step(score)\n",
    "                            \n",
    "    def validation(self, eval_model, thr=0.999):\n",
    "        cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        eval_model.eval()\n",
    "        pred = []\n",
    "        true = []\n",
    "        with torch.no_grad():\n",
    "            for x, y in iter(self.val_loader):\n",
    "                x = x.float().to(self.device)\n",
    "                _x = self.model(x)\n",
    "                \n",
    "                log_target = F.log_softmax(_x, dim=1)\n",
    "                log_input = F.log_softmax(x.reshape(-1,257), dim=1)\n",
    "                \n",
    "                diff = cos(log_input, log_target).cpu().tolist()\n",
    "                # print(diff)\n",
    "                batch_pred = np.where(np.array(diff) > thr, 0, 1).tolist()\n",
    "\n",
    "                pred += batch_pred\n",
    "                true += y.tolist()\n",
    "\n",
    "        return f1_score(true, pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "5c4ed34d-cf4f-46ac-a7dc-e0af29698759",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [0] Train loss : [0.14858538061380386] Val Score : [0.4155251141552511])\n",
      "counter : set 0 min loss : 0.14858538061380386\n",
      "Epoch : [1] Train loss : [0.08728182315826416] Val Score : [0.42857142857142855])\n",
      "counter : set 0 min loss : 0.08728182315826416\n",
      "Epoch : [2] Train loss : [0.06534713953733444] Val Score : [0.4410480349344978])\n",
      "counter : set 0 min loss : 0.06534713953733444\n",
      "Epoch : [3] Train loss : [0.05427104830741882] Val Score : [0.46218487394957986])\n",
      "counter : set 0 min loss : 0.05427104830741882\n",
      "Epoch : [4] Train loss : [0.04691462963819504] Val Score : [0.4796747967479675])\n",
      "counter : set 0 min loss : 0.04691462963819504\n",
      "Epoch : [5] Train loss : [0.039523719996213916] Val Score : [0.4859437751004016])\n",
      "counter : set 0 min loss : 0.039523719996213916\n",
      "Epoch : [6] Train loss : [0.03675646707415581] Val Score : [0.4732510288065844])\n",
      "counter : set 0 min loss : 0.03675646707415581\n",
      "Epoch : [7] Train loss : [0.03313372023403645] Val Score : [0.4666666666666667])\n",
      "counter : set 0 min loss : 0.03313372023403645\n",
      "Epoch : [8] Train loss : [0.02946436554193497] Val Score : [0.452991452991453])\n",
      "counter : set 0 min loss : 0.02946436554193497\n",
      "Epoch : [9] Train loss : [0.02796301953494549] Val Score : [0.4482758620689655])\n",
      "counter : set 0 min loss : 0.02796301953494549\n",
      "Epoch : [10] Train loss : [0.024648043885827066] Val Score : [0.4434782608695652])\n",
      "counter : set 0 min loss : 0.024648043885827066\n",
      "Epoch : [11] Train loss : [0.02344106249511242] Val Score : [0.43859649122807015])\n",
      "counter : set 0 min loss : 0.02344106249511242\n",
      "Epoch : [12] Train loss : [0.020813308656215668] Val Score : [0.43612334801762115])\n",
      "counter : set 0 min loss : 0.020813308656215668\n",
      "Epoch : [13] Train loss : [0.019853029027581216] Val Score : [0.43612334801762115])\n",
      "counter : set 0 min loss : 0.019853029027581216\n",
      "Epoch : [14] Train loss : [0.019312482699751853] Val Score : [0.43612334801762115])\n",
      "counter : set 0 min loss : 0.019312482699751853\n",
      "Epoch : [15] Train loss : [0.0174374058842659] Val Score : [0.43612334801762115])\n",
      "counter : set 0 min loss : 0.0174374058842659\n",
      "Epoch : [16] Train loss : [0.01733516249805689] Val Score : [0.43859649122807015])\n",
      "counter : set 0 min loss : 0.01733516249805689\n",
      "Epoch 00017: reducing learning rate of group 0 to 5.0000e-02.\n",
      "Epoch : [17] Train loss : [0.01625408437103033] Val Score : [0.43859649122807015])\n",
      "counter : set 0 min loss : 0.01625408437103033\n",
      "Epoch : [18] Train loss : [0.015307706035673618] Val Score : [0.43859649122807015])\n",
      "counter : set 0 min loss : 0.015307706035673618\n",
      "Epoch : [19] Train loss : [0.014957865700125694] Val Score : [0.4410480349344978])\n",
      "counter : set 0 min loss : 0.014957865700125694\n",
      "Epoch : [20] Train loss : [0.01474327202886343] Val Score : [0.4410480349344978])\n",
      "counter : set 0 min loss : 0.01474327202886343\n",
      "Epoch : [21] Train loss : [0.01617431789636612] Val Score : [0.4434782608695652])\n",
      "counter : 1\n",
      "Epoch : [22] Train loss : [0.014823520742356776] Val Score : [0.4458874458874459])\n",
      "counter : 2\n",
      "Epoch : [23] Train loss : [0.01506216749548912] Val Score : [0.4410480349344978])\n",
      "counter : 3\n",
      "Epoch : [24] Train loss : [0.01366343218833208] Val Score : [0.45064377682403434])\n",
      "counter : set 0 min loss : 0.01366343218833208\n",
      "Epoch : [25] Train loss : [0.014739467948675155] Val Score : [0.452991452991453])\n",
      "counter : 1\n",
      "Epoch : [26] Train loss : [0.013746022433042526] Val Score : [0.459915611814346])\n",
      "counter : 2\n",
      "Epoch : [27] Train loss : [0.013205675967037677] Val Score : [0.459915611814346])\n",
      "counter : set 0 min loss : 0.013205675967037677\n",
      "Epoch 00028: reducing learning rate of group 0 to 2.5000e-02.\n",
      "Epoch : [28] Train loss : [0.012253138609230519] Val Score : [0.46218487394957986])\n",
      "counter : set 0 min loss : 0.012253138609230519\n",
      "Epoch : [29] Train loss : [0.013107805885374546] Val Score : [0.46218487394957986])\n",
      "counter : 1\n",
      "Epoch : [30] Train loss : [0.013036218285560609] Val Score : [0.46443514644351463])\n",
      "counter : 2\n",
      "Epoch : [31] Train loss : [0.013820289261639118] Val Score : [0.459915611814346])\n",
      "counter : 3\n",
      "Epoch : [32] Train loss : [0.011949383094906807] Val Score : [0.4666666666666667])\n",
      "counter : set 0 min loss : 0.011949383094906807\n",
      "Epoch : [33] Train loss : [0.01219734102487564] Val Score : [0.4666666666666667])\n",
      "counter : 1\n",
      "Epoch : [34] Train loss : [0.013949347846210002] Val Score : [0.46443514644351463])\n",
      "counter : 2\n",
      "Epoch : [35] Train loss : [0.011122938059270383] Val Score : [0.4666666666666667])\n",
      "counter : set 0 min loss : 0.011122938059270383\n",
      "Epoch : [36] Train loss : [0.011635101586580276] Val Score : [0.4666666666666667])\n",
      "counter : 1\n",
      "Epoch : [37] Train loss : [0.012840330041944981] Val Score : [0.4666666666666667])\n",
      "counter : 2\n",
      "Epoch : [38] Train loss : [0.010984920337796212] Val Score : [0.4666666666666667])\n",
      "counter : set 0 min loss : 0.010984920337796212\n",
      "Epoch 00039: reducing learning rate of group 0 to 1.2500e-02.\n",
      "Epoch : [39] Train loss : [0.011142443865537643] Val Score : [0.4666666666666667])\n",
      "counter : 1\n",
      "Epoch : [40] Train loss : [0.012657993845641613] Val Score : [0.4666666666666667])\n",
      "counter : 2\n",
      "Epoch : [41] Train loss : [0.011078610271215438] Val Score : [0.4666666666666667])\n",
      "counter : 3\n",
      "Epoch : [42] Train loss : [0.012296041660010814] Val Score : [0.46887966804979253])\n",
      "counter : 4\n",
      "Epoch : [43] Train loss : [0.010211027413606643] Val Score : [0.46887966804979253])\n",
      "counter : set 0 min loss : 0.010211027413606643\n",
      "Epoch : [44] Train loss : [0.010431577265262604] Val Score : [0.46887966804979253])\n",
      "counter : 1\n",
      "Epoch : [45] Train loss : [0.011756490543484687] Val Score : [0.46887966804979253])\n",
      "counter : 2\n",
      "Epoch : [46] Train loss : [0.010888078995049] Val Score : [0.46887966804979253])\n",
      "counter : 3\n",
      "Epoch : [47] Train loss : [0.01150461584329605] Val Score : [0.46887966804979253])\n",
      "counter : 4\n",
      "Epoch : [48] Train loss : [0.010787334479391574] Val Score : [0.46887966804979253])\n",
      "counter : 5\n",
      "Epoch : [49] Train loss : [0.010965119861066342] Val Score : [0.46887966804979253])\n",
      "counter : 6\n",
      "Epoch 00050: reducing learning rate of group 0 to 6.2500e-03.\n",
      "Epoch : [50] Train loss : [0.010323773138225079] Val Score : [0.46887966804979253])\n",
      "counter : 7\n",
      "Epoch : [51] Train loss : [0.0106128366664052] Val Score : [0.46887966804979253])\n",
      "counter : 8\n",
      "Epoch : [52] Train loss : [0.011149942129850387] Val Score : [0.46887966804979253])\n",
      "counter : 9\n",
      "Epoch : [53] Train loss : [0.01061147004365921] Val Score : [0.46887966804979253])\n",
      "counter : 10\n",
      "Epoch : [54] Train loss : [0.011041634529829026] Val Score : [0.47107438016528924])\n",
      "counter : 11\n",
      "Epoch : [55] Train loss : [0.010506690293550492] Val Score : [0.46887966804979253])\n",
      "counter : 12\n",
      "Epoch : [56] Train loss : [0.010408816859126091] Val Score : [0.47107438016528924])\n",
      "counter : 13\n",
      "Epoch : [57] Train loss : [0.010200090333819389] Val Score : [0.47107438016528924])\n",
      "counter : set 0 min loss : 0.010200090333819389\n",
      "Epoch : [58] Train loss : [0.011521091870963573] Val Score : [0.46887966804979253])\n",
      "counter : 1\n",
      "Epoch : [59] Train loss : [0.011135385371744633] Val Score : [0.46887966804979253])\n",
      "counter : 2\n",
      "Epoch : [60] Train loss : [0.01106086578220129] Val Score : [0.46887966804979253])\n",
      "counter : 3\n",
      "Epoch 00061: reducing learning rate of group 0 to 3.1250e-03.\n",
      "Epoch : [61] Train loss : [0.010543933883309365] Val Score : [0.46887966804979253])\n",
      "counter : 4\n",
      "Epoch : [62] Train loss : [0.010630986467003823] Val Score : [0.46887966804979253])\n",
      "counter : 5\n",
      "Epoch : [63] Train loss : [0.010945561714470387] Val Score : [0.46887966804979253])\n",
      "counter : 6\n",
      "Epoch : [64] Train loss : [0.01131373904645443] Val Score : [0.46887966804979253])\n",
      "counter : 7\n",
      "Epoch : [65] Train loss : [0.01037509497255087] Val Score : [0.47107438016528924])\n",
      "counter : 8\n",
      "Epoch : [66] Train loss : [0.010591472685337066] Val Score : [0.46887966804979253])\n",
      "counter : 9\n",
      "Epoch : [67] Train loss : [0.011851140297949314] Val Score : [0.46887966804979253])\n",
      "counter : 10\n",
      "Epoch : [68] Train loss : [0.010632641427218915] Val Score : [0.47107438016528924])\n",
      "counter : 11\n",
      "Epoch : [69] Train loss : [0.010583585314452647] Val Score : [0.47107438016528924])\n",
      "counter : 12\n",
      "Epoch : [70] Train loss : [0.009665611200034618] Val Score : [0.47107438016528924])\n",
      "counter : set 0 min loss : 0.009665611200034618\n",
      "Epoch : [71] Train loss : [0.0102925393730402] Val Score : [0.47107438016528924])\n",
      "counter : 1\n",
      "Epoch 00072: reducing learning rate of group 0 to 1.5625e-03.\n",
      "Epoch : [72] Train loss : [0.010333809442818165] Val Score : [0.47107438016528924])\n",
      "counter : 2\n",
      "Epoch : [73] Train loss : [0.010893718525767326] Val Score : [0.47107438016528924])\n",
      "counter : 3\n",
      "Epoch : [74] Train loss : [0.011334945075213909] Val Score : [0.47107438016528924])\n",
      "counter : 4\n",
      "Epoch : [75] Train loss : [0.009999136999249458] Val Score : [0.47107438016528924])\n",
      "counter : 5\n",
      "Epoch : [76] Train loss : [0.011622675135731697] Val Score : [0.47107438016528924])\n",
      "counter : 6\n",
      "Epoch : [77] Train loss : [0.010396030731499195] Val Score : [0.47107438016528924])\n",
      "counter : 7\n",
      "Epoch : [78] Train loss : [0.010395155847072601] Val Score : [0.47107438016528924])\n",
      "counter : 8\n",
      "Epoch : [79] Train loss : [0.01118776798248291] Val Score : [0.47107438016528924])\n",
      "counter : 9\n",
      "Epoch : [80] Train loss : [0.01111433282494545] Val Score : [0.47107438016528924])\n",
      "counter : 10\n",
      "Epoch : [81] Train loss : [0.010780209489166737] Val Score : [0.47107438016528924])\n",
      "counter : 11\n",
      "Epoch : [82] Train loss : [0.009675036557018757] Val Score : [0.47107438016528924])\n",
      "counter : 12\n",
      "Epoch 00083: reducing learning rate of group 0 to 7.8125e-04.\n",
      "Epoch : [83] Train loss : [0.010207797400653362] Val Score : [0.47107438016528924])\n",
      "counter : 13\n",
      "Epoch : [84] Train loss : [0.01019833590835333] Val Score : [0.46887966804979253])\n",
      "counter : 14\n",
      "Epoch : [85] Train loss : [0.01104882899671793] Val Score : [0.47107438016528924])\n",
      "counter : 15\n",
      "Epoch : [86] Train loss : [0.010319586843252182] Val Score : [0.47107438016528924])\n",
      "counter : 16\n",
      "Epoch : [87] Train loss : [0.01062898226082325] Val Score : [0.47107438016528924])\n",
      "counter : 17\n",
      "Epoch : [88] Train loss : [0.010994932800531387] Val Score : [0.47107438016528924])\n",
      "counter : 18\n",
      "Epoch : [89] Train loss : [0.010806262120604515] Val Score : [0.47107438016528924])\n",
      "counter : 19\n",
      "Epoch : [90] Train loss : [0.00972312930971384] Val Score : [0.47107438016528924])\n",
      "counter : 20\n",
      "Epoch : [91] Train loss : [0.010191346704959869] Val Score : [0.47107438016528924])\n",
      "counter : 21\n",
      "Epoch : [92] Train loss : [0.010042611509561539] Val Score : [0.47107438016528924])\n",
      "counter : 22\n",
      "Epoch : [93] Train loss : [0.009888316132128238] Val Score : [0.47107438016528924])\n",
      "counter : 23\n",
      "Epoch 00094: reducing learning rate of group 0 to 3.9063e-04.\n",
      "Epoch : [94] Train loss : [0.01071548443287611] Val Score : [0.47107438016528924])\n",
      "counter : 24\n",
      "Epoch : [95] Train loss : [0.01014572661370039] Val Score : [0.47107438016528924])\n",
      "counter : 25\n",
      "Epoch : [96] Train loss : [0.010718490928411484] Val Score : [0.47107438016528924])\n",
      "counter : 26\n",
      "Epoch : [97] Train loss : [0.009702115878462791] Val Score : [0.47107438016528924])\n",
      "counter : 27\n",
      "Epoch : [98] Train loss : [0.010371368750929833] Val Score : [0.47107438016528924])\n",
      "counter : 28\n",
      "Epoch : [99] Train loss : [0.01058968361467123] Val Score : [0.47107438016528924])\n",
      "counter : 29\n",
      "Epoch : [100] Train loss : [0.010123172216117382] Val Score : [0.46887966804979253])\n",
      "counter : 30\n",
      "Epoch : [101] Train loss : [0.010166634060442447] Val Score : [0.47107438016528924])\n",
      "counter : 31\n",
      "Epoch : [102] Train loss : [0.010810231789946556] Val Score : [0.47107438016528924])\n",
      "counter : 32\n",
      "Epoch : [103] Train loss : [0.011484507098793983] Val Score : [0.47107438016528924])\n",
      "counter : 33\n",
      "Epoch : [104] Train loss : [0.010593441501259803] Val Score : [0.47107438016528924])\n",
      "counter : 34\n",
      "Epoch 00105: reducing learning rate of group 0 to 1.9531e-04.\n",
      "Epoch : [105] Train loss : [0.010290073044598103] Val Score : [0.47107438016528924])\n",
      "counter : 35\n",
      "Epoch : [106] Train loss : [0.01020530927926302] Val Score : [0.47107438016528924])\n",
      "counter : 36\n",
      "Epoch : [107] Train loss : [0.010955508798360825] Val Score : [0.47107438016528924])\n",
      "counter : 37\n",
      "Epoch : [108] Train loss : [0.011182349734008312] Val Score : [0.47107438016528924])\n",
      "counter : 38\n",
      "Epoch : [109] Train loss : [0.009762991219758987] Val Score : [0.47107438016528924])\n",
      "counter : 39\n",
      "Epoch : [110] Train loss : [0.011273879930377007] Val Score : [0.47107438016528924])\n",
      "counter : 40\n",
      "Epoch : [111] Train loss : [0.01259254552423954] Val Score : [0.47107438016528924])\n",
      "counter : 41\n",
      "Epoch : [112] Train loss : [0.009844707697629929] Val Score : [0.47107438016528924])\n",
      "counter : 42\n",
      "Epoch : [113] Train loss : [0.010035129450261592] Val Score : [0.47107438016528924])\n",
      "counter : 43\n",
      "Epoch : [114] Train loss : [0.011104743555188179] Val Score : [0.47107438016528924])\n",
      "counter : 44\n",
      "Epoch : [115] Train loss : [0.010139804519712924] Val Score : [0.47107438016528924])\n",
      "counter : 45\n",
      "Epoch 00116: reducing learning rate of group 0 to 9.7656e-05.\n",
      "Epoch : [116] Train loss : [0.010138696245849132] Val Score : [0.47107438016528924])\n",
      "counter : 46\n",
      "Epoch : [117] Train loss : [0.009680443815886975] Val Score : [0.47107438016528924])\n",
      "counter : 47\n",
      "Epoch : [118] Train loss : [0.01011311337351799] Val Score : [0.47107438016528924])\n",
      "counter : 48\n",
      "Epoch : [119] Train loss : [0.01034308336675167] Val Score : [0.47107438016528924])\n",
      "counter : 49\n",
      "Epoch : [120] Train loss : [0.010527845844626427] Val Score : [0.47107438016528924])\n",
      "counter : 50\n",
      "Epoch : [121] Train loss : [0.010483619756996632] Val Score : [0.47107438016528924])\n",
      "counter : 51\n",
      "Epoch : [122] Train loss : [0.01023632362484932] Val Score : [0.47107438016528924])\n",
      "counter : 52\n",
      "Epoch : [123] Train loss : [0.009543328545987606] Val Score : [0.47107438016528924])\n",
      "counter : set 0 min loss : 0.009543328545987606\n",
      "Epoch : [124] Train loss : [0.010265141353011131] Val Score : [0.47107438016528924])\n",
      "counter : 1\n",
      "Epoch : [125] Train loss : [0.010626907274127007] Val Score : [0.47107438016528924])\n",
      "counter : 2\n",
      "Epoch : [126] Train loss : [0.010136189311742783] Val Score : [0.47107438016528924])\n",
      "counter : 3\n",
      "Epoch 00127: reducing learning rate of group 0 to 4.8828e-05.\n",
      "Epoch : [127] Train loss : [0.009433371387422085] Val Score : [0.47107438016528924])\n",
      "counter : set 0 min loss : 0.009433371387422085\n",
      "Epoch : [128] Train loss : [0.010087137669324875] Val Score : [0.47107438016528924])\n",
      "counter : 1\n",
      "Epoch : [129] Train loss : [0.012037765607237816] Val Score : [0.47107438016528924])\n",
      "counter : 2\n",
      "Epoch : [130] Train loss : [0.009938592463731766] Val Score : [0.47107438016528924])\n",
      "counter : 3\n",
      "Epoch : [131] Train loss : [0.009969554468989372] Val Score : [0.47107438016528924])\n",
      "counter : 4\n",
      "Epoch : [132] Train loss : [0.011501790396869183] Val Score : [0.46887966804979253])\n",
      "counter : 5\n",
      "Epoch : [133] Train loss : [0.009981072135269642] Val Score : [0.47107438016528924])\n",
      "counter : 6\n",
      "Epoch : [134] Train loss : [0.010351905226707458] Val Score : [0.47107438016528924])\n",
      "counter : 7\n",
      "Epoch : [135] Train loss : [0.010628456249833107] Val Score : [0.47107438016528924])\n",
      "counter : 8\n",
      "Epoch : [136] Train loss : [0.010883174650371075] Val Score : [0.47107438016528924])\n",
      "counter : 9\n",
      "Epoch : [137] Train loss : [0.01016037855297327] Val Score : [0.47107438016528924])\n",
      "counter : 10\n",
      "Epoch 00138: reducing learning rate of group 0 to 2.4414e-05.\n",
      "Epoch : [138] Train loss : [0.009977479279041291] Val Score : [0.47107438016528924])\n",
      "counter : 11\n",
      "Epoch : [139] Train loss : [0.01122846156358719] Val Score : [0.47107438016528924])\n",
      "counter : 12\n",
      "Epoch : [140] Train loss : [0.010338558815419674] Val Score : [0.47107438016528924])\n",
      "counter : 13\n",
      "Epoch : [141] Train loss : [0.012692497856914998] Val Score : [0.47107438016528924])\n",
      "counter : 14\n",
      "Epoch : [142] Train loss : [0.010686860978603363] Val Score : [0.47107438016528924])\n",
      "counter : 15\n",
      "Epoch : [143] Train loss : [0.00995761938393116] Val Score : [0.47107438016528924])\n",
      "counter : 16\n",
      "Epoch : [144] Train loss : [0.010760078765451908] Val Score : [0.47107438016528924])\n",
      "counter : 17\n",
      "Epoch : [145] Train loss : [0.010562979057431221] Val Score : [0.47107438016528924])\n",
      "counter : 18\n",
      "Epoch : [146] Train loss : [0.009664312936365604] Val Score : [0.47107438016528924])\n",
      "counter : 19\n",
      "Epoch : [147] Train loss : [0.010111412405967713] Val Score : [0.47107438016528924])\n",
      "counter : 20\n",
      "Epoch : [148] Train loss : [0.010867817886173725] Val Score : [0.47107438016528924])\n",
      "counter : 21\n",
      "Epoch 00149: reducing learning rate of group 0 to 1.2207e-05.\n",
      "Epoch : [149] Train loss : [0.010624779388308525] Val Score : [0.47107438016528924])\n",
      "counter : 22\n",
      "Epoch : [150] Train loss : [0.010453929007053376] Val Score : [0.47107438016528924])\n",
      "counter : 23\n",
      "Epoch : [151] Train loss : [0.009571594186127186] Val Score : [0.47107438016528924])\n",
      "counter : 24\n",
      "Epoch : [152] Train loss : [0.01128494068980217] Val Score : [0.47107438016528924])\n",
      "counter : 25\n",
      "Epoch : [153] Train loss : [0.010559294931590557] Val Score : [0.47107438016528924])\n",
      "counter : 26\n",
      "Epoch : [154] Train loss : [0.0097852423787117] Val Score : [0.47107438016528924])\n",
      "counter : 27\n",
      "Epoch : [155] Train loss : [0.011833381839096546] Val Score : [0.47107438016528924])\n",
      "counter : 28\n",
      "Epoch : [156] Train loss : [0.011738128587603569] Val Score : [0.47107438016528924])\n",
      "counter : 29\n",
      "Epoch : [157] Train loss : [0.010237882286310196] Val Score : [0.47107438016528924])\n",
      "counter : 30\n",
      "Epoch : [158] Train loss : [0.009716628678143024] Val Score : [0.47107438016528924])\n",
      "counter : 31\n",
      "Epoch : [159] Train loss : [0.010765691660344601] Val Score : [0.47107438016528924])\n",
      "counter : 32\n",
      "Epoch 00160: reducing learning rate of group 0 to 6.1035e-06.\n",
      "Epoch : [160] Train loss : [0.011095168255269527] Val Score : [0.47107438016528924])\n",
      "counter : 33\n",
      "Epoch : [161] Train loss : [0.010422688908874989] Val Score : [0.47107438016528924])\n",
      "counter : 34\n",
      "Epoch : [162] Train loss : [0.011121220141649246] Val Score : [0.47107438016528924])\n",
      "counter : 35\n",
      "Epoch : [163] Train loss : [0.01035724114626646] Val Score : [0.47107438016528924])\n",
      "counter : 36\n",
      "Epoch : [164] Train loss : [0.009760299324989319] Val Score : [0.47107438016528924])\n",
      "counter : 37\n",
      "Epoch : [165] Train loss : [0.010657686553895474] Val Score : [0.47107438016528924])\n",
      "counter : 38\n",
      "Epoch : [166] Train loss : [0.010282724723219871] Val Score : [0.47107438016528924])\n",
      "counter : 39\n",
      "Epoch : [167] Train loss : [0.010006901808083057] Val Score : [0.47107438016528924])\n",
      "counter : 40\n",
      "Epoch : [168] Train loss : [0.010669860988855362] Val Score : [0.47107438016528924])\n",
      "counter : 41\n",
      "Epoch : [169] Train loss : [0.009825816936790943] Val Score : [0.47107438016528924])\n",
      "counter : 42\n",
      "Epoch : [170] Train loss : [0.010049301013350487] Val Score : [0.47107438016528924])\n",
      "counter : 43\n",
      "Epoch 00171: reducing learning rate of group 0 to 3.0518e-06.\n",
      "Epoch : [171] Train loss : [0.010189729183912278] Val Score : [0.47107438016528924])\n",
      "counter : 44\n",
      "Epoch : [172] Train loss : [0.01015222854912281] Val Score : [0.47107438016528924])\n",
      "counter : 45\n",
      "Epoch : [173] Train loss : [0.010373604856431485] Val Score : [0.47107438016528924])\n",
      "counter : 46\n",
      "Epoch : [174] Train loss : [0.011935489624738694] Val Score : [0.47107438016528924])\n",
      "counter : 47\n",
      "Epoch : [175] Train loss : [0.010410281829535961] Val Score : [0.47107438016528924])\n",
      "counter : 48\n",
      "Epoch : [176] Train loss : [0.01226223148405552] Val Score : [0.47107438016528924])\n",
      "counter : 49\n",
      "Epoch : [177] Train loss : [0.010676241852343083] Val Score : [0.47107438016528924])\n",
      "counter : 50\n",
      "Epoch : [178] Train loss : [0.010065441019833088] Val Score : [0.47107438016528924])\n",
      "counter : 51\n",
      "Epoch : [179] Train loss : [0.010697866417467594] Val Score : [0.47107438016528924])\n",
      "counter : 52\n",
      "Epoch : [180] Train loss : [0.010839016921818257] Val Score : [0.47107438016528924])\n",
      "counter : 53\n",
      "Epoch : [181] Train loss : [0.009928568825125694] Val Score : [0.47107438016528924])\n",
      "counter : 54\n",
      "Epoch 00182: reducing learning rate of group 0 to 1.5259e-06.\n",
      "Epoch : [182] Train loss : [0.01076319646090269] Val Score : [0.47107438016528924])\n",
      "counter : 55\n",
      "Epoch : [183] Train loss : [0.009965595975518226] Val Score : [0.47107438016528924])\n",
      "counter : 56\n",
      "Epoch : [184] Train loss : [0.011091205850243568] Val Score : [0.47107438016528924])\n",
      "counter : 57\n",
      "Epoch : [185] Train loss : [0.010474764928221702] Val Score : [0.47107438016528924])\n",
      "counter : 58\n",
      "Epoch : [186] Train loss : [0.010498753935098647] Val Score : [0.47107438016528924])\n",
      "counter : 59\n",
      "Epoch : [187] Train loss : [0.010145557671785354] Val Score : [0.47107438016528924])\n",
      "counter : 60\n",
      "Epoch : [188] Train loss : [0.010722641833126545] Val Score : [0.47107438016528924])\n",
      "counter : 61\n",
      "Epoch : [189] Train loss : [0.010044497065246105] Val Score : [0.47107438016528924])\n",
      "counter : 62\n",
      "Epoch : [190] Train loss : [0.010388858430087567] Val Score : [0.47107438016528924])\n",
      "counter : 63\n",
      "Epoch : [191] Train loss : [0.010686924308538437] Val Score : [0.47107438016528924])\n",
      "counter : 64\n",
      "Epoch : [192] Train loss : [0.010680889338254928] Val Score : [0.47107438016528924])\n",
      "counter : 65\n",
      "Epoch 00193: reducing learning rate of group 0 to 7.6294e-07.\n",
      "Epoch : [193] Train loss : [0.010465090908110142] Val Score : [0.47107438016528924])\n",
      "counter : 66\n",
      "Epoch : [194] Train loss : [0.010239786654710769] Val Score : [0.47107438016528924])\n",
      "counter : 67\n",
      "Epoch : [195] Train loss : [0.00981430970132351] Val Score : [0.47107438016528924])\n",
      "counter : 68\n",
      "Epoch : [196] Train loss : [0.00994570255279541] Val Score : [0.47107438016528924])\n",
      "counter : 69\n",
      "Epoch : [197] Train loss : [0.009929541312158108] Val Score : [0.47107438016528924])\n",
      "counter : 70\n",
      "Epoch : [198] Train loss : [0.01022235993295908] Val Score : [0.47107438016528924])\n",
      "counter : 71\n",
      "Epoch : [199] Train loss : [0.009946883097290993] Val Score : [0.47107438016528924])\n",
      "counter : 72\n",
      "Epoch : [200] Train loss : [0.009802989847958087] Val Score : [0.47107438016528924])\n",
      "counter : 73\n",
      "Epoch : [201] Train loss : [0.009954226948320866] Val Score : [0.47107438016528924])\n",
      "counter : 74\n",
      "Epoch : [202] Train loss : [0.010785233974456788] Val Score : [0.47107438016528924])\n",
      "counter : 75\n",
      "Epoch : [203] Train loss : [0.010024026408791542] Val Score : [0.47107438016528924])\n",
      "counter : 76\n",
      "Epoch 00204: reducing learning rate of group 0 to 3.8147e-07.\n",
      "Epoch : [204] Train loss : [0.010313375107944011] Val Score : [0.47107438016528924])\n",
      "counter : 77\n",
      "Epoch : [205] Train loss : [0.010149250924587249] Val Score : [0.47107438016528924])\n",
      "counter : 78\n",
      "Epoch : [206] Train loss : [0.009950351156294345] Val Score : [0.47107438016528924])\n",
      "counter : 79\n",
      "Epoch : [207] Train loss : [0.010207976400852203] Val Score : [0.47107438016528924])\n",
      "counter : 80\n",
      "Epoch : [208] Train loss : [0.010738161951303482] Val Score : [0.47107438016528924])\n",
      "counter : 81\n",
      "Epoch : [209] Train loss : [0.010251271910965443] Val Score : [0.47107438016528924])\n",
      "counter : 82\n",
      "Epoch : [210] Train loss : [0.011663081683218479] Val Score : [0.46887966804979253])\n",
      "counter : 83\n",
      "Epoch : [211] Train loss : [0.010400663502514363] Val Score : [0.47107438016528924])\n",
      "counter : 84\n",
      "Epoch : [212] Train loss : [0.010162321291863919] Val Score : [0.47107438016528924])\n",
      "counter : 85\n",
      "Epoch : [213] Train loss : [0.009652883186936379] Val Score : [0.47107438016528924])\n",
      "counter : 86\n",
      "Epoch : [214] Train loss : [0.010731346718966962] Val Score : [0.47107438016528924])\n",
      "counter : 87\n",
      "Epoch 00215: reducing learning rate of group 0 to 1.9073e-07.\n",
      "Epoch : [215] Train loss : [0.01020988803356886] Val Score : [0.47107438016528924])\n",
      "counter : 88\n",
      "Epoch : [216] Train loss : [0.0094571553170681] Val Score : [0.47107438016528924])\n",
      "counter : 89\n",
      "Epoch : [217] Train loss : [0.010554239712655544] Val Score : [0.47107438016528924])\n",
      "counter : 90\n",
      "Epoch : [218] Train loss : [0.010368869826197624] Val Score : [0.47107438016528924])\n",
      "counter : 91\n",
      "Epoch : [219] Train loss : [0.010543821752071381] Val Score : [0.47107438016528924])\n",
      "counter : 92\n",
      "Epoch : [220] Train loss : [0.009989644773304463] Val Score : [0.47107438016528924])\n",
      "counter : 93\n",
      "Epoch : [221] Train loss : [0.009739714674651623] Val Score : [0.47107438016528924])\n",
      "counter : 94\n",
      "Epoch : [222] Train loss : [0.010120827332139015] Val Score : [0.47107438016528924])\n",
      "counter : 95\n",
      "Epoch : [223] Train loss : [0.009896021522581577] Val Score : [0.47107438016528924])\n",
      "counter : 96\n",
      "Epoch : [224] Train loss : [0.010580879636108875] Val Score : [0.47107438016528924])\n",
      "counter : 97\n",
      "Epoch : [225] Train loss : [0.010239253938198089] Val Score : [0.47107438016528924])\n",
      "counter : 98\n",
      "Epoch 00226: reducing learning rate of group 0 to 9.5367e-08.\n",
      "Epoch : [226] Train loss : [0.01006255131214857] Val Score : [0.47107438016528924])\n",
      "counter : 99\n",
      "Epoch : [227] Train loss : [0.010740701295435428] Val Score : [0.47107438016528924])\n",
      "counter : 100\n",
      "early_stopping: 227\n"
     ]
    }
   ],
   "source": [
    "model = CNN()\n",
    "model.eval()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=10, threshold_mode='abs', min_lr=1e-12, verbose=True)\n",
    "\n",
    "trainer = Trainer(model, optimizer, train_loader, val_loader, scheduler, device)\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "96ca587b-6af7-444c-8396-0387d8b8f450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (Encoder): Sequential(\n",
       "    (0): Conv1d(257, 300, kernel_size=(1,), stride=(1,))\n",
       "    (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): GELU(approximate='none')\n",
       "    (3): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv1d(300, 400, kernel_size=(1,), stride=(1,))\n",
       "    (5): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): GELU(approximate='none')\n",
       "    (7): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv1d(400, 500, kernel_size=(1,), stride=(1,))\n",
       "    (9): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): GELU(approximate='none')\n",
       "    (11): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (12): ResBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv1d(500, 500, kernel_size=(1,), stride=(1,))\n",
       "        (1): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "        (4): Conv1d(500, 500, kernel_size=(1,), stride=(1,))\n",
       "        (5): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (6): GELU(approximate='none')\n",
       "        (7): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "        (8): Conv1d(500, 500, kernel_size=(1,), stride=(1,))\n",
       "        (9): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (10): GELU(approximate='none')\n",
       "        (11): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (Decoder): Sequential(\n",
       "    (0): Conv1d(500, 500, kernel_size=(1,), stride=(1,))\n",
       "    (1): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): GELU(approximate='none')\n",
       "    (3): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): ResBlock(\n",
       "      (block): Sequential(\n",
       "        (0): Conv1d(500, 500, kernel_size=(1,), stride=(1,))\n",
       "        (1): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "        (4): Conv1d(500, 500, kernel_size=(1,), stride=(1,))\n",
       "        (5): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (6): GELU(approximate='none')\n",
       "        (7): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "    (5): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (linear1): Sequential(\n",
       "    (0): Linear(in_features=500, out_features=257, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNN()\n",
    "model.load_state_dict(torch.load('./best_model.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a634dee-9e6d-4332-898e-c564866d09c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "762db097-20e2-47db-a5b0-21e2b97b1553",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(test, False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "e6872caa-8a20-45a4-8ded-76124adae2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(model, test_loader, device, thr=0.999):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    pred = []\n",
    "    with torch.no_grad():\n",
    "        for x in iter(test_loader):\n",
    "            x = x.float().to(device)\n",
    "            _x = model(x)\n",
    "            log_target = F.log_softmax(_x, dim=1)\n",
    "            log_input = F.log_softmax(x.reshape(-1,257), dim=1)\n",
    "                \n",
    "            diff = cos(log_input, log_target).cpu().tolist()\n",
    "            print(diff)\n",
    "            batch_pred = np.where(np.array(diff) > thr, 0, 1).tolist()\n",
    "            pred += batch_pred\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "cf0f701d-7862-42bc-a8df-a235785dd876",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9997195601463318, 0.9999411106109619, 0.9993948340415955, 0.9962132573127747, 0.9994881749153137, 0.9999520778656006, 0.9999375343322754, 0.9998384714126587, 0.9966261982917786, 0.9993036985397339, 0.9986661672592163, 0.9998211860656738, 0.9993982315063477, 0.9994067549705505, 0.9998674988746643, 0.9994348287582397, 0.9999391436576843, 0.9998628497123718, 0.9998482465744019, 0.9969238638877869, 0.9966180324554443, 0.9994403123855591, 0.9995037913322449, 0.9993054866790771, 0.9997602701187134, 0.9997673630714417, 0.9993541240692139, 0.9997580051422119, 0.9981852173805237, 0.9999403953552246, 0.9998349547386169, 0.999808669090271, 0.9999027848243713, 0.9997929334640503, 0.999423623085022, 0.9999315738677979, 0.999935507774353, 0.9991163611412048, 0.9991644620895386, 0.9999282956123352, 0.9992177486419678, 0.9998941421508789, 0.9998664855957031, 0.999627947807312, 0.9997066259384155, 0.9997566342353821, 0.9996449947357178, 0.9998663663864136, 0.9999490976333618, 0.9992729425430298, 0.9999352693557739, 0.9999140501022339, 0.9995672106742859, 0.9995648264884949, 0.9992853403091431, 0.9998919367790222, 0.9993384480476379, 0.9998920559883118, 0.999778151512146, 0.9999188184738159, 0.9998157620429993, 0.9994271993637085, 0.999543309211731, 0.9999443888664246, 0.9997293949127197, 0.9998921155929565, 0.999579906463623, 0.9998348355293274, 0.999565839767456, 0.999092698097229, 0.9999573826789856, 0.9999585151672363, 0.9999068975448608, 0.9999411702156067, 0.9994908571243286, 0.99994957447052, 0.9986640214920044, 0.9999637007713318, 0.998340368270874, 0.9999043941497803, 0.9993109703063965, 0.9992808699607849, 0.9998167753219604, 0.9992531538009644, 0.9995014071464539, 0.9994643330574036, 0.9998945593833923, 0.9996973276138306, 0.9996225833892822, 0.9997426867485046, 0.9999570250511169, 0.9991806745529175, 0.999775767326355, 0.9995018839836121, 0.9989755749702454, 0.9961832165718079, 0.9976322650909424, 0.9998816251754761, 0.9998337030410767, 0.9998198747634888, 0.9994604587554932, 0.9998655915260315, 0.9999645948410034, 0.9975306391716003, 0.9995080828666687, 0.9994244575500488, 0.9999236464500427, 0.9996236562728882, 0.9993588924407959, 0.9999037981033325, 0.999684751033783, 0.9988940954208374, 0.9996458292007446, 0.9990813732147217, 0.9988608360290527, 0.9988600015640259, 0.9999231100082397, 0.9994305968284607, 0.9988890886306763, 0.9990492463111877, 0.9999104738235474, 0.9993678331375122, 0.9994193315505981, 0.9978888034820557, 0.9988062381744385, 0.9996644258499146, 0.999905526638031, 0.9996287822723389, 0.9990949630737305, 0.9988493323326111, 0.9998601675033569, 0.999549388885498, 0.9997918605804443, 0.9991108179092407, 0.9992896914482117, 0.9999434947967529, 0.9992891550064087, 0.9998595714569092, 0.9993760585784912, 0.9998884201049805, 0.999900221824646, 0.9993531107902527, 0.9984397292137146, 0.9999362826347351, 0.9993563890457153, 0.9986077547073364, 0.9998888373374939, 0.9991007447242737, 0.9985812306404114, 0.9997645616531372, 0.9997603893280029, 0.9999135136604309, 0.9998912811279297, 0.9996517896652222, 0.9998219013214111, 0.999096155166626, 0.9999067783355713, 0.9988394975662231, 0.9979188442230225, 0.9997876882553101, 0.9988812208175659, 0.9994878172874451, 0.9993152618408203, 0.9999000430107117, 0.9999231100082397, 0.9994341135025024, 0.9951010346412659, 0.9992179870605469, 0.997841477394104, 0.9995087385177612, 0.9994786977767944, 0.9992256164550781, 0.9994776248931885, 0.9979870319366455, 0.9996823668479919, 0.9993236660957336, 0.9999351501464844, 0.9989241361618042, 0.9998675584793091, 0.9992212653160095, 0.9989909529685974, 0.9998284578323364, 0.998699963092804, 0.9998237490653992, 0.9998534321784973, 0.9997480511665344, 0.9999281167984009, 0.998489499092102, 0.9995791912078857, 0.9998560547828674, 0.9970133900642395, 0.9991261959075928, 0.999219536781311, 0.9992623329162598, 0.9996563196182251, 0.9998049139976501, 0.9993624687194824, 0.9989426732063293, 0.9984836578369141, 0.9999213218688965, 0.9995090961456299, 0.9998885989189148, 0.999446451663971, 0.9997384548187256, 0.999907374382019, 0.9999343156814575, 0.999819815158844, 0.9994806051254272, 0.999956488609314, 0.9999538064002991, 0.9990745782852173, 0.999066948890686, 0.9998549818992615, 0.9991739988327026, 0.999956488609314, 0.9998937845230103, 0.9983386993408203, 0.9997736215591431, 0.9989979267120361, 0.9989145398139954, 0.9990940093994141, 0.9991226196289062, 0.9995874762535095, 0.9991915225982666, 0.9998466372489929, 0.9976373910903931, 0.9984613656997681, 0.999335765838623, 0.9999464750289917, 0.99982750415802, 0.9999436140060425, 0.9980086088180542, 0.9994452595710754, 0.9999302625656128, 0.9996910095214844, 0.9999223351478577, 0.9997072219848633, 0.9999452233314514, 0.9995384216308594, 0.9998424053192139, 0.9996297359466553, 0.9986101388931274, 0.9999035596847534, 0.9997303485870361, 0.9993219375610352, 0.999930202960968, 0.9990988373756409, 0.9999601244926453, 0.9993507862091064, 0.9998612403869629, 0.9998408555984497, 0.9993478059768677, 0.9992671012878418, 0.999819278717041, 0.9993864297866821, 0.9991303086280823]\n",
      "[0.9983928203582764, 0.9991034269332886, 0.9992555379867554, 0.9988136291503906, 0.9998757243156433, 0.9999551773071289, 0.9994038343429565, 0.9999309182167053, 0.9998054504394531, 0.9992679357528687, 0.997797966003418, 0.999958336353302, 0.9997695088386536, 0.9998520612716675, 0.9997128844261169, 0.9997706413269043, 0.9999476075172424, 0.9997110366821289, 0.9981065988540649, 0.999653160572052, 0.9982747435569763, 0.9988883137702942, 0.9996103048324585, 0.9998362064361572, 0.9998279809951782, 0.9996985793113708, 0.9997981190681458, 0.9994459748268127, 0.9991124868392944, 0.9993671178817749, 0.9999170303344727, 0.9997760057449341, 0.9998927116394043, 0.9998899698257446, 0.9998589158058167, 0.9997932314872742, 0.9998451471328735, 0.9995324611663818, 0.9969035387039185, 0.999381959438324, 0.9983006715774536, 0.9999515414237976, 0.9998725056648254, 0.9996936321258545, 0.9998630285263062, 0.9989876747131348, 0.9998655915260315, 0.999783992767334, 0.9982086420059204, 0.9997003078460693, 0.9998735785484314, 0.9998204708099365, 0.9999100565910339, 0.9993220567703247, 0.9997689723968506, 0.9989707469940186, 0.9984172582626343, 0.999137282371521, 0.9993999600410461, 0.9998903870582581, 0.9993940591812134, 0.9997555613517761, 0.9998847246170044, 0.9994527101516724, 0.9990493059158325, 0.9971791505813599, 0.9999544620513916, 0.997970461845398, 0.9964133501052856, 0.9998877644538879, 0.9991430044174194, 0.9989818930625916, 0.9999575614929199, 0.9991472959518433, 0.9999401569366455, 0.9998988509178162, 0.9999072551727295, 0.999847412109375, 0.9999305009841919, 0.999366283416748, 0.9998868703842163, 0.9995845556259155, 0.9992574453353882, 0.9999257922172546, 0.9999356865882874, 0.9994513988494873, 0.9993237257003784, 0.9986234307289124, 0.9997538328170776, 0.9991695284843445, 0.9997772574424744, 0.999096691608429, 0.9998384714126587, 0.999237060546875, 0.9993745684623718, 0.9992552995681763, 0.9980801343917847, 0.9997636079788208, 0.9992668628692627, 0.9986813068389893, 0.9999291896820068, 0.9999370574951172, 0.9999154210090637, 0.9999176263809204, 0.9996761679649353, 0.9987608194351196, 0.9999510645866394, 0.9999417662620544, 0.9992940425872803, 0.9997318387031555, 0.9996139407157898, 0.999802827835083, 0.9994913339614868, 0.9999526739120483, 0.9993585348129272, 0.9999277591705322, 0.9995629787445068, 0.9996465444564819, 0.9990758895874023, 0.999955415725708, 0.9999282360076904, 0.9995755553245544, 0.9997650980949402, 0.9985969066619873, 0.9995467662811279, 0.999165952205658, 0.9993354082107544, 0.9998084306716919, 0.9994072914123535, 0.9998161196708679, 0.9999508261680603, 0.9965988397598267, 0.9992592930793762, 0.9982638955116272, 0.9997807741165161, 0.999612033367157, 0.9994696378707886, 0.9995893239974976, 0.9975005388259888, 0.9996098279953003, 0.9997837543487549, 0.999943733215332, 0.9997646808624268, 0.9996926188468933, 0.9995575547218323, 0.9995834231376648, 0.9995434284210205, 0.9977238178253174, 0.9970869421958923, 0.9996276497840881, 0.998629093170166, 0.9999467134475708, 0.9996716976165771, 0.9995356798171997, 0.999901294708252, 0.9987887740135193, 0.998991847038269, 0.9963654279708862, 0.9990036487579346, 0.9997696876525879, 0.9999006986618042, 0.9988347887992859, 0.9981775283813477, 0.999056339263916, 0.9992390275001526, 0.9996792078018188, 0.9999545216560364, 0.9994407892227173, 0.9998102188110352, 0.9993767142295837, 0.9984534382820129, 0.9997619390487671, 0.999605655670166, 0.9997791051864624, 0.9995214939117432, 0.9999052286148071, 0.9999512434005737, 0.9996476769447327, 0.999199628829956, 0.9999442100524902, 0.9997363090515137, 0.9998858571052551, 0.9992666840553284, 0.9998605847358704, 0.9991068840026855, 0.9994974136352539, 0.9994359016418457, 0.9999361038208008, 0.9992743730545044, 0.9995702505111694, 0.9997687935829163, 0.9997105002403259, 0.9997222423553467, 0.9998582005500793, 0.9966154098510742, 0.9987819194793701, 0.9973294734954834, 0.9997761845588684, 0.9996664524078369, 0.9997814893722534, 0.9999494552612305, 0.997831404209137, 0.9998902678489685, 0.9994872808456421, 0.9998229146003723, 0.9995421767234802, 0.9991486072540283, 0.9999003410339355, 0.99957275390625, 0.9992408752441406, 0.999794602394104, 0.9993213415145874, 0.9987710118293762, 0.9998540878295898, 0.9995622038841248, 0.9995927214622498, 0.999687671661377, 0.9999144077301025, 0.9995566606521606, 0.9990999698638916, 0.999085545539856, 0.9998050928115845, 0.999871015548706, 0.9969463348388672, 0.9995219111442566, 0.9996862411499023, 0.9994809627532959, 0.9983925819396973, 0.9997974038124084, 0.9955908060073853, 0.9997332096099854, 0.999563455581665, 0.9995874166488647, 0.9989184737205505, 0.9998998045921326, 0.9997121095657349, 0.9999319314956665, 0.9998891353607178, 0.9995971918106079, 0.9996427893638611, 0.9990172982215881, 0.9997768998146057, 0.9996483325958252, 0.999828577041626, 0.9999533891677856, 0.9998223185539246, 0.9992642402648926, 0.9981791973114014, 0.9966568946838379, 0.9999366402626038, 0.9985928535461426, 0.999352753162384, 0.9999285936355591, 0.9999605417251587, 0.9999533891677856, 0.9999367594718933]\n",
      "[0.9998939037322998, 0.9999040365219116, 0.9991593360900879, 0.9993356466293335, 0.9997928142547607, 0.9998714923858643, 0.9999566078186035, 0.9992215633392334, 0.9999397993087769, 0.9996577501296997, 0.9990214705467224, 0.9997062087059021, 0.9990614652633667, 0.9989033937454224, 0.9983106255531311, 0.9982854723930359, 0.9987096786499023, 0.9998019933700562, 0.9998337626457214, 0.9974377155303955, 0.9999113082885742, 0.9999362230300903, 0.9986217021942139, 0.9996662735939026, 0.9999032616615295, 0.9989407062530518, 0.9985961318016052, 0.9998118877410889, 0.9999643564224243, 0.9990430474281311, 0.9997920989990234, 0.9999125599861145, 0.9992914795875549, 0.9990607500076294, 0.9955571293830872, 0.999916672706604, 0.9993614554405212, 0.9998250007629395, 0.99928218126297, 0.9974948167800903, 0.9993661642074585, 0.9997982978820801, 0.9998220205307007, 0.9997137784957886, 0.9990407228469849, 0.9999068975448608, 0.9998918771743774, 0.9999584555625916, 0.9977982044219971, 0.9999338388442993, 0.9994779825210571, 0.9992092847824097, 0.9997179508209229, 0.9998722076416016, 0.9994766116142273, 0.9997587203979492, 0.9967544674873352, 0.9993805289268494, 0.999939501285553, 0.999560534954071, 0.9998149275779724, 0.9998815059661865, 0.9994146823883057, 0.9998456239700317, 0.9989541172981262, 0.9999141693115234, 0.9993942379951477, 0.9977315068244934, 0.9991326332092285, 0.9999384880065918, 0.9996630549430847, 0.9978988170623779, 0.99932861328125, 0.9998734593391418, 0.9995112419128418, 0.9993993043899536, 0.9995721578598022, 0.9996464252471924, 0.9999522566795349, 0.9996324181556702, 0.9994204044342041, 0.9990990161895752, 0.9992296099662781, 0.9989849328994751, 0.9992744326591492, 0.9990145564079285, 0.999830961227417, 0.9997797012329102, 0.9992918968200684, 0.9999455213546753, 0.9998992085456848, 0.9997338056564331, 0.9989738464355469, 0.9994378089904785, 0.9994591474533081, 0.9995558261871338, 0.9999478459358215, 0.9993120431900024, 0.9996098279953003, 0.9997210502624512, 0.9995197057723999, 0.9998379945755005, 0.9989291429519653, 0.9998633861541748, 0.9998629093170166, 0.9994654655456543, 0.999927282333374, 0.9980363845825195, 0.9999365210533142, 0.9986177086830139, 0.9999127984046936, 0.998847246170044, 0.9990031123161316, 0.9997700452804565, 0.9998974800109863, 0.9996408820152283, 0.9998668432235718, 0.9992098808288574, 0.9995487928390503, 0.9998433589935303, 0.999663233757019, 0.9999321699142456, 0.9988589286804199, 0.9993166923522949, 0.9995048642158508, 0.9994866847991943, 0.9994666576385498, 0.9977390170097351, 0.9996082186698914, 0.9983861446380615, 0.9997461438179016, 0.9998435378074646, 0.9994267225265503, 0.9991494417190552, 0.9999014139175415, 0.9987293481826782, 0.9983997344970703, 0.9991306066513062, 0.9988124370574951, 0.9998514652252197, 0.9990735054016113, 0.9993540048599243, 0.9992901682853699, 0.9995549321174622, 0.9982692003250122, 0.9993511438369751, 0.9999352693557739, 0.9984750747680664, 0.9999673366546631, 0.9991365075111389, 0.999596893787384, 0.9999470710754395, 0.9996280670166016, 0.9996979236602783, 0.9997166395187378, 0.9988670349121094, 0.9982492327690125, 0.9999649524688721, 0.9954934120178223, 0.998736560344696, 0.9999353885650635, 0.9972732067108154, 0.9990154504776001, 0.9994997382164001, 0.9994387030601501, 0.9990947842597961, 0.9980088472366333, 0.9999202489852905, 0.9989234805107117, 0.99977707862854, 0.9997245073318481, 0.9989699125289917, 0.9997355341911316, 0.9995799660682678, 0.9994542598724365, 0.999423086643219, 0.999685525894165, 0.9992594718933105, 0.9999048709869385, 0.9996666312217712, 0.9996203184127808, 0.9998502135276794, 0.9994411468505859, 0.9999295473098755, 0.9995814561843872, 0.9996554851531982, 0.9996663331985474, 0.9994974732398987, 0.9996756315231323, 0.9999043941497803, 0.9990448951721191, 0.999854326248169, 0.999591052532196, 0.998968780040741, 0.9993757605552673, 0.9992529153823853, 0.9994510412216187, 0.9974954724311829, 0.9993399977684021, 0.999397873878479, 0.9982355237007141, 0.9988867044448853, 0.9994837641716003, 0.998806893825531, 0.9998230934143066, 0.9999526739120483, 0.9997866153717041, 0.9994018077850342, 0.9997273683547974, 0.9998487234115601, 0.9994838237762451, 0.9998290538787842, 0.9979223012924194, 0.9999349117279053, 0.9982361793518066, 0.9996524453163147, 0.9996672868728638, 0.9999010562896729, 0.9991448521614075, 0.9994478225708008, 0.9997369647026062, 0.9989660978317261, 0.9992573261260986, 0.9989377856254578, 0.9997876882553101, 0.9998717308044434, 0.9979410767555237, 0.9994720220565796, 0.9998807311058044, 0.9998583197593689, 0.9992992877960205, 0.9998825192451477, 0.9998329877853394, 0.9994198679924011, 0.9999478459358215, 0.9998984336853027, 0.9977718591690063, 0.9990687370300293, 0.9995386004447937, 0.9995219707489014, 0.999624490737915, 0.9997971057891846, 0.9991616010665894, 0.9999269843101501, 0.9988470673561096, 0.9983643293380737, 0.9999529123306274, 0.9981426000595093, 0.9998860359191895, 0.9996789693832397, 0.9999333024024963, 0.99961918592453, 0.9988236427307129, 0.9995331764221191, 0.9999557733535767, 0.9986547231674194]\n",
      "[0.9985295534133911, 0.999220609664917, 0.9990744590759277, 0.9998255372047424, 0.9968363642692566, 0.9998735785484314, 0.9965872764587402, 0.9998489618301392, 0.9992778897285461, 0.9999492764472961, 0.9992676377296448, 0.9998568892478943, 0.99961918592453, 0.999962568283081, 0.9993307590484619, 0.99989914894104, 0.9995089173316956, 0.997880220413208, 0.9986615180969238, 0.9996476173400879, 0.9979598522186279, 0.9999067783355713, 0.9992490410804749, 0.9994243383407593, 0.9990991353988647, 0.9993809461593628, 0.999588131904602, 0.9996837973594666, 0.9994872212409973, 0.9996429085731506, 0.9998804330825806, 0.9994468688964844, 0.9994046688079834, 0.9990503787994385, 0.999634325504303, 0.9999550580978394, 0.9977033734321594, 0.9999575018882751, 0.9991076588630676, 0.9995813965797424, 0.9998540878295898, 0.9999410510063171, 0.999256432056427, 0.9999415874481201, 0.9989866018295288, 0.9995951652526855, 0.9995892643928528, 0.9998476505279541, 0.999415934085846, 0.9999323487281799, 0.9999151229858398, 0.9999375343322754, 0.9996351599693298, 0.9995018839836121, 0.9963566660881042, 0.9991761445999146, 0.9990653991699219, 0.9984236359596252, 0.9995848536491394, 0.9997960925102234, 0.9994616508483887, 0.999086320400238, 0.9998283386230469, 0.9998841285705566, 0.9997621774673462, 0.9995666146278381, 0.9998943209648132, 0.9998921155929565, 0.9975546002388, 0.9991934299468994, 0.9994878768920898, 0.9998306632041931, 0.9995894432067871, 0.9999487996101379, 0.9997281432151794, 0.9994660019874573, 0.999409019947052, 0.9998080134391785, 0.9972062110900879, 0.9994944334030151, 0.9966578483581543, 0.9978210926055908, 0.999847948551178, 0.9995694160461426, 0.999671995639801, 0.9993815422058105, 0.9997988343238831, 0.9998317360877991, 0.9997950792312622, 0.9996095299720764, 0.999943733215332, 0.9999066591262817, 0.9996033906936646, 0.9998104572296143, 0.9999426603317261, 0.9984120726585388, 0.9993650913238525, 0.999901533126831, 0.9998127222061157, 0.9993182420730591, 0.9995757341384888, 0.9969311952590942, 0.9969840049743652, 0.9999311566352844, 0.9996316432952881, 0.999822199344635, 0.9994428157806396, 0.9996027946472168, 0.9992322325706482, 0.9990724921226501, 0.9995278120040894, 0.9997441172599792, 0.9995719790458679, 0.9998315572738647, 0.9996796250343323, 0.9997358322143555, 0.9994016289710999, 0.99983149766922, 0.9997121095657349, 0.9997824430465698, 0.9999490976333618, 0.9993529319763184, 0.9998617768287659, 0.9995980262756348, 0.9998630285263062, 0.9992793798446655, 0.9999023675918579, 0.999805212020874, 0.9997535347938538, 0.998921811580658, 0.9998030066490173, 0.9999282360076904, 0.9990379214286804, 0.9999150633811951, 0.9998344779014587, 0.9973016977310181, 0.9993919134140015, 0.9998666644096375, 0.9998666644096375, 0.9993000030517578, 0.9992864727973938, 0.9997701644897461, 0.9996265769004822, 0.9999538064002991, 0.9998669028282166, 0.9967842102050781, 0.9983761310577393, 0.9997917413711548, 0.9995917677879333, 0.9968294501304626, 0.9998998045921326, 0.9992737770080566, 0.9973049163818359, 0.99993896484375, 0.9998389482498169, 0.9992395043373108, 0.9990764856338501, 0.9994420409202576, 0.9983032941818237, 0.9997320175170898, 0.9975759387016296, 0.9998491406440735, 0.9993956685066223, 0.9996107816696167, 0.9994694590568542, 0.9991141557693481, 0.9992952346801758, 0.9980058670043945, 0.9997895359992981, 0.9992278814315796, 0.9998641014099121, 0.9998571276664734, 0.9972913861274719, 0.999255359172821, 0.9967291355133057, 0.9998325109481812, 0.9999561309814453, 0.9996318221092224, 0.999369204044342, 0.9960887432098389, 0.9999561309814453, 0.999893069267273, 0.9997794032096863, 0.9999482035636902, 0.9993011951446533, 0.998837411403656, 0.999812126159668, 0.9994077682495117, 0.9998493194580078, 0.9994962215423584, 0.9992836117744446, 0.9989330172538757, 0.9999238848686218, 0.9999457597732544, 0.9963788986206055, 0.997204601764679, 0.9989885091781616, 0.9995599985122681, 0.9993754625320435, 0.9998443126678467, 0.9997690916061401, 0.9991071224212646, 0.99946129322052, 0.9999363422393799, 0.9991908669471741, 0.9994961023330688, 0.9989814162254333, 0.9993952512741089, 0.9988164901733398, 0.9998830556869507, 0.9995170831680298, 0.9990478754043579, 0.9999448657035828, 0.9990527629852295, 0.9993131160736084, 0.9995107054710388, 0.9995760321617126, 0.9998817443847656, 0.9993547201156616, 0.9999222159385681, 0.9992532730102539, 0.99994295835495, 0.9993573427200317, 0.999834418296814, 0.9975813627243042, 0.9999219179153442, 0.9996682405471802, 0.9998070001602173, 0.9999535083770752, 0.9998815655708313, 0.9996088743209839, 0.9999394416809082, 0.9993329644203186, 0.9999405741691589, 0.999933123588562, 0.9999607801437378, 0.9998289346694946, 0.9999547004699707, 0.9999264478683472, 0.9998911619186401, 0.9997669458389282, 0.9989088177680969, 0.9997636675834656, 0.9995803833007812, 0.9996117949485779, 0.9999284148216248, 0.9989424347877502, 0.9997435808181763, 0.9994442462921143, 0.9988508224487305, 0.9998549818992615, 0.9986300468444824, 0.9996461868286133, 0.9952598214149475, 0.9998693466186523, 0.9996862411499023]\n",
      "[0.9999257326126099, 0.9995620846748352, 0.9957664012908936, 0.9998456835746765, 0.9998332262039185, 0.9993183612823486, 0.9982277154922485, 0.998685896396637, 0.9992634654045105, 0.9977641701698303, 0.9996463060379028, 0.9989795684814453, 0.9993956089019775, 0.9974222183227539, 0.9993320107460022, 0.9989303946495056, 0.9997495412826538, 0.9996365308761597, 0.9996728301048279, 0.9993361234664917, 0.9994163513183594, 0.9998196363449097, 0.9994492530822754, 0.9998331069946289, 0.9998890161514282, 0.9996587634086609, 0.9998972415924072, 0.9994007349014282, 0.9998139142990112, 0.9989796876907349, 0.9994714260101318, 0.9999340176582336, 0.9977403879165649, 0.9995875358581543, 0.9992088079452515, 0.9994247555732727, 0.9998959302902222, 0.9968935251235962, 0.998059093952179, 0.9994392395019531, 0.9999498128890991, 0.9999546408653259, 0.9995462894439697, 0.9999385476112366, 0.9998747110366821, 0.9992470741271973, 0.999893069267273, 0.999919056892395, 0.999908447265625, 0.9998786449432373, 0.9989558458328247, 0.9986007213592529, 0.9999353289604187, 0.9995884895324707, 0.9991861581802368, 0.9995484352111816, 0.9991593360900879, 0.9999496340751648, 0.9996026754379272, 0.9994944334030151, 0.9996243119239807, 0.9999192953109741, 0.9989951848983765, 0.9994226694107056, 0.9998305439949036, 0.999727189540863, 0.9997212886810303, 0.9993695616722107, 0.9996936321258545, 0.9998582601547241, 0.9992760419845581, 0.9965696334838867, 0.9993329644203186, 0.9993290901184082, 0.9998950958251953, 0.9984456300735474, 0.9999148845672607, 0.9990937113761902, 0.9972683787345886, 0.9987960457801819, 0.9994348883628845, 0.9999340772628784, 0.9981384873390198, 0.999929666519165, 0.9988939166069031, 0.9999480247497559, 0.9997527599334717, 0.9989669322967529, 0.9994913935661316, 0.9996360540390015, 0.9980387091636658, 0.9996508359909058, 0.9998144507408142, 0.9998394846916199, 0.9998947381973267, 0.999193549156189, 0.9999340772628784, 0.9991786479949951, 0.9992066621780396, 0.999933660030365, 0.9988694190979004, 0.9996691942214966, 0.9997640252113342, 0.9990280270576477, 0.9998531937599182, 0.9999474287033081, 0.9995419979095459, 0.9995740056037903, 0.9993701577186584, 0.9998412132263184, 0.9996534585952759, 0.9977469444274902, 0.9998835921287537, 0.9992345571517944, 0.9996249675750732, 0.999576985836029, 0.9995822906494141, 0.9990242719650269, 0.9979970455169678, 0.9969408512115479, 0.9998767971992493, 0.9996141195297241, 0.9988067150115967, 0.9999566078186035, 0.9998787641525269, 0.9997930526733398, 0.9999316334724426, 0.9984710216522217, 0.9990481734275818, 0.9994511604309082, 0.9999297857284546, 0.9998244047164917, 0.999925971031189, 0.9999229907989502, 0.9983376264572144, 0.999444842338562, 0.9999304413795471, 0.9997695088386536, 0.9996150732040405, 0.9998579621315002, 0.9998376369476318, 0.999942421913147, 0.9995732307434082, 0.9987567663192749, 0.9999391436576843, 0.9996145963668823, 0.999753475189209, 0.9996116161346436, 0.9998388290405273, 0.9994493722915649, 0.9965447783470154, 0.9999589920043945, 0.9996204376220703, 0.9998914003372192, 0.999770998954773, 0.9999640583992004, 0.9991493225097656, 0.9969947338104248, 0.9985467791557312, 0.9986927509307861, 0.9995057582855225, 0.9994289875030518, 0.99933260679245, 0.9993157386779785, 0.9999358654022217, 0.9996765851974487, 0.9991025924682617, 0.99989253282547, 0.9998356699943542, 0.9991761445999146, 0.9999300837516785, 0.9986569881439209, 0.9982666969299316, 0.9991024136543274, 0.9977970123291016, 0.9997637271881104, 0.9999359846115112, 0.9996610879898071, 0.9999319911003113, 0.9999138712882996, 0.9983376264572144, 0.9982528686523438, 0.9999412298202515, 0.9995919466018677, 0.998909592628479, 0.9980408549308777, 0.9992817640304565, 0.9987226724624634, 0.9999217987060547, 0.9996480941772461, 0.9981109499931335, 0.9996916055679321, 0.999798595905304, 0.9995423555374146, 0.9980577230453491, 0.999849796295166, 0.9998739361763, 0.9998089671134949, 0.9954239130020142, 0.9999638795852661, 0.9990407228469849, 0.9999167323112488, 0.9998032450675964, 0.9996041655540466, 0.9994633197784424, 0.9999550580978394, 0.9995557069778442, 0.9999518394470215, 0.9994495511054993, 0.9996146559715271, 0.9993867874145508, 0.9996290802955627, 0.9992719888687134, 0.9999420046806335, 0.9984610676765442, 0.9997968077659607, 0.9998458027839661, 0.9998084306716919, 0.9993681907653809, 0.9998458623886108, 0.9981271028518677, 0.9966863393783569, 0.9993463754653931, 0.9993917346000671, 0.9997618198394775, 0.9998185038566589, 0.9999455213546753, 0.9982801079750061, 0.999832808971405, 0.9999558329582214, 0.9978634715080261, 0.9999534487724304, 0.9985580444335938, 0.9999305009841919, 0.9987517595291138, 0.9998377561569214, 0.9992889165878296, 0.9990904331207275, 0.9995280504226685, 0.9999221563339233, 0.9994571805000305, 0.9999270439147949, 0.9992638230323792, 0.9998524785041809, 0.9999363422393799, 0.9998159408569336, 0.9999203681945801, 0.9997753500938416, 0.9994717836380005, 0.9999616146087646, 0.998996913433075, 0.9997017979621887, 0.9990819692611694, 0.9993586540222168, 0.9991345405578613, 0.9990522861480713]\n",
      "[0.9995197057723999, 0.9964510202407837, 0.9998021125793457, 0.9991798996925354, 0.9994147419929504, 0.9997718930244446, 0.997718095779419, 0.9991139769554138, 0.9996596574783325, 0.9989155530929565, 0.9996134042739868, 0.9995113611221313, 0.9986599087715149, 0.9990609884262085, 0.9993547797203064, 0.999699592590332, 0.999599814414978, 0.9998368620872498, 0.9994195699691772, 0.9991545081138611, 0.9997420907020569, 0.9988631010055542, 0.9998432397842407, 0.999122142791748, 0.9995472431182861, 0.9992479681968689, 0.9997356534004211, 0.9999257326126099, 0.9999427795410156, 0.9964830875396729, 0.9999178647994995, 0.999274492263794, 0.9998120069503784, 0.9995676875114441, 0.9995896816253662, 0.9965208172798157, 0.9973626136779785, 0.9983122944831848, 0.9998915791511536, 0.9993287324905396, 0.9999188184738159, 0.9998065233230591, 0.9999383687973022, 0.9989391565322876, 0.9994556903839111, 0.9998387098312378, 0.9994415640830994, 0.9998751282691956, 0.9999318718910217, 0.9996675252914429, 0.9983053803443909, 0.9999609589576721, 0.9999457597732544, 0.99888676404953, 0.9982482194900513, 0.9999312162399292, 0.9996119737625122, 0.9994343519210815, 0.9999161958694458, 0.9984912872314453, 0.9999169111251831, 0.9993285536766052, 0.9982523918151855, 0.9996891617774963, 0.9976300001144409, 0.9992460012435913, 0.9990531206130981, 0.9995476007461548, 0.9989782571792603, 0.9984946846961975, 0.999863862991333, 0.9999075531959534, 0.9999128580093384, 0.9995012283325195, 0.999754011631012, 0.9999158382415771, 0.9999418258666992, 0.9991534352302551, 0.999199628829956, 0.9998818039894104, 0.9994373917579651, 0.9992743730545044, 0.9998037219047546, 0.9993962049484253, 0.9998472929000854, 0.9994719624519348, 0.9999308586120605, 0.9994000792503357, 0.9998916387557983, 0.9984843134880066, 0.9995217323303223, 0.9994633793830872, 0.9999459981918335, 0.9999628663063049, 0.9997074007987976, 0.9998359084129333, 0.9999059438705444, 0.9979031085968018, 0.9993194341659546, 0.999915599822998, 0.9993908405303955, 0.9988902807235718, 0.9999055862426758, 0.9994891881942749, 0.9996536374092102, 0.9999222755432129, 0.9994449615478516, 0.9996228218078613, 0.9996897578239441, 0.9998311400413513, 0.9995008707046509, 0.9998538494110107, 0.9998985528945923, 0.9999128580093384, 0.9999030828475952, 0.9995932579040527, 0.9992661476135254, 0.9995680451393127, 0.9998360872268677, 0.9998415112495422, 0.9975154399871826, 0.9999568462371826, 0.9995460510253906, 0.999721348285675, 0.9985154867172241, 0.9999607801437378, 0.9979039430618286, 0.9999058842658997, 0.9994379281997681, 0.9995127320289612, 0.9998217821121216, 0.999664843082428, 0.9987037181854248, 0.9984477758407593, 0.9994385838508606, 0.9993911981582642, 0.9998342990875244, 0.9999561905860901, 0.9997108578681946, 0.9995442032814026, 0.9999488592147827, 0.9994727373123169, 0.9985438585281372, 0.9998400211334229, 0.9999024868011475, 0.9997460246086121, 0.9994918704032898, 0.9997562766075134, 0.9994246363639832, 0.9979190230369568, 0.9998437762260437, 0.9990060329437256, 0.9998029470443726, 0.9996757507324219, 0.9992437362670898, 0.9962378740310669, 0.9995744228363037, 0.9995400905609131, 0.99919193983078, 0.9999279975891113, 0.9998332262039185, 0.9982457160949707, 0.9991037845611572, 0.999941349029541, 0.9995232224464417, 0.999453604221344, 0.9999349117279053, 0.9985133409500122, 0.9993414878845215, 0.9995208978652954, 0.997668981552124, 0.9996280670166016, 0.9998152852058411, 0.9999388456344604, 0.9990111589431763, 0.9966729879379272, 0.999912440776825, 0.9989064335823059, 0.9985774755477905, 0.9996708631515503, 0.9997951984405518, 0.9997849464416504, 0.9994484782218933, 0.9990023970603943, 0.9991067051887512, 0.9999347925186157, 0.9999505281448364, 0.9998950958251953, 0.9999100565910339, 0.9996891617774963, 0.9994217157363892, 0.998308002948761, 0.999642014503479, 0.998236894607544, 0.999872624874115, 0.9993693232536316, 0.9998182058334351, 0.9998902082443237, 0.9980883002281189, 0.9998224377632141, 0.999066948890686, 0.9998708963394165, 0.9973604679107666, 0.9995761513710022, 0.9996490478515625, 0.99986732006073, 0.9997052550315857, 0.9999386072158813, 0.9997945427894592, 0.999860942363739, 0.9998779892921448, 0.9996669292449951, 0.9999696612358093, 0.99972003698349, 0.9996249675750732, 0.9989522099494934, 0.9992644190788269, 0.9951953291893005, 0.999931275844574, 0.999319851398468, 0.9999260306358337, 0.9996200799942017, 0.999178409576416, 0.9996442794799805, 0.9999001622200012, 0.9998756051063538, 0.9996158480644226, 0.9998427629470825, 0.9998911023139954, 0.9969815611839294, 0.9981091618537903, 0.999930202960968, 0.9999015927314758, 0.998767077922821]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "279"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = prediction(model, test_loader, device)\n",
    "sum(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fecf184-57e8-4969-bce1-46da090bdd32",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "05157612-68bf-4102-a723-8c0ad0f76755",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv(path+'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "38b5a428-5c17-4056-a3fe-4f50e30412ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAMPLE_ID</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_0001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_0002</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_0003</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_0004</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1509</th>\n",
       "      <td>TEST_1509</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1510</th>\n",
       "      <td>TEST_1510</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1511</th>\n",
       "      <td>TEST_1511</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1512</th>\n",
       "      <td>TEST_1512</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1513</th>\n",
       "      <td>TEST_1513</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1514 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      SAMPLE_ID  LABEL\n",
       "0     TEST_0000      0\n",
       "1     TEST_0001      0\n",
       "2     TEST_0002      0\n",
       "3     TEST_0003      1\n",
       "4     TEST_0004      0\n",
       "...         ...    ...\n",
       "1509  TEST_1509      1\n",
       "1510  TEST_1510      1\n",
       "1511  TEST_1511      0\n",
       "1512  TEST_1512      0\n",
       "1513  TEST_1513      1\n",
       "\n",
       "[1514 rows x 2 columns]"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit['LABEL'] = preds\n",
    "submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "6c1c8567-eacd-4de3-b5e2-490102224edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('submit.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "2fe3fa16-acc3-4628-a401-bc752544e41b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "279"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit['LABEL'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731ccaef-e266-4079-8101-126bb441a031",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17c3583-8594-431b-849c-19592dabbd2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
