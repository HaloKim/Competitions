{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2bcba5f-002e-4f49-9622-ada6117faf0a",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b0d9b68-7102-4eca-9543-3b9b8acafc6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hist\\miniconda3\\envs\\ml\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import librosa\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from scipy.spatial import distance\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba7851e-f369-4b1b-b5e3-b45233535157",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7c0d625-02ae-42bf-b87c-bc60423e2284",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1000\n",
    "LR = 1e-1\n",
    "SR = 16000\n",
    "SEED = 42\n",
    "N_MFCC = 128\n",
    "BATCH = 256\n",
    "device = 'cuda'\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(SEED) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a4172e-5791-446f-9616-35c09d8bf25a",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a62c78cd-4f40-4e98-b8a6-1b6f1d906b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'open/'\n",
    "train_df = pd.read_csv(path+'train.csv') # 모두 정상 Sample\n",
    "test_df = pd.read_csv(path+'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33f37236-257c-44ab-8a9f-b2daf6c50ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAMPLE_ID</th>\n",
       "      <th>SAMPLE_PATH</th>\n",
       "      <th>FAN_TYPE</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>./train/TRAIN_0000.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>./train/TRAIN_0001.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>./train/TRAIN_0002.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>./train/TRAIN_0003.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>./train/TRAIN_0004.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274</th>\n",
       "      <td>TRAIN_1274</td>\n",
       "      <td>./train/TRAIN_1274.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275</th>\n",
       "      <td>TRAIN_1275</td>\n",
       "      <td>./train/TRAIN_1275.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1276</th>\n",
       "      <td>TRAIN_1276</td>\n",
       "      <td>./train/TRAIN_1276.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1277</th>\n",
       "      <td>TRAIN_1277</td>\n",
       "      <td>./train/TRAIN_1277.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1278</th>\n",
       "      <td>TRAIN_1278</td>\n",
       "      <td>./train/TRAIN_1278.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1279 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       SAMPLE_ID             SAMPLE_PATH  FAN_TYPE  LABEL\n",
       "0     TRAIN_0000  ./train/TRAIN_0000.wav         2      0\n",
       "1     TRAIN_0001  ./train/TRAIN_0001.wav         0      0\n",
       "2     TRAIN_0002  ./train/TRAIN_0002.wav         0      0\n",
       "3     TRAIN_0003  ./train/TRAIN_0003.wav         2      0\n",
       "4     TRAIN_0004  ./train/TRAIN_0004.wav         2      0\n",
       "...          ...                     ...       ...    ...\n",
       "1274  TRAIN_1274  ./train/TRAIN_1274.wav         2      0\n",
       "1275  TRAIN_1275  ./train/TRAIN_1275.wav         2      0\n",
       "1276  TRAIN_1276  ./train/TRAIN_1276.wav         2      0\n",
       "1277  TRAIN_1277  ./train/TRAIN_1277.wav         2      0\n",
       "1278  TRAIN_1278  ./train/TRAIN_1278.wav         0      0\n",
       "\n",
       "[1279 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee800463-995c-43ac-a05d-f3988923add9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mfcc_feature(df):\n",
    "    features = []\n",
    "    features2 = []\n",
    "    for path in tqdm(df['SAMPLE_PATH']):\n",
    "        # librosa패키지를 사용하여 wav 파일 load\n",
    "        y, sr = librosa.load(path, sr=SR)\n",
    "        \n",
    "        # melspectrogram\n",
    "        mels = librosa.feature.melspectrogram(y, sr=sr, n_mels=N_MFCC)\n",
    "        mels = librosa.power_to_db(mels, ref=np.max)\n",
    "        \n",
    "        # librosa패키지를 사용하여 mfcc 추출\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=N_MFCC)\n",
    "        \n",
    "        y_feature2 = []\n",
    "        # 추출된 MFCC들의 평균을 Feature로 사용\n",
    "        for e in mels:\n",
    "            y_feature2.append(np.mean(e))\n",
    "        features2.append(y_feature2)\n",
    "        \n",
    "        y_feature = []\n",
    "        # 추출된 MFCC들의 평균을 Feature로 사용\n",
    "        for e in mfcc:\n",
    "            y_feature.append(np.mean(e))\n",
    "        features.append(y_feature)\n",
    "    return features, features2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5b4be22-d948-4407-afe9-cf16c4281826",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1279/1279 [00:31<00:00, 40.11it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1514/1514 [00:38<00:00, 39.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 24s\n",
      "Wall time: 1min 10s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_features, train_features2 = get_mfcc_feature(train_df)\n",
    "test_features, test_features2 = get_mfcc_feature(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c86bd05-d873-4943-885c-b7efffe41048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 반복문으로 전체 컬럼명 변경하기\n",
    "def rename(df):\n",
    "    flag = 0\n",
    "    for col_name in df.columns:\n",
    "        if col_name == 0:\n",
    "            flag = 1\n",
    "        if flag == 1:\n",
    "            df.rename(columns = {col_name : 128+col_name}, inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03d6602e-38a0-4a7e-99b4-ba10b6d03d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAMPLE_ID</th>\n",
       "      <th>SAMPLE_PATH</th>\n",
       "      <th>FAN_TYPE</th>\n",
       "      <th>LABEL</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>...</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>./train/TRAIN_0000.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.952273</td>\n",
       "      <td>0.284621</td>\n",
       "      <td>0.295843</td>\n",
       "      <td>0.363586</td>\n",
       "      <td>0.357530</td>\n",
       "      <td>0.294327</td>\n",
       "      <td>...</td>\n",
       "      <td>0.833846</td>\n",
       "      <td>0.842291</td>\n",
       "      <td>0.818590</td>\n",
       "      <td>0.817896</td>\n",
       "      <td>0.799984</td>\n",
       "      <td>0.799645</td>\n",
       "      <td>0.811261</td>\n",
       "      <td>0.816584</td>\n",
       "      <td>0.825692</td>\n",
       "      <td>0.805742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>./train/TRAIN_0001.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.081981</td>\n",
       "      <td>0.935459</td>\n",
       "      <td>0.514880</td>\n",
       "      <td>0.613406</td>\n",
       "      <td>0.691659</td>\n",
       "      <td>0.825306</td>\n",
       "      <td>...</td>\n",
       "      <td>0.148846</td>\n",
       "      <td>0.144862</td>\n",
       "      <td>0.149996</td>\n",
       "      <td>0.165012</td>\n",
       "      <td>0.156853</td>\n",
       "      <td>0.160930</td>\n",
       "      <td>0.163661</td>\n",
       "      <td>0.162874</td>\n",
       "      <td>0.158788</td>\n",
       "      <td>0.084692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>./train/TRAIN_0002.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.240260</td>\n",
       "      <td>0.664368</td>\n",
       "      <td>0.724483</td>\n",
       "      <td>0.354632</td>\n",
       "      <td>0.625930</td>\n",
       "      <td>0.695975</td>\n",
       "      <td>...</td>\n",
       "      <td>0.454258</td>\n",
       "      <td>0.520886</td>\n",
       "      <td>0.454087</td>\n",
       "      <td>0.465858</td>\n",
       "      <td>0.423989</td>\n",
       "      <td>0.497721</td>\n",
       "      <td>0.504808</td>\n",
       "      <td>0.502295</td>\n",
       "      <td>0.487844</td>\n",
       "      <td>0.409240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>./train/TRAIN_0003.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.943729</td>\n",
       "      <td>0.295295</td>\n",
       "      <td>0.312391</td>\n",
       "      <td>0.371455</td>\n",
       "      <td>0.329344</td>\n",
       "      <td>0.268686</td>\n",
       "      <td>...</td>\n",
       "      <td>0.808817</td>\n",
       "      <td>0.827032</td>\n",
       "      <td>0.815166</td>\n",
       "      <td>0.787913</td>\n",
       "      <td>0.779337</td>\n",
       "      <td>0.781981</td>\n",
       "      <td>0.771998</td>\n",
       "      <td>0.779565</td>\n",
       "      <td>0.796365</td>\n",
       "      <td>0.798277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>./train/TRAIN_0004.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.949608</td>\n",
       "      <td>0.188729</td>\n",
       "      <td>0.179787</td>\n",
       "      <td>0.154144</td>\n",
       "      <td>0.007228</td>\n",
       "      <td>0.055841</td>\n",
       "      <td>...</td>\n",
       "      <td>0.877542</td>\n",
       "      <td>0.909439</td>\n",
       "      <td>0.894097</td>\n",
       "      <td>0.863824</td>\n",
       "      <td>0.849838</td>\n",
       "      <td>0.867351</td>\n",
       "      <td>0.865360</td>\n",
       "      <td>0.861068</td>\n",
       "      <td>0.868706</td>\n",
       "      <td>0.875541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274</th>\n",
       "      <td>TRAIN_1274</td>\n",
       "      <td>./train/TRAIN_1274.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.964989</td>\n",
       "      <td>0.096119</td>\n",
       "      <td>0.129592</td>\n",
       "      <td>0.222531</td>\n",
       "      <td>0.039493</td>\n",
       "      <td>0.075616</td>\n",
       "      <td>...</td>\n",
       "      <td>0.857574</td>\n",
       "      <td>0.875432</td>\n",
       "      <td>0.862135</td>\n",
       "      <td>0.853636</td>\n",
       "      <td>0.852105</td>\n",
       "      <td>0.854758</td>\n",
       "      <td>0.852087</td>\n",
       "      <td>0.851688</td>\n",
       "      <td>0.860304</td>\n",
       "      <td>0.881918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275</th>\n",
       "      <td>TRAIN_1275</td>\n",
       "      <td>./train/TRAIN_1275.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.959506</td>\n",
       "      <td>0.283203</td>\n",
       "      <td>0.293961</td>\n",
       "      <td>0.390142</td>\n",
       "      <td>0.322176</td>\n",
       "      <td>0.239990</td>\n",
       "      <td>...</td>\n",
       "      <td>0.842018</td>\n",
       "      <td>0.853391</td>\n",
       "      <td>0.822730</td>\n",
       "      <td>0.825484</td>\n",
       "      <td>0.822317</td>\n",
       "      <td>0.809976</td>\n",
       "      <td>0.820805</td>\n",
       "      <td>0.835689</td>\n",
       "      <td>0.836974</td>\n",
       "      <td>0.819587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1276</th>\n",
       "      <td>TRAIN_1276</td>\n",
       "      <td>./train/TRAIN_1276.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.930908</td>\n",
       "      <td>0.223856</td>\n",
       "      <td>0.210020</td>\n",
       "      <td>0.151861</td>\n",
       "      <td>0.134018</td>\n",
       "      <td>0.149416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.918239</td>\n",
       "      <td>0.932157</td>\n",
       "      <td>0.898576</td>\n",
       "      <td>0.905386</td>\n",
       "      <td>0.894843</td>\n",
       "      <td>0.879353</td>\n",
       "      <td>0.900221</td>\n",
       "      <td>0.910439</td>\n",
       "      <td>0.903620</td>\n",
       "      <td>0.895840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1277</th>\n",
       "      <td>TRAIN_1277</td>\n",
       "      <td>./train/TRAIN_1277.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.932890</td>\n",
       "      <td>0.247222</td>\n",
       "      <td>0.265837</td>\n",
       "      <td>0.223214</td>\n",
       "      <td>0.170546</td>\n",
       "      <td>0.141445</td>\n",
       "      <td>...</td>\n",
       "      <td>0.922436</td>\n",
       "      <td>0.946884</td>\n",
       "      <td>0.920099</td>\n",
       "      <td>0.916849</td>\n",
       "      <td>0.900980</td>\n",
       "      <td>0.903257</td>\n",
       "      <td>0.919087</td>\n",
       "      <td>0.922190</td>\n",
       "      <td>0.920950</td>\n",
       "      <td>0.932029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1278</th>\n",
       "      <td>TRAIN_1278</td>\n",
       "      <td>./train/TRAIN_1278.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.407188</td>\n",
       "      <td>0.620077</td>\n",
       "      <td>0.356668</td>\n",
       "      <td>0.769474</td>\n",
       "      <td>0.548411</td>\n",
       "      <td>0.806605</td>\n",
       "      <td>...</td>\n",
       "      <td>0.306191</td>\n",
       "      <td>0.317135</td>\n",
       "      <td>0.329551</td>\n",
       "      <td>0.308712</td>\n",
       "      <td>0.285871</td>\n",
       "      <td>0.289114</td>\n",
       "      <td>0.280076</td>\n",
       "      <td>0.270611</td>\n",
       "      <td>0.257248</td>\n",
       "      <td>0.177652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1279 rows × 260 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       SAMPLE_ID             SAMPLE_PATH  FAN_TYPE  LABEL       128       129  \\\n",
       "0     TRAIN_0000  ./train/TRAIN_0000.wav         2      0  0.952273  0.284621   \n",
       "1     TRAIN_0001  ./train/TRAIN_0001.wav         0      0  0.081981  0.935459   \n",
       "2     TRAIN_0002  ./train/TRAIN_0002.wav         0      0  0.240260  0.664368   \n",
       "3     TRAIN_0003  ./train/TRAIN_0003.wav         2      0  0.943729  0.295295   \n",
       "4     TRAIN_0004  ./train/TRAIN_0004.wav         2      0  0.949608  0.188729   \n",
       "...          ...                     ...       ...    ...       ...       ...   \n",
       "1274  TRAIN_1274  ./train/TRAIN_1274.wav         2      0  0.964989  0.096119   \n",
       "1275  TRAIN_1275  ./train/TRAIN_1275.wav         2      0  0.959506  0.283203   \n",
       "1276  TRAIN_1276  ./train/TRAIN_1276.wav         2      0  0.930908  0.223856   \n",
       "1277  TRAIN_1277  ./train/TRAIN_1277.wav         2      0  0.932890  0.247222   \n",
       "1278  TRAIN_1278  ./train/TRAIN_1278.wav         0      0  0.407188  0.620077   \n",
       "\n",
       "           130       131       132       133  ...       118       119  \\\n",
       "0     0.295843  0.363586  0.357530  0.294327  ...  0.833846  0.842291   \n",
       "1     0.514880  0.613406  0.691659  0.825306  ...  0.148846  0.144862   \n",
       "2     0.724483  0.354632  0.625930  0.695975  ...  0.454258  0.520886   \n",
       "3     0.312391  0.371455  0.329344  0.268686  ...  0.808817  0.827032   \n",
       "4     0.179787  0.154144  0.007228  0.055841  ...  0.877542  0.909439   \n",
       "...        ...       ...       ...       ...  ...       ...       ...   \n",
       "1274  0.129592  0.222531  0.039493  0.075616  ...  0.857574  0.875432   \n",
       "1275  0.293961  0.390142  0.322176  0.239990  ...  0.842018  0.853391   \n",
       "1276  0.210020  0.151861  0.134018  0.149416  ...  0.918239  0.932157   \n",
       "1277  0.265837  0.223214  0.170546  0.141445  ...  0.922436  0.946884   \n",
       "1278  0.356668  0.769474  0.548411  0.806605  ...  0.306191  0.317135   \n",
       "\n",
       "           120       121       122       123       124       125       126  \\\n",
       "0     0.818590  0.817896  0.799984  0.799645  0.811261  0.816584  0.825692   \n",
       "1     0.149996  0.165012  0.156853  0.160930  0.163661  0.162874  0.158788   \n",
       "2     0.454087  0.465858  0.423989  0.497721  0.504808  0.502295  0.487844   \n",
       "3     0.815166  0.787913  0.779337  0.781981  0.771998  0.779565  0.796365   \n",
       "4     0.894097  0.863824  0.849838  0.867351  0.865360  0.861068  0.868706   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1274  0.862135  0.853636  0.852105  0.854758  0.852087  0.851688  0.860304   \n",
       "1275  0.822730  0.825484  0.822317  0.809976  0.820805  0.835689  0.836974   \n",
       "1276  0.898576  0.905386  0.894843  0.879353  0.900221  0.910439  0.903620   \n",
       "1277  0.920099  0.916849  0.900980  0.903257  0.919087  0.922190  0.920950   \n",
       "1278  0.329551  0.308712  0.285871  0.289114  0.280076  0.270611  0.257248   \n",
       "\n",
       "           127  \n",
       "0     0.805742  \n",
       "1     0.084692  \n",
       "2     0.409240  \n",
       "3     0.798277  \n",
       "4     0.875541  \n",
       "...        ...  \n",
       "1274  0.881918  \n",
       "1275  0.819587  \n",
       "1276  0.895840  \n",
       "1277  0.932029  \n",
       "1278  0.177652  \n",
       "\n",
       "[1279 rows x 260 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = pd.concat([train_df, pd.DataFrame(train_features)], axis=1)\n",
    "tmp = rename(tmp)\n",
    "tmp = pd.concat([tmp, pd.DataFrame(train_features2)], axis=1)\n",
    "\n",
    "test = pd.concat([test_df, pd.DataFrame(test_features)], axis=1)\n",
    "test = rename(test)\n",
    "test = pd.concat([test, pd.DataFrame(test_features2)], axis=1)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "tmp.iloc[:,range(4,len(tmp.columns))] = scaler.fit_transform(tmp.iloc[:,range(4,len(tmp.columns))])\n",
    "test.iloc[:,range(3,len(test.columns))] = scaler.transform(test.iloc[:,range(3,len(test.columns))])\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09100fa5-d563-42aa-9575-1798d8ebbd40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAMPLE_ID</th>\n",
       "      <th>SAMPLE_PATH</th>\n",
       "      <th>FAN_TYPE</th>\n",
       "      <th>LABEL</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>...</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>./train/TRAIN_0000.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.952273</td>\n",
       "      <td>0.284621</td>\n",
       "      <td>0.295843</td>\n",
       "      <td>0.363586</td>\n",
       "      <td>0.357530</td>\n",
       "      <td>0.294327</td>\n",
       "      <td>...</td>\n",
       "      <td>0.833846</td>\n",
       "      <td>0.842291</td>\n",
       "      <td>0.818590</td>\n",
       "      <td>0.817896</td>\n",
       "      <td>0.799984</td>\n",
       "      <td>0.799645</td>\n",
       "      <td>0.811261</td>\n",
       "      <td>0.816584</td>\n",
       "      <td>0.825692</td>\n",
       "      <td>0.805742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>./train/TRAIN_0001.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.081981</td>\n",
       "      <td>0.935459</td>\n",
       "      <td>0.514880</td>\n",
       "      <td>0.613406</td>\n",
       "      <td>0.691659</td>\n",
       "      <td>0.825306</td>\n",
       "      <td>...</td>\n",
       "      <td>0.148846</td>\n",
       "      <td>0.144862</td>\n",
       "      <td>0.149996</td>\n",
       "      <td>0.165012</td>\n",
       "      <td>0.156853</td>\n",
       "      <td>0.160930</td>\n",
       "      <td>0.163661</td>\n",
       "      <td>0.162874</td>\n",
       "      <td>0.158788</td>\n",
       "      <td>0.084692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>./train/TRAIN_0002.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.240260</td>\n",
       "      <td>0.664368</td>\n",
       "      <td>0.724483</td>\n",
       "      <td>0.354632</td>\n",
       "      <td>0.625930</td>\n",
       "      <td>0.695975</td>\n",
       "      <td>...</td>\n",
       "      <td>0.454258</td>\n",
       "      <td>0.520886</td>\n",
       "      <td>0.454087</td>\n",
       "      <td>0.465858</td>\n",
       "      <td>0.423989</td>\n",
       "      <td>0.497721</td>\n",
       "      <td>0.504808</td>\n",
       "      <td>0.502295</td>\n",
       "      <td>0.487844</td>\n",
       "      <td>0.409240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>./train/TRAIN_0003.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.943729</td>\n",
       "      <td>0.295295</td>\n",
       "      <td>0.312391</td>\n",
       "      <td>0.371455</td>\n",
       "      <td>0.329344</td>\n",
       "      <td>0.268686</td>\n",
       "      <td>...</td>\n",
       "      <td>0.808817</td>\n",
       "      <td>0.827032</td>\n",
       "      <td>0.815166</td>\n",
       "      <td>0.787913</td>\n",
       "      <td>0.779337</td>\n",
       "      <td>0.781981</td>\n",
       "      <td>0.771998</td>\n",
       "      <td>0.779565</td>\n",
       "      <td>0.796365</td>\n",
       "      <td>0.798277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>./train/TRAIN_0004.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.949608</td>\n",
       "      <td>0.188729</td>\n",
       "      <td>0.179787</td>\n",
       "      <td>0.154144</td>\n",
       "      <td>0.007228</td>\n",
       "      <td>0.055841</td>\n",
       "      <td>...</td>\n",
       "      <td>0.877542</td>\n",
       "      <td>0.909439</td>\n",
       "      <td>0.894097</td>\n",
       "      <td>0.863824</td>\n",
       "      <td>0.849838</td>\n",
       "      <td>0.867351</td>\n",
       "      <td>0.865360</td>\n",
       "      <td>0.861068</td>\n",
       "      <td>0.868706</td>\n",
       "      <td>0.875541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274</th>\n",
       "      <td>TRAIN_1274</td>\n",
       "      <td>./train/TRAIN_1274.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.964989</td>\n",
       "      <td>0.096119</td>\n",
       "      <td>0.129592</td>\n",
       "      <td>0.222531</td>\n",
       "      <td>0.039493</td>\n",
       "      <td>0.075616</td>\n",
       "      <td>...</td>\n",
       "      <td>0.857574</td>\n",
       "      <td>0.875432</td>\n",
       "      <td>0.862135</td>\n",
       "      <td>0.853636</td>\n",
       "      <td>0.852105</td>\n",
       "      <td>0.854758</td>\n",
       "      <td>0.852087</td>\n",
       "      <td>0.851688</td>\n",
       "      <td>0.860304</td>\n",
       "      <td>0.881918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275</th>\n",
       "      <td>TRAIN_1275</td>\n",
       "      <td>./train/TRAIN_1275.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.959506</td>\n",
       "      <td>0.283203</td>\n",
       "      <td>0.293961</td>\n",
       "      <td>0.390142</td>\n",
       "      <td>0.322176</td>\n",
       "      <td>0.239990</td>\n",
       "      <td>...</td>\n",
       "      <td>0.842018</td>\n",
       "      <td>0.853391</td>\n",
       "      <td>0.822730</td>\n",
       "      <td>0.825484</td>\n",
       "      <td>0.822317</td>\n",
       "      <td>0.809976</td>\n",
       "      <td>0.820805</td>\n",
       "      <td>0.835689</td>\n",
       "      <td>0.836974</td>\n",
       "      <td>0.819587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1276</th>\n",
       "      <td>TRAIN_1276</td>\n",
       "      <td>./train/TRAIN_1276.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.930908</td>\n",
       "      <td>0.223856</td>\n",
       "      <td>0.210020</td>\n",
       "      <td>0.151861</td>\n",
       "      <td>0.134018</td>\n",
       "      <td>0.149416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.918239</td>\n",
       "      <td>0.932157</td>\n",
       "      <td>0.898576</td>\n",
       "      <td>0.905386</td>\n",
       "      <td>0.894843</td>\n",
       "      <td>0.879353</td>\n",
       "      <td>0.900221</td>\n",
       "      <td>0.910439</td>\n",
       "      <td>0.903620</td>\n",
       "      <td>0.895840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1277</th>\n",
       "      <td>TRAIN_1277</td>\n",
       "      <td>./train/TRAIN_1277.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.932890</td>\n",
       "      <td>0.247222</td>\n",
       "      <td>0.265837</td>\n",
       "      <td>0.223214</td>\n",
       "      <td>0.170546</td>\n",
       "      <td>0.141445</td>\n",
       "      <td>...</td>\n",
       "      <td>0.922436</td>\n",
       "      <td>0.946884</td>\n",
       "      <td>0.920099</td>\n",
       "      <td>0.916849</td>\n",
       "      <td>0.900980</td>\n",
       "      <td>0.903257</td>\n",
       "      <td>0.919087</td>\n",
       "      <td>0.922190</td>\n",
       "      <td>0.920950</td>\n",
       "      <td>0.932029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1278</th>\n",
       "      <td>TRAIN_1278</td>\n",
       "      <td>./train/TRAIN_1278.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.407188</td>\n",
       "      <td>0.620077</td>\n",
       "      <td>0.356668</td>\n",
       "      <td>0.769474</td>\n",
       "      <td>0.548411</td>\n",
       "      <td>0.806605</td>\n",
       "      <td>...</td>\n",
       "      <td>0.306191</td>\n",
       "      <td>0.317135</td>\n",
       "      <td>0.329551</td>\n",
       "      <td>0.308712</td>\n",
       "      <td>0.285871</td>\n",
       "      <td>0.289114</td>\n",
       "      <td>0.280076</td>\n",
       "      <td>0.270611</td>\n",
       "      <td>0.257248</td>\n",
       "      <td>0.177652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1279 rows × 260 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       SAMPLE_ID             SAMPLE_PATH  FAN_TYPE  LABEL       128       129  \\\n",
       "0     TRAIN_0000  ./train/TRAIN_0000.wav         2      0  0.952273  0.284621   \n",
       "1     TRAIN_0001  ./train/TRAIN_0001.wav         0      0  0.081981  0.935459   \n",
       "2     TRAIN_0002  ./train/TRAIN_0002.wav         0      0  0.240260  0.664368   \n",
       "3     TRAIN_0003  ./train/TRAIN_0003.wav         2      0  0.943729  0.295295   \n",
       "4     TRAIN_0004  ./train/TRAIN_0004.wav         2      0  0.949608  0.188729   \n",
       "...          ...                     ...       ...    ...       ...       ...   \n",
       "1274  TRAIN_1274  ./train/TRAIN_1274.wav         2      0  0.964989  0.096119   \n",
       "1275  TRAIN_1275  ./train/TRAIN_1275.wav         2      0  0.959506  0.283203   \n",
       "1276  TRAIN_1276  ./train/TRAIN_1276.wav         2      0  0.930908  0.223856   \n",
       "1277  TRAIN_1277  ./train/TRAIN_1277.wav         2      0  0.932890  0.247222   \n",
       "1278  TRAIN_1278  ./train/TRAIN_1278.wav         0      0  0.407188  0.620077   \n",
       "\n",
       "           130       131       132       133  ...       118       119  \\\n",
       "0     0.295843  0.363586  0.357530  0.294327  ...  0.833846  0.842291   \n",
       "1     0.514880  0.613406  0.691659  0.825306  ...  0.148846  0.144862   \n",
       "2     0.724483  0.354632  0.625930  0.695975  ...  0.454258  0.520886   \n",
       "3     0.312391  0.371455  0.329344  0.268686  ...  0.808817  0.827032   \n",
       "4     0.179787  0.154144  0.007228  0.055841  ...  0.877542  0.909439   \n",
       "...        ...       ...       ...       ...  ...       ...       ...   \n",
       "1274  0.129592  0.222531  0.039493  0.075616  ...  0.857574  0.875432   \n",
       "1275  0.293961  0.390142  0.322176  0.239990  ...  0.842018  0.853391   \n",
       "1276  0.210020  0.151861  0.134018  0.149416  ...  0.918239  0.932157   \n",
       "1277  0.265837  0.223214  0.170546  0.141445  ...  0.922436  0.946884   \n",
       "1278  0.356668  0.769474  0.548411  0.806605  ...  0.306191  0.317135   \n",
       "\n",
       "           120       121       122       123       124       125       126  \\\n",
       "0     0.818590  0.817896  0.799984  0.799645  0.811261  0.816584  0.825692   \n",
       "1     0.149996  0.165012  0.156853  0.160930  0.163661  0.162874  0.158788   \n",
       "2     0.454087  0.465858  0.423989  0.497721  0.504808  0.502295  0.487844   \n",
       "3     0.815166  0.787913  0.779337  0.781981  0.771998  0.779565  0.796365   \n",
       "4     0.894097  0.863824  0.849838  0.867351  0.865360  0.861068  0.868706   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1274  0.862135  0.853636  0.852105  0.854758  0.852087  0.851688  0.860304   \n",
       "1275  0.822730  0.825484  0.822317  0.809976  0.820805  0.835689  0.836974   \n",
       "1276  0.898576  0.905386  0.894843  0.879353  0.900221  0.910439  0.903620   \n",
       "1277  0.920099  0.916849  0.900980  0.903257  0.919087  0.922190  0.920950   \n",
       "1278  0.329551  0.308712  0.285871  0.289114  0.280076  0.270611  0.257248   \n",
       "\n",
       "           127  \n",
       "0     0.805742  \n",
       "1     0.084692  \n",
       "2     0.409240  \n",
       "3     0.798277  \n",
       "4     0.875541  \n",
       "...        ...  \n",
       "1274  0.881918  \n",
       "1275  0.819587  \n",
       "1276  0.895840  \n",
       "1277  0.932029  \n",
       "1278  0.177652  \n",
       "\n",
       "[1279 rows x 260 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a8fecd3-b50a-4c2a-9cb2-301a69345620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['FAN_TYPE',        128,        129,        130,        131,        132,\n",
       "              133,        134,        135,        136,\n",
       "       ...\n",
       "              118,        119,        120,        121,        122,        123,\n",
       "              124,        125,        126,        127],\n",
       "      dtype='object', length=257)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = tmp.columns.drop(['SAMPLE_ID', 'SAMPLE_PATH', 'LABEL'])\n",
    "cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e967190-c87e-458a-b9cb-4399574fa696",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0bdc4b0-54e4-45c1-8bca-57c252dc591c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, eval_mode):\n",
    "        self.eval_mode = eval_mode\n",
    "        if self.eval_mode:\n",
    "            self.labels = df['LABEL'].values\n",
    "        self.df = df[cols].values\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if self.eval_mode:\n",
    "            self.x = self.df[index]\n",
    "            self.y = self.labels[index]\n",
    "            return torch.Tensor(self.x).reshape(-1,1), self.y\n",
    "        else:\n",
    "            self.x = self.df[index]\n",
    "            return torch.Tensor(self.x).reshape(-1,1)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2afb059-203b-4ac7-aaec-79a0380abadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, tolerance=5, min_delta=0):\n",
    "        self.tolerance = tolerance\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.min_loss = np.inf\n",
    "\n",
    "    def __call__(self, train_loss, validation_loss=None):\n",
    "        if train_loss < self.min_loss:\n",
    "            self.counter = 0\n",
    "            self.min_loss = train_loss\n",
    "            print(f'counter : set 0 min loss : {self.min_loss}')\n",
    "        elif train_loss > self.min_loss:\n",
    "            self.counter += 1\n",
    "            print(f'counter : {self.counter}')\n",
    "        if self.counter >= self.tolerance:  \n",
    "            self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20032097-fc78-4ae7-bf1a-bcd831ecca78",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(tmp, test_size=0.1, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d381b860-662d-4cc1-a144-5410e9554877",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(df=train_df, eval_mode=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH, shuffle=True)\n",
    "\n",
    "val_dataset = CustomDataset(df = val_df, eval_mode=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "2b146980-5d90-4639-ac2c-df6ea0e4fd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, model, optimizer, train_loader, val_loader, scheduler, device):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.scheduler = scheduler\n",
    "        self.device = device\n",
    "        self.early_stopping = EarlyStopping(tolerance=100, min_delta=10)\n",
    "        \n",
    "        # Loss Function\n",
    "        self.criterion = nn.KLDivLoss(reduction='batchmean', log_target=True).to(self.device)\n",
    "        # self.criterion = nn.L1Loss().to(self.device)\n",
    "        \n",
    "    def fit(self):\n",
    "        self.model.to(self.device)\n",
    "        best_score = float('-inf')\n",
    "        for epoch in range(EPOCHS):\n",
    "            self.model.train()\n",
    "            train_loss = []\n",
    "            for x in iter(self.train_loader):\n",
    "                self.optimizer.zero_grad()\n",
    "                x = x.float().to(self.device)\n",
    "                _x = self.model(x)\n",
    "\n",
    "                log_target = F.log_softmax(_x, dim=1).to(self.device)\n",
    "                log_input = F.log_softmax(x.reshape(-1,257), dim=1).to(self.device)\n",
    "                # print(log_input[0], log_target[0])\n",
    "                loss = self.criterion(log_input, log_target)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                train_loss.append(loss.item())\n",
    "            \n",
    "            score = self.validation(self.model)\n",
    "            print(f'Epoch : [{epoch}] Train loss : [{np.mean(train_loss)}] Val Score : [{score}])')\n",
    "            \n",
    "            if best_score <= score:\n",
    "                best_score = score\n",
    "                torch.save(model.state_dict(), './best_model.pth', _use_new_zipfile_serialization=False)\n",
    "            torch.save(model.state_dict(), './model.pth', _use_new_zipfile_serialization=False)\n",
    "            # early stopping\n",
    "            self.early_stopping(np.mean(train_loss))\n",
    "            if self.early_stopping.early_stop:\n",
    "                print(\"early_stopping:\", epoch)\n",
    "                break\n",
    "            \n",
    "            if self.scheduler is not None:\n",
    "                self.scheduler.step(score)\n",
    "                            \n",
    "    def validation(self, eval_model, thr=0.999):\n",
    "        cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        eval_model.eval()\n",
    "        pred = []\n",
    "        true = []\n",
    "        with torch.no_grad():\n",
    "            for x, y in iter(self.val_loader):\n",
    "                x = x.float().to(self.device)\n",
    "                _x = self.model(x)\n",
    "                \n",
    "                log_target = F.log_softmax(_x, dim=1)\n",
    "                log_input = F.log_softmax(x.reshape(-1,257), dim=1)\n",
    "                \n",
    "                diff = cos(log_input, log_target).cpu().tolist()\n",
    "                # print(diff)\n",
    "                batch_pred = np.where(np.array(diff) > thr, 0, 1).tolist()\n",
    "\n",
    "                pred += batch_pred\n",
    "                true += y.tolist()\n",
    "\n",
    "        return f1_score(true, pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "638207fa-027e-4b11-ab15-38ea0a9222df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResConv, self).__init__()\n",
    "\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = in_channels, out_channels = out_channels, \n",
    "                                 kernel_size = 1, stride = 1),\n",
    "            nn.BatchNorm1d(num_features = out_channels),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "        self.conv_skip = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = in_channels, out_channels = out_channels, \n",
    "                                 kernel_size = 1, stride = 1),\n",
    "            nn.BatchNorm1d(num_features = out_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_block(x) + self.conv_skip(x)\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Upsample, self).__init__()\n",
    "\n",
    "        self.upsample = nn.ConvTranspose1d(\n",
    "            in_channels, out_channels, kernel_size=1, stride=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.upsample(x)\n",
    "    \n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.input_layer = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 257, out_channels = 512, \n",
    "                                 kernel_size = 1, stride = 1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.GELU(),\n",
    "            nn.Conv1d(in_channels = 512, out_channels = 512, \n",
    "                                 kernel_size = 1, stride = 1),\n",
    "        )\n",
    "        \n",
    "        self.input_skip = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 257, out_channels = 512, \n",
    "                                 kernel_size = 1, stride = 1)\n",
    "        )\n",
    "        \n",
    "        self.resconv1 = ResConv(512, 512)\n",
    "        self.resconv2 = ResConv(512, 1024)\n",
    "        \n",
    "        self.bridge = ResConv(1024, 1024)\n",
    "        \n",
    "        \n",
    "        # Decoder        \n",
    "        self.upsample_1 = Upsample(1024, 1024)\n",
    "        self.upresconv1 = ResConv(1024*2, 512)\n",
    "        \n",
    "        self.upsample_2 = Upsample(1024, 512)\n",
    "        self.upresconv2 = ResConv(1024, 512)\n",
    "        \n",
    "        self.upsample_3 = Upsample(512, 512)\n",
    "        self.upresconv3 = ResConv(512*2, 512)\n",
    "\n",
    "        self.flat = nn.Flatten()\n",
    "        self.fc = nn.Conv1d(in_channels = 512, out_channels = 257, kernel_size = 1, stride = 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encode\n",
    "        x1 = self.input_layer(x) + self.input_skip(x)\n",
    "        x2 = self.resconv1(x1) + self.input_skip(x)\n",
    "        x3 = self.resconv2(x2)\n",
    "        \n",
    "        # Bridge\n",
    "        x4 = self.bridge(x3)\n",
    "        \n",
    "        # Decode\n",
    "        # x4 = self.upsample_1(x4)\n",
    "        x5 = torch.cat([x4, x3], dim=1)\n",
    "        x6 = self.upresconv1(x5)\n",
    "        \n",
    "        # x6 = self.upsample_2(x6)\n",
    "        x7 = torch.cat([x6, x2], dim=1)\n",
    "        \n",
    "        x8 = self.upresconv2(x7)\n",
    "\n",
    "        # x8 = self.upsample_3(x8)\n",
    "        x9 = torch.cat([x8, x1], dim=1)\n",
    "\n",
    "        x10 = self.upresconv3(x9)\n",
    "\n",
    "        output = self.fc(x10)\n",
    "        return self.flat(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4ed34d-cf4f-46ac-a7dc-e0af29698759",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [0] Train loss : [0.17930056452751159] Val Score : [0.4074074074074074])\n",
      "counter : set 0 min loss : 0.17930056452751159\n",
      "Epoch : [1] Train loss : [0.09885907024145127] Val Score : [0.43859649122807015])\n",
      "counter : set 0 min loss : 0.09885907024145127\n",
      "Epoch : [2] Train loss : [0.07162650823593139] Val Score : [0.4458874458874459])\n",
      "counter : set 0 min loss : 0.07162650823593139\n",
      "Epoch : [3] Train loss : [0.05666284039616585] Val Score : [0.45064377682403434])\n",
      "counter : set 0 min loss : 0.05666284039616585\n",
      "Epoch : [4] Train loss : [0.046596828103065493] Val Score : [0.459915611814346])\n",
      "counter : set 0 min loss : 0.046596828103065493\n",
      "Epoch : [5] Train loss : [0.039390365779399875] Val Score : [0.46443514644351463])\n",
      "counter : set 0 min loss : 0.039390365779399875\n",
      "Epoch : [6] Train loss : [0.03425737842917442] Val Score : [0.459915611814346])\n",
      "counter : set 0 min loss : 0.03425737842917442\n",
      "Epoch : [7] Train loss : [0.030386653169989584] Val Score : [0.4576271186440678])\n",
      "counter : set 0 min loss : 0.030386653169989584\n",
      "Epoch : [8] Train loss : [0.02681708298623562] Val Score : [0.45064377682403434])\n",
      "counter : set 0 min loss : 0.02681708298623562\n",
      "Epoch : [9] Train loss : [0.02416560649871826] Val Score : [0.45064377682403434])\n",
      "counter : set 0 min loss : 0.02416560649871826\n",
      "Epoch : [10] Train loss : [0.02151338867843151] Val Score : [0.452991452991453])\n",
      "counter : set 0 min loss : 0.02151338867843151\n",
      "Epoch : [11] Train loss : [0.02022947333753109] Val Score : [0.459915611814346])\n",
      "counter : set 0 min loss : 0.02022947333753109\n",
      "Epoch : [12] Train loss : [0.018203167989850045] Val Score : [0.46443514644351463])\n",
      "counter : set 0 min loss : 0.018203167989850045\n",
      "Epoch : [13] Train loss : [0.01662929207086563] Val Score : [0.46443514644351463])\n",
      "counter : set 0 min loss : 0.01662929207086563\n",
      "Epoch : [14] Train loss : [0.015818213857710362] Val Score : [0.46443514644351463])\n",
      "counter : set 0 min loss : 0.015818213857710362\n",
      "Epoch : [15] Train loss : [0.014605616591870785] Val Score : [0.4666666666666667])\n",
      "counter : set 0 min loss : 0.014605616591870785\n",
      "Epoch : [16] Train loss : [0.013365230150520801] Val Score : [0.47540983606557374])\n",
      "counter : set 0 min loss : 0.013365230150520801\n",
      "Epoch : [17] Train loss : [0.013030082359910012] Val Score : [0.4775510204081633])\n",
      "counter : set 0 min loss : 0.013030082359910012\n",
      "Epoch : [18] Train loss : [0.011903427727520465] Val Score : [0.4817813765182186])\n",
      "counter : set 0 min loss : 0.011903427727520465\n",
      "Epoch : [19] Train loss : [0.011415095813572407] Val Score : [0.4838709677419355])\n",
      "counter : set 0 min loss : 0.011415095813572407\n",
      "Epoch : [20] Train loss : [0.011109104938805103] Val Score : [0.4838709677419355])\n",
      "counter : set 0 min loss : 0.011109104938805103\n",
      "Epoch : [21] Train loss : [0.01021608877927065] Val Score : [0.4859437751004016])\n",
      "counter : set 0 min loss : 0.01021608877927065\n",
      "Epoch : [22] Train loss : [0.009878372959792613] Val Score : [0.4859437751004016])\n",
      "counter : set 0 min loss : 0.009878372959792613\n",
      "Epoch : [23] Train loss : [0.009740915149450302] Val Score : [0.4859437751004016])\n",
      "counter : set 0 min loss : 0.009740915149450302\n",
      "Epoch : [24] Train loss : [0.009088871069252491] Val Score : [0.488])\n",
      "counter : set 0 min loss : 0.009088871069252491\n",
      "Epoch : [25] Train loss : [0.009022049978375435] Val Score : [0.4900398406374502])\n",
      "counter : set 0 min loss : 0.009022049978375435\n",
      "Epoch : [26] Train loss : [0.008906040899455547] Val Score : [0.4900398406374502])\n",
      "counter : set 0 min loss : 0.008906040899455547\n",
      "Epoch : [27] Train loss : [0.008578835893422366] Val Score : [0.49206349206349204])\n",
      "counter : set 0 min loss : 0.008578835893422366\n",
      "Epoch : [28] Train loss : [0.007896806485950947] Val Score : [0.49407114624505927])\n",
      "counter : set 0 min loss : 0.007896806485950947\n",
      "Epoch : [29] Train loss : [0.007987815607339143] Val Score : [0.49407114624505927])\n",
      "counter : 1\n",
      "Epoch : [30] Train loss : [0.007351649831980467] Val Score : [0.49606299212598426])\n",
      "counter : set 0 min loss : 0.007351649831980467\n",
      "Epoch : [31] Train loss : [0.007065895199775696] Val Score : [0.49606299212598426])\n",
      "counter : set 0 min loss : 0.007065895199775696\n",
      "Epoch : [32] Train loss : [0.006942500732839108] Val Score : [0.49606299212598426])\n",
      "counter : set 0 min loss : 0.006942500732839108\n",
      "Epoch : [33] Train loss : [0.007046266552060842] Val Score : [0.49606299212598426])\n",
      "counter : 1\n",
      "Epoch : [34] Train loss : [0.006693909596651793] Val Score : [0.4980392156862745])\n",
      "counter : set 0 min loss : 0.006693909596651793\n",
      "Epoch : [35] Train loss : [0.006459453795105219] Val Score : [0.4980392156862745])\n",
      "counter : set 0 min loss : 0.006459453795105219\n",
      "Epoch : [36] Train loss : [0.006451242044568062] Val Score : [0.4980392156862745])\n",
      "counter : set 0 min loss : 0.006451242044568062\n",
      "Epoch : [37] Train loss : [0.006597405672073365] Val Score : [0.4980392156862745])\n",
      "counter : 1\n",
      "Epoch : [38] Train loss : [0.006194837670773268] Val Score : [0.4980392156862745])\n",
      "counter : set 0 min loss : 0.006194837670773268\n",
      "Epoch : [39] Train loss : [0.005954574793577194] Val Score : [0.4980392156862745])\n",
      "counter : set 0 min loss : 0.005954574793577194\n",
      "Epoch : [40] Train loss : [0.006236536987125873] Val Score : [0.4980392156862745])\n",
      "counter : 1\n",
      "Epoch : [41] Train loss : [0.005914442706853151] Val Score : [0.4980392156862745])\n",
      "counter : set 0 min loss : 0.005914442706853151\n",
      "Epoch : [42] Train loss : [0.005722808837890625] Val Score : [1.0])\n",
      "counter : set 0 min loss : 0.005722808837890625\n",
      "Epoch : [43] Train loss : [0.005579729657620191] Val Score : [1.0])\n",
      "counter : set 0 min loss : 0.005579729657620191\n",
      "Epoch : [44] Train loss : [0.005508952029049397] Val Score : [1.0])\n",
      "counter : set 0 min loss : 0.005508952029049397\n",
      "Epoch : [45] Train loss : [0.0053605189546942714] Val Score : [1.0])\n",
      "counter : set 0 min loss : 0.0053605189546942714\n",
      "Epoch : [46] Train loss : [0.005371948983520269] Val Score : [1.0])\n",
      "counter : 1\n",
      "Epoch : [47] Train loss : [0.005441706255078315] Val Score : [1.0])\n",
      "counter : 2\n",
      "Epoch : [48] Train loss : [0.005264908354729414] Val Score : [1.0])\n",
      "counter : set 0 min loss : 0.005264908354729414\n",
      "Epoch : [49] Train loss : [0.00519602419808507] Val Score : [1.0])\n",
      "counter : set 0 min loss : 0.00519602419808507\n",
      "Epoch : [50] Train loss : [0.005139435268938541] Val Score : [1.0])\n",
      "counter : set 0 min loss : 0.005139435268938541\n",
      "Epoch : [51] Train loss : [0.0052241113036870955] Val Score : [1.0])\n",
      "counter : 1\n",
      "Epoch : [52] Train loss : [0.004856870509684086] Val Score : [1.0])\n",
      "counter : set 0 min loss : 0.004856870509684086\n",
      "Epoch : [53] Train loss : [0.004982147458940744] Val Score : [1.0])\n",
      "counter : 1\n",
      "Epoch 00054: reducing learning rate of group 0 to 5.0000e-02.\n",
      "Epoch : [54] Train loss : [0.004910280928015709] Val Score : [1.0])\n",
      "counter : 2\n",
      "Epoch : [55] Train loss : [0.004646116588264704] Val Score : [1.0])\n",
      "counter : set 0 min loss : 0.004646116588264704\n",
      "Epoch : [56] Train loss : [0.004771112464368343] Val Score : [1.0])\n",
      "counter : 1\n",
      "Epoch : [57] Train loss : [0.0047889219596982] Val Score : [1.0])\n",
      "counter : 2\n",
      "Epoch : [58] Train loss : [0.004501455835998059] Val Score : [1.0])\n",
      "counter : set 0 min loss : 0.004501455835998059\n",
      "Epoch : [59] Train loss : [0.0046101993881165985] Val Score : [1.0])\n",
      "counter : 1\n",
      "Epoch : [60] Train loss : [0.004728935193270445] Val Score : [1.0])\n",
      "counter : 2\n",
      "Epoch : [61] Train loss : [0.004659798555076122] Val Score : [1.0])\n",
      "counter : 3\n",
      "Epoch : [62] Train loss : [0.004449875745922327] Val Score : [1.0])\n",
      "counter : set 0 min loss : 0.004449875745922327\n",
      "Epoch : [63] Train loss : [0.004459214117377997] Val Score : [1.0])\n",
      "counter : 1\n",
      "Epoch : [64] Train loss : [0.004713352583348751] Val Score : [1.0])\n",
      "counter : 2\n",
      "Epoch 00065: reducing learning rate of group 0 to 2.5000e-02.\n",
      "Epoch : [65] Train loss : [0.0044432666152715685] Val Score : [1.0])\n",
      "counter : set 0 min loss : 0.0044432666152715685\n",
      "Epoch : [66] Train loss : [0.004758730251342058] Val Score : [1.0])\n",
      "counter : 1\n",
      "Epoch : [67] Train loss : [0.004614085517823696] Val Score : [1.0])\n",
      "counter : 2\n",
      "Epoch : [68] Train loss : [0.004671398643404245] Val Score : [1.0])\n",
      "counter : 3\n",
      "Epoch : [69] Train loss : [0.004380432795733214] Val Score : [1.0])\n",
      "counter : set 0 min loss : 0.004380432795733214\n",
      "Epoch : [70] Train loss : [0.004333127290010452] Val Score : [1.0])\n",
      "counter : set 0 min loss : 0.004333127290010452\n",
      "Epoch : [71] Train loss : [0.004446470411494374] Val Score : [1.0])\n",
      "counter : 1\n",
      "Epoch : [72] Train loss : [0.0044445695355534555] Val Score : [1.0])\n",
      "counter : 2\n",
      "Epoch : [73] Train loss : [0.004573863465338945] Val Score : [1.0])\n",
      "counter : 3\n",
      "Epoch : [74] Train loss : [0.004471619706600904] Val Score : [1.0])\n",
      "counter : 4\n",
      "Epoch : [75] Train loss : [0.004430873598903417] Val Score : [1.0])\n",
      "counter : 5\n",
      "Epoch 00076: reducing learning rate of group 0 to 1.2500e-02.\n",
      "Epoch : [76] Train loss : [0.00447645029053092] Val Score : [1.0])\n",
      "counter : 6\n",
      "Epoch : [77] Train loss : [0.004415054805576801] Val Score : [1.0])\n",
      "counter : 7\n",
      "Epoch : [78] Train loss : [0.0043611394241452215] Val Score : [1.0])\n",
      "counter : 8\n",
      "Epoch : [79] Train loss : [0.004482768196612597] Val Score : [1.0])\n",
      "counter : 9\n",
      "Epoch : [80] Train loss : [0.004488868080079556] Val Score : [1.0])\n",
      "counter : 10\n",
      "Epoch : [81] Train loss : [0.004393240716308355] Val Score : [1.0])\n",
      "counter : 11\n",
      "Epoch : [82] Train loss : [0.004105959134176373] Val Score : [1.0])\n",
      "counter : set 0 min loss : 0.004105959134176373\n",
      "Epoch : [83] Train loss : [0.004583076480776072] Val Score : [1.0])\n",
      "counter : 1\n",
      "Epoch : [84] Train loss : [0.004298244509845972] Val Score : [1.0])\n",
      "counter : 2\n",
      "Epoch : [85] Train loss : [0.004239901108667255] Val Score : [1.0])\n",
      "counter : 3\n",
      "Epoch : [86] Train loss : [0.0042322619818151] Val Score : [1.0])\n",
      "counter : 4\n",
      "Epoch 00087: reducing learning rate of group 0 to 6.2500e-03.\n",
      "Epoch : [87] Train loss : [0.004252719134092331] Val Score : [1.0])\n",
      "counter : 5\n",
      "Epoch : [88] Train loss : [0.004130598064512015] Val Score : [1.0])\n",
      "counter : 6\n",
      "Epoch : [89] Train loss : [0.00425543487071991] Val Score : [1.0])\n",
      "counter : 7\n",
      "Epoch : [90] Train loss : [0.00430954834446311] Val Score : [1.0])\n",
      "counter : 8\n",
      "Epoch : [91] Train loss : [0.004181885439902544] Val Score : [1.0])\n",
      "counter : 9\n",
      "Epoch : [92] Train loss : [0.004264897573739291] Val Score : [1.0])\n",
      "counter : 10\n",
      "Epoch : [93] Train loss : [0.004131808970123529] Val Score : [1.0])\n",
      "counter : 11\n",
      "Epoch : [94] Train loss : [0.004112147819250822] Val Score : [1.0])\n",
      "counter : 12\n",
      "Epoch : [95] Train loss : [0.004304597061127425] Val Score : [1.0])\n",
      "counter : 13\n",
      "Epoch : [96] Train loss : [0.004800597205758095] Val Score : [1.0])\n",
      "counter : 14\n",
      "Epoch : [97] Train loss : [0.004184905719012022] Val Score : [1.0])\n",
      "counter : 15\n",
      "Epoch 00098: reducing learning rate of group 0 to 3.1250e-03.\n",
      "Epoch : [98] Train loss : [0.004310466069728136] Val Score : [1.0])\n",
      "counter : 16\n",
      "Epoch : [99] Train loss : [0.004813509527593851] Val Score : [1.0])\n",
      "counter : 17\n",
      "Epoch : [100] Train loss : [0.0041486815549433235] Val Score : [1.0])\n",
      "counter : 18\n",
      "Epoch : [101] Train loss : [0.0040732607245445255] Val Score : [1.0])\n",
      "counter : set 0 min loss : 0.0040732607245445255\n",
      "Epoch : [102] Train loss : [0.00451267808675766] Val Score : [1.0])\n",
      "counter : 1\n",
      "Epoch : [103] Train loss : [0.004111538082361222] Val Score : [1.0])\n",
      "counter : 2\n",
      "Epoch : [104] Train loss : [0.004139195615425706] Val Score : [1.0])\n",
      "counter : 3\n",
      "Epoch : [105] Train loss : [0.004481543134897947] Val Score : [1.0])\n",
      "counter : 4\n",
      "Epoch : [106] Train loss : [0.004291564598679543] Val Score : [1.0])\n",
      "counter : 5\n",
      "Epoch : [107] Train loss : [0.0048696335405111315] Val Score : [1.0])\n",
      "counter : 6\n",
      "Epoch : [108] Train loss : [0.0042164533399045466] Val Score : [1.0])\n",
      "counter : 7\n",
      "Epoch 00109: reducing learning rate of group 0 to 1.5625e-03.\n",
      "Epoch : [109] Train loss : [0.004219854343682527] Val Score : [1.0])\n",
      "counter : 8\n",
      "Epoch : [110] Train loss : [0.004259226936846971] Val Score : [1.0])\n",
      "counter : 9\n",
      "Epoch : [111] Train loss : [0.004276800435036421] Val Score : [1.0])\n",
      "counter : 10\n",
      "Epoch : [112] Train loss : [0.004167249239981174] Val Score : [1.0])\n",
      "counter : 11\n",
      "Epoch : [113] Train loss : [0.004244072921574115] Val Score : [1.0])\n",
      "counter : 12\n",
      "Epoch : [114] Train loss : [0.004262707941234112] Val Score : [1.0])\n",
      "counter : 13\n",
      "Epoch : [115] Train loss : [0.004084366699680686] Val Score : [1.0])\n",
      "counter : 14\n",
      "Epoch : [116] Train loss : [0.0044931616634130474] Val Score : [1.0])\n",
      "counter : 15\n",
      "Epoch : [117] Train loss : [0.004215657152235508] Val Score : [1.0])\n",
      "counter : 16\n",
      "Epoch : [118] Train loss : [0.004444867232814431] Val Score : [1.0])\n",
      "counter : 17\n",
      "Epoch : [119] Train loss : [0.004339691810309887] Val Score : [1.0])\n",
      "counter : 18\n",
      "Epoch 00120: reducing learning rate of group 0 to 7.8125e-04.\n",
      "Epoch : [120] Train loss : [0.004297815635800362] Val Score : [1.0])\n",
      "counter : 19\n",
      "Epoch : [121] Train loss : [0.004250990506261587] Val Score : [1.0])\n",
      "counter : 20\n",
      "Epoch : [122] Train loss : [0.004179857205599546] Val Score : [1.0])\n",
      "counter : 21\n",
      "Epoch : [123] Train loss : [0.004313622415065765] Val Score : [1.0])\n",
      "counter : 22\n",
      "Epoch : [124] Train loss : [0.004106702655553818] Val Score : [1.0])\n",
      "counter : 23\n",
      "Epoch : [125] Train loss : [0.004226461797952652] Val Score : [1.0])\n",
      "counter : 24\n",
      "Epoch : [126] Train loss : [0.004337028972804547] Val Score : [1.0])\n",
      "counter : 25\n",
      "Epoch : [127] Train loss : [0.004209456872195006] Val Score : [1.0])\n",
      "counter : 26\n",
      "Epoch : [128] Train loss : [0.004366950318217278] Val Score : [1.0])\n",
      "counter : 27\n",
      "Epoch : [129] Train loss : [0.004061465058475733] Val Score : [1.0])\n",
      "counter : set 0 min loss : 0.004061465058475733\n",
      "Epoch : [130] Train loss : [0.004626652598381043] Val Score : [1.0])\n",
      "counter : 1\n",
      "Epoch 00131: reducing learning rate of group 0 to 3.9063e-04.\n",
      "Epoch : [131] Train loss : [0.004188098618760705] Val Score : [1.0])\n",
      "counter : 2\n",
      "Epoch : [132] Train loss : [0.004219531826674938] Val Score : [1.0])\n",
      "counter : 3\n",
      "Epoch : [133] Train loss : [0.004120299080386758] Val Score : [1.0])\n",
      "counter : 4\n",
      "Epoch : [134] Train loss : [0.004486602544784546] Val Score : [1.0])\n",
      "counter : 5\n",
      "Epoch : [135] Train loss : [0.004213187098503113] Val Score : [1.0])\n",
      "counter : 6\n",
      "Epoch : [136] Train loss : [0.004389160964637995] Val Score : [1.0])\n",
      "counter : 7\n",
      "Epoch : [137] Train loss : [0.004511335399001837] Val Score : [1.0])\n",
      "counter : 8\n",
      "Epoch : [138] Train loss : [0.004208061099052429] Val Score : [1.0])\n",
      "counter : 9\n",
      "Epoch : [139] Train loss : [0.004473615391179919] Val Score : [1.0])\n",
      "counter : 10\n",
      "Epoch : [140] Train loss : [0.0044337216764688495] Val Score : [1.0])\n",
      "counter : 11\n",
      "Epoch : [141] Train loss : [0.0042057553306221965] Val Score : [1.0])\n",
      "counter : 12\n",
      "Epoch 00142: reducing learning rate of group 0 to 1.9531e-04.\n",
      "Epoch : [142] Train loss : [0.004268494900316] Val Score : [1.0])\n",
      "counter : 13\n",
      "Epoch : [143] Train loss : [0.004142921883612871] Val Score : [1.0])\n",
      "counter : 14\n",
      "Epoch : [144] Train loss : [0.004177762754261493] Val Score : [1.0])\n",
      "counter : 15\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=10, threshold_mode='abs', min_lr=1e-12, verbose=True)\n",
    "\n",
    "trainer = Trainer(model, optimizer, train_loader, val_loader, scheduler, device)\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "96ca587b-6af7-444c-8396-0387d8b8f450",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (input_layer): Sequential(\n",
       "    (0): Conv1d(257, 512, kernel_size=(1,), stride=(1,))\n",
       "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): GELU(approximate='none')\n",
       "    (3): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (input_skip): Sequential(\n",
       "    (0): Conv1d(257, 512, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (resconv1): ResConv(\n",
       "    (conv_block): Sequential(\n",
       "      (0): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): GELU(approximate='none')\n",
       "    )\n",
       "    (conv_skip): Sequential(\n",
       "      (0): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (resconv2): ResConv(\n",
       "    (conv_block): Sequential(\n",
       "      (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): GELU(approximate='none')\n",
       "    )\n",
       "    (conv_skip): Sequential(\n",
       "      (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (bridge): ResConv(\n",
       "    (conv_block): Sequential(\n",
       "      (0): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): GELU(approximate='none')\n",
       "    )\n",
       "    (conv_skip): Sequential(\n",
       "      (0): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (upsample_1): Upsample(\n",
       "    (upsample): ConvTranspose1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (upresconv1): ResConv(\n",
       "    (conv_block): Sequential(\n",
       "      (0): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): GELU(approximate='none')\n",
       "    )\n",
       "    (conv_skip): Sequential(\n",
       "      (0): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (upsample_2): Upsample(\n",
       "    (upsample): ConvTranspose1d(1024, 512, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (upresconv2): ResConv(\n",
       "    (conv_block): Sequential(\n",
       "      (0): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): GELU(approximate='none')\n",
       "    )\n",
       "    (conv_skip): Sequential(\n",
       "      (0): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (upsample_3): Upsample(\n",
       "    (upsample): ConvTranspose1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (upresconv3): ResConv(\n",
       "    (conv_block): Sequential(\n",
       "      (0): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): GELU(approximate='none')\n",
       "    )\n",
       "    (conv_skip): Sequential(\n",
       "      (0): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc): Conv1d(512, 257, kernel_size=(1,), stride=(1,))\n",
       ")"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNN()\n",
    "model.load_state_dict(torch.load('./best_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# model = CNN()\n",
    "# model.load_state_dict(torch.load('./model.pth'))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a634dee-9e6d-4332-898e-c564866d09c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "762db097-20e2-47db-a5b0-21e2b97b1553",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(test, False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "e6872caa-8a20-45a4-8ded-76124adae2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(model, test_loader, device, thr=0.999):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    pred = []\n",
    "    with torch.no_grad():\n",
    "        for x in iter(test_loader):\n",
    "            x = x.float().to(device)\n",
    "            _x = model(x)\n",
    "            log_target = F.log_softmax(_x, dim=1)\n",
    "            log_input = F.log_softmax(x.reshape(-1,257), dim=1)\n",
    "                \n",
    "            diff = cos(log_input, log_target).cpu().tolist()\n",
    "            print(diff)\n",
    "            batch_pred = np.where(np.array(diff) > thr, 0, 1).tolist()\n",
    "            pred += batch_pred\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "cf0f701d-7862-42bc-a8df-a235785dd876",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9996505975723267, 0.999908447265625, 0.9990004301071167, 0.9973986148834229, 0.9991329312324524, 0.9999409914016724, 0.9999246597290039, 0.999808669090271, 0.9969272613525391, 0.9992584586143494, 0.9985594749450684, 0.9997560977935791, 0.9988080263137817, 0.9989929795265198, 0.9998619556427002, 0.9991437196731567, 0.999944806098938, 0.9997737407684326, 0.9997348785400391, 0.9975021481513977, 0.9974305033683777, 0.9990506172180176, 0.9991956353187561, 0.9987855553627014, 0.9997345209121704, 0.9997130036354065, 0.9990031123161316, 0.9996923208236694, 0.9986710548400879, 0.9999092221260071, 0.9998365640640259, 0.9997600317001343, 0.9998252391815186, 0.9996917843818665, 0.999175488948822, 0.9999167919158936, 0.9998964667320251, 0.9985170364379883, 0.9984459280967712, 0.9999053478240967, 0.999008297920227, 0.9998723268508911, 0.9998548030853271, 0.9993114471435547, 0.9996931552886963, 0.9997081160545349, 0.9993782043457031, 0.9998681545257568, 0.9999277591705322, 0.9985414743423462, 0.9999180436134338, 0.9998788833618164, 0.9992210865020752, 0.9989551305770874, 0.9988389015197754, 0.9998005628585815, 0.9990941882133484, 0.9998779296875, 0.9997429847717285, 0.9999045133590698, 0.9997722506523132, 0.9987496137619019, 0.9989205598831177, 0.9999345541000366, 0.9997514486312866, 0.9998798966407776, 0.9996927976608276, 0.9997302293777466, 0.9992315173149109, 0.9985216856002808, 0.9999276399612427, 0.9999476671218872, 0.9999025464057922, 0.9999243021011353, 0.9995113015174866, 0.999942421913147, 0.9982466101646423, 0.9999439716339111, 0.9974339008331299, 0.9998916387557983, 0.9988430738449097, 0.9988020658493042, 0.9998338222503662, 0.998714804649353, 0.9986225962638855, 0.999139130115509, 0.999862790107727, 0.9992430806159973, 0.999733567237854, 0.9996119141578674, 0.9999166131019592, 0.9987876415252686, 0.9997009038925171, 0.9991554021835327, 0.9982224702835083, 0.9964894652366638, 0.9979090690612793, 0.9998704195022583, 0.9998800158500671, 0.9998146295547485, 0.9989221096038818, 0.9998809695243835, 0.9999500513076782, 0.9973047971725464, 0.9997038841247559, 0.9987393617630005, 0.9998787641525269, 0.9991598129272461, 0.9990993142127991, 0.9999176859855652, 0.9997612237930298, 0.9980005025863647, 0.9993194937705994, 0.9988752603530884, 0.9982210397720337, 0.9982619285583496, 0.9998810887336731, 0.9990801811218262, 0.9978054761886597, 0.9987508058547974, 0.9998282194137573, 0.9990593194961548, 0.9988086223602295, 0.9978758096694946, 0.9993494749069214, 0.9993299245834351, 0.9998047947883606, 0.999393105506897, 0.9995591640472412, 0.9984657764434814, 0.9998571872711182, 0.9991258382797241, 0.9997782707214355, 0.9981904625892639, 0.9984877109527588, 0.9999221563339233, 0.9989991188049316, 0.9998637437820435, 0.9986695051193237, 0.9998620748519897, 0.9998373985290527, 0.9987400770187378, 0.9987892508506775, 0.9999018311500549, 0.9990267753601074, 0.9979280233383179, 0.9998753070831299, 0.9982469081878662, 0.9971825480461121, 0.9997680187225342, 0.9997493624687195, 0.999886155128479, 0.9998067617416382, 0.9993594288825989, 0.9997358918190002, 0.9982047080993652, 0.9998908638954163, 0.9977886080741882, 0.9971010684967041, 0.9998124837875366, 0.9977902173995972, 0.9992622137069702, 0.9986878037452698, 0.9998714923858643, 0.9998998045921326, 0.9988077282905579, 0.9966422915458679, 0.9986796379089355, 0.9976474642753601, 0.9990699291229248, 0.9991066455841064, 0.9991276264190674, 0.9990548491477966, 0.9973388314247131, 0.9997625350952148, 0.9988227486610413, 0.9999098181724548, 0.9985739588737488, 0.9997942447662354, 0.998665988445282, 0.99821937084198, 0.9998276829719543, 0.9986859560012817, 0.9997856616973877, 0.999799370765686, 0.9997556209564209, 0.9999365210533142, 0.9967714548110962, 0.9997097253799438, 0.9998058080673218, 0.9968343377113342, 0.9985276460647583, 0.9979043006896973, 0.9988874793052673, 0.9997538924217224, 0.9997509717941284, 0.9989407062530518, 0.9987920522689819, 0.9972208142280579, 0.9999104142189026, 0.9990783333778381, 0.9998579025268555, 0.9989904165267944, 0.9996155500411987, 0.9998725652694702, 0.9998881220817566, 0.9998074173927307, 0.9990423917770386, 0.999942421913147, 0.9999343156814575, 0.9983908534049988, 0.9986932277679443, 0.999783992767334, 0.9987367391586304, 0.9999387860298157, 0.9998631477355957, 0.997908353805542, 0.999740481376648, 0.9982829093933105, 0.998670220375061, 0.9984356164932251, 0.998461127281189, 0.999260663986206, 0.9988288879394531, 0.9998248815536499, 0.9976046085357666, 0.9977226853370667, 0.9989603757858276, 0.9999271631240845, 0.9998087882995605, 0.9999376535415649, 0.9979867935180664, 0.9996464252471924, 0.9999148845672607, 0.9994192123413086, 0.9999064207077026, 0.999603807926178, 0.9999136328697205, 0.999276876449585, 0.999820351600647, 0.9991720914840698, 0.9978427886962891, 0.9998883008956909, 0.999768853187561, 0.9989990592002869, 0.999861478805542, 0.9983137845993042, 0.9999309778213501, 0.9991143941879272, 0.999829888343811, 0.9998080730438232, 0.9989504814147949, 0.9988508820533752, 0.9997895956039429, 0.9989252686500549, 0.9986339211463928]\n",
      "[0.9983429908752441, 0.9989174008369446, 0.9987937211990356, 0.998418390750885, 0.9998570680618286, 0.9999286532402039, 0.9987146258354187, 0.9999096989631653, 0.9996819496154785, 0.9991352558135986, 0.9972599744796753, 0.9999432563781738, 0.9997884035110474, 0.9998435974121094, 0.9997221827507019, 0.9996509552001953, 0.9999237060546875, 0.9993922710418701, 0.9980306625366211, 0.9994347095489502, 0.9965337514877319, 0.9978905320167542, 0.9992663264274597, 0.9997743368148804, 0.9998199939727783, 0.9995111227035522, 0.999792218208313, 0.9989734888076782, 0.9983829855918884, 0.999047040939331, 0.999877393245697, 0.9997507333755493, 0.9998332858085632, 0.9999064803123474, 0.9998301267623901, 0.999786913394928, 0.9997941851615906, 0.9988622069358826, 0.9972138404846191, 0.9986690282821655, 0.998354971408844, 0.9999369382858276, 0.999794602394104, 0.9996184706687927, 0.9998584985733032, 0.9980894327163696, 0.9998184442520142, 0.9997088313102722, 0.9983054995536804, 0.9993886351585388, 0.9997962713241577, 0.9997429847717285, 0.9998375177383423, 0.9988847970962524, 0.9997563362121582, 0.998084306716919, 0.9979905486106873, 0.9986668229103088, 0.9985116124153137, 0.9998776912689209, 0.9991588592529297, 0.9996805787086487, 0.9998454451560974, 0.9992246627807617, 0.998287558555603, 0.9968515038490295, 0.999931812286377, 0.9978886842727661, 0.9972895383834839, 0.9998835325241089, 0.9987341165542603, 0.9986252188682556, 0.9999278783798218, 0.9984277486801147, 0.9999113082885742, 0.9998642802238464, 0.9999033808708191, 0.9998208284378052, 0.9998844861984253, 0.998539924621582, 0.9998776316642761, 0.999313235282898, 0.9987744092941284, 0.9999243021011353, 0.9998970031738281, 0.9992884993553162, 0.9993554353713989, 0.9973000288009644, 0.9997162818908691, 0.998697817325592, 0.9998149871826172, 0.9986388683319092, 0.9998641610145569, 0.9989742040634155, 0.999172568321228, 0.9988546967506409, 0.9981204271316528, 0.9995134472846985, 0.9987360239028931, 0.9981271028518677, 0.9999109506607056, 0.9998996257781982, 0.9998895525932312, 0.9999094605445862, 0.9994046688079834, 0.9985653162002563, 0.9999303221702576, 0.9999372363090515, 0.9985307455062866, 0.9994997978210449, 0.999396800994873, 0.999657154083252, 0.9994179010391235, 0.9999462366104126, 0.9985190629959106, 0.9999274611473083, 0.9992644190788269, 0.9994046092033386, 0.9983561038970947, 0.9999569654464722, 0.9998811483383179, 0.9992765784263611, 0.9997609853744507, 0.9981203079223633, 0.9991976022720337, 0.9988832473754883, 0.9990407824516296, 0.9998456239700317, 0.9990077018737793, 0.9998125433921814, 0.9999324083328247, 0.9972093105316162, 0.9988043308258057, 0.998216986656189, 0.9997801184654236, 0.9996417760848999, 0.9988898038864136, 0.9997556209564209, 0.9966895580291748, 0.9991740584373474, 0.9997421503067017, 0.9998817443847656, 0.9996973276138306, 0.9993761777877808, 0.9988635778427124, 0.9997043609619141, 0.9990219473838806, 0.9977937340736389, 0.9973617792129517, 0.9993018507957458, 0.9973706603050232, 0.999912440776825, 0.9994292259216309, 0.9993227124214172, 0.9998746514320374, 0.9985033869743347, 0.9984601140022278, 0.9960794448852539, 0.9984830617904663, 0.9997705221176147, 0.9998862743377686, 0.9986042380332947, 0.9981850385665894, 0.9987744688987732, 0.9984833598136902, 0.9994803667068481, 0.9999314546585083, 0.9989604949951172, 0.9997807145118713, 0.9987928867340088, 0.9984774589538574, 0.9997680187225342, 0.9994516968727112, 0.9997082948684692, 0.9997000098228455, 0.9998587369918823, 0.9999494552612305, 0.9995683431625366, 0.9989466071128845, 0.9999243021011353, 0.999754011631012, 0.999823808670044, 0.9984254837036133, 0.9998261332511902, 0.9986230134963989, 0.999170184135437, 0.9988586902618408, 0.9998992085456848, 0.9989684820175171, 0.9992255568504333, 0.9997845888137817, 0.9997178316116333, 0.999384343624115, 0.9998425245285034, 0.9978722333908081, 0.9988916516304016, 0.9982365965843201, 0.9998376369476318, 0.9993915557861328, 0.9998196959495544, 0.9999386668205261, 0.997816801071167, 0.9998441934585571, 0.9991452693939209, 0.9997563362121582, 0.9993737936019897, 0.998464822769165, 0.9998584985733032, 0.9997140169143677, 0.9984142780303955, 0.9997304677963257, 0.9989069700241089, 0.9994644522666931, 0.9998246431350708, 0.9993113279342651, 0.9990194439888, 0.9995617270469666, 0.9999022483825684, 0.9992240071296692, 0.9985069036483765, 0.9984195232391357, 0.9997872114181519, 0.9998379349708557, 0.997197151184082, 0.9995969533920288, 0.9995324611663818, 0.9988307952880859, 0.9983053207397461, 0.9995023608207703, 0.997200608253479, 0.9995826482772827, 0.999752938747406, 0.9992525577545166, 0.9980036020278931, 0.9998162388801575, 0.999672532081604, 0.9999250173568726, 0.9998061656951904, 0.9997498393058777, 0.9994298815727234, 0.9984696507453918, 0.9995737671852112, 0.9995540976524353, 0.9997684359550476, 0.9998924732208252, 0.9997559785842896, 0.9985721707344055, 0.9980916380882263, 0.996601939201355, 0.9999029636383057, 0.9976810216903687, 0.9989460706710815, 0.9999071955680847, 0.9999595284461975, 0.9999300241470337, 0.9999126195907593]\n",
      "[0.9999085068702698, 0.9998264312744141, 0.9987354874610901, 0.9988824129104614, 0.9997507333755493, 0.9998631477355957, 0.9999422430992126, 0.998801589012146, 0.9999265074729919, 0.9993093609809875, 0.9986477494239807, 0.9996054768562317, 0.9981595277786255, 0.9985826015472412, 0.9983531832695007, 0.9981997013092041, 0.9974825978279114, 0.9997527599334717, 0.9998174905776978, 0.9967769980430603, 0.9998580813407898, 0.9999169111251831, 0.9971139430999756, 0.9991588592529297, 0.999847948551178, 0.9986709356307983, 0.996971607208252, 0.9996975660324097, 0.9999386072158813, 0.9989314079284668, 0.9997820854187012, 0.9999055862426758, 0.9986538887023926, 0.9983406662940979, 0.996902585029602, 0.9998854398727417, 0.9993979334831238, 0.9998020529747009, 0.9987947940826416, 0.9974803328514099, 0.9991105794906616, 0.9997406005859375, 0.9997470378875732, 0.9995201826095581, 0.9979422688484192, 0.999867856502533, 0.9998331069946289, 0.9999546408653259, 0.9977989196777344, 0.9998893737792969, 0.9988623857498169, 0.9992014169692993, 0.9995838403701782, 0.9998347759246826, 0.9991002082824707, 0.9997050762176514, 0.9970834851264954, 0.9994044303894043, 0.9999176263809204, 0.9997403621673584, 0.9997901916503906, 0.9998739957809448, 0.9987324476242065, 0.9998090863227844, 0.9987586140632629, 0.9998946189880371, 0.9988723993301392, 0.9977952837944031, 0.9980988502502441, 0.9999288320541382, 0.9993075132369995, 0.9973893165588379, 0.9989801049232483, 0.9997900724411011, 0.9992251992225647, 0.9993681311607361, 0.9993007183074951, 0.9995887279510498, 0.9999169707298279, 0.9991260170936584, 0.9990127086639404, 0.9982743263244629, 0.9991824626922607, 0.9981250762939453, 0.9990065693855286, 0.9982156157493591, 0.9998081922531128, 0.9997158050537109, 0.9987375140190125, 0.999908983707428, 0.9998828768730164, 0.9996989965438843, 0.9979165196418762, 0.9991635084152222, 0.9987497925758362, 0.9991738200187683, 0.9999150633811951, 0.9987010955810547, 0.9997840523719788, 0.9995224475860596, 0.9991748332977295, 0.9997521638870239, 0.9985631108283997, 0.9998425245285034, 0.9998193979263306, 0.9991014003753662, 0.9998992681503296, 0.996636688709259, 0.9999125599861145, 0.9984838962554932, 0.9998908638954163, 0.998700737953186, 0.9985437393188477, 0.9997384548187256, 0.9998771548271179, 0.9994224309921265, 0.9998693466186523, 0.9987661838531494, 0.9990246891975403, 0.9998476505279541, 0.9996325373649597, 0.9999035596847534, 0.9979430437088013, 0.9988268613815308, 0.9992208480834961, 0.9991329312324524, 0.9990877509117126, 0.9970409870147705, 0.9993195533752441, 0.9984008073806763, 0.9995009899139404, 0.9997316598892212, 0.9990404844284058, 0.9983020424842834, 0.9998613595962524, 0.9980859756469727, 0.9982138872146606, 0.9985712170600891, 0.9975378513336182, 0.9998577833175659, 0.9988741278648376, 0.999520480632782, 0.9984778761863708, 0.9992318153381348, 0.9973493218421936, 0.9988462924957275, 0.999906599521637, 0.9984569549560547, 0.9999469518661499, 0.998653769493103, 0.9994494915008545, 0.9999233484268188, 0.9992052316665649, 0.9995039105415344, 0.9997138977050781, 0.9986769556999207, 0.9962832927703857, 0.9999478459358215, 0.9964942932128906, 0.9981716871261597, 0.9998981952667236, 0.9975782036781311, 0.9980781674385071, 0.9994169473648071, 0.9988637566566467, 0.9986416697502136, 0.9982690811157227, 0.9998676180839539, 0.9992700815200806, 0.9998302459716797, 0.9996389150619507, 0.9980711340904236, 0.9996932744979858, 0.9992707371711731, 0.999005913734436, 0.9990708231925964, 0.9994896650314331, 0.9990949034690857, 0.999844491481781, 0.999707818031311, 0.9992868900299072, 0.999798059463501, 0.9991675615310669, 0.9999088048934937, 0.9992443323135376, 0.9994978308677673, 0.9995974898338318, 0.9991379976272583, 0.9996344447135925, 0.9999001622200012, 0.998794436454773, 0.999846339225769, 0.9992159008979797, 0.9984773993492126, 0.9991565346717834, 0.9983699321746826, 0.9990934133529663, 0.9973364472389221, 0.9991577863693237, 0.9988172054290771, 0.9976654052734375, 0.9983081221580505, 0.999075710773468, 0.9982101917266846, 0.9996824264526367, 0.9999405145645142, 0.9997361898422241, 0.9995901584625244, 0.999599814414978, 0.9998335838317871, 0.9991534352302551, 0.9998084902763367, 0.9971828460693359, 0.9999115467071533, 0.9972858428955078, 0.9997108578681946, 0.9994844198226929, 0.9998418092727661, 0.9985952973365784, 0.999031662940979, 0.9995890855789185, 0.9982172250747681, 0.9985796213150024, 0.9982638359069824, 0.9998009204864502, 0.9998492002487183, 0.9981787204742432, 0.9991846084594727, 0.9998446106910706, 0.9998352527618408, 0.9995238780975342, 0.9998725652694702, 0.9997952580451965, 0.9987772703170776, 0.9999469518661499, 0.9998502135276794, 0.9980003833770752, 0.9983484148979187, 0.9991778135299683, 0.9991136789321899, 0.9992036819458008, 0.9996697902679443, 0.9983243942260742, 0.9998915791511536, 0.9986603260040283, 0.9978305101394653, 0.9999409914016724, 0.9976918697357178, 0.9998782873153687, 0.999366283416748, 0.9999201893806458, 0.9991540908813477, 0.9988399744033813, 0.9990406036376953, 0.9999488592147827, 0.9978163242340088]\n",
      "[0.9976420402526855, 0.998936653137207, 0.9983288645744324, 0.9998387098312378, 0.9973040223121643, 0.999789834022522, 0.9972974061965942, 0.9998481273651123, 0.9989805221557617, 0.999909520149231, 0.9987889528274536, 0.9998691082000732, 0.9994954466819763, 0.9999472498893738, 0.998602032661438, 0.9998359680175781, 0.9992139339447021, 0.9978598356246948, 0.9979730844497681, 0.9992663860321045, 0.9972324967384338, 0.9998640418052673, 0.9988417029380798, 0.999118983745575, 0.99847012758255, 0.9990869164466858, 0.9993587732315063, 0.9997456073760986, 0.9990764856338501, 0.9993770122528076, 0.9998534321784973, 0.9990926384925842, 0.9991209506988525, 0.9983077645301819, 0.9993254542350769, 0.9999440908432007, 0.9972350597381592, 0.9999349117279053, 0.9984313249588013, 0.9992456436157227, 0.999820351600647, 0.9999236464500427, 0.999093234539032, 0.9999407529830933, 0.9985162019729614, 0.9996850490570068, 0.9995239973068237, 0.9997999668121338, 0.9991466999053955, 0.9999116659164429, 0.9998795986175537, 0.9999421238899231, 0.9993197917938232, 0.999193012714386, 0.9964702129364014, 0.9983182549476624, 0.9983706474304199, 0.9976687431335449, 0.9992135763168335, 0.9997900128364563, 0.9991416931152344, 0.9982625246047974, 0.9997901916503906, 0.9998496174812317, 0.999587893486023, 0.9989734292030334, 0.9998453855514526, 0.9999027252197266, 0.9974339008331299, 0.9986677765846252, 0.9991984963417053, 0.9998000860214233, 0.9986720085144043, 0.9999160766601562, 0.9994735717773438, 0.9990010261535645, 0.9989074468612671, 0.9996844530105591, 0.9972776174545288, 0.9991469383239746, 0.9975388646125793, 0.9976440668106079, 0.9998299479484558, 0.9994075894355774, 0.9994692206382751, 0.9991453289985657, 0.9994968175888062, 0.9997416138648987, 0.9995558857917786, 0.9991152882575989, 0.9999221563339233, 0.9998910427093506, 0.9993146061897278, 0.9997768402099609, 0.9999194145202637, 0.9981895089149475, 0.9990085959434509, 0.9998273849487305, 0.9997771978378296, 0.9989197254180908, 0.9994017481803894, 0.9971668124198914, 0.9968856573104858, 0.9999227523803711, 0.9992911219596863, 0.9998216032981873, 0.998725175857544, 0.9994117617607117, 0.9989991188049316, 0.9982579350471497, 0.9991496801376343, 0.9996926784515381, 0.9993337392807007, 0.9997396469116211, 0.9994762539863586, 0.9995852112770081, 0.9989673495292664, 0.9997831583023071, 0.9995602369308472, 0.9997906684875488, 0.9999484419822693, 0.9991116523742676, 0.9998350143432617, 0.9992666244506836, 0.9998375177383423, 0.9989430904388428, 0.9998673796653748, 0.999718964099884, 0.999565064907074, 0.9985911846160889, 0.9997536540031433, 0.9998903274536133, 0.9980380535125732, 0.999888002872467, 0.9998223781585693, 0.9977665543556213, 0.9986932277679443, 0.9998210072517395, 0.9998679161071777, 0.9993061423301697, 0.9987419843673706, 0.999772310256958, 0.9997460246086121, 0.9999411106109619, 0.9998264312744141, 0.9968059659004211, 0.9975917935371399, 0.9997129440307617, 0.9992144107818604, 0.9968552589416504, 0.9998853206634521, 0.9994084239006042, 0.9974443912506104, 0.9999155402183533, 0.9998435378074646, 0.9986447095870972, 0.9986419677734375, 0.9990866184234619, 0.9983571171760559, 0.9995815753936768, 0.9977810978889465, 0.9997532963752747, 0.9989220499992371, 0.999269962310791, 0.9990702867507935, 0.9985606670379639, 0.9988474249839783, 0.9979149103164673, 0.9997583031654358, 0.9987436532974243, 0.9997497797012329, 0.9998389482498169, 0.9973722696304321, 0.9989063739776611, 0.99735426902771, 0.9997971057891846, 0.9999264478683472, 0.9993607997894287, 0.9987068772315979, 0.9971233606338501, 0.9999271631240845, 0.9997962713241577, 0.9997327327728271, 0.9999268651008606, 0.9993505477905273, 0.99774169921875, 0.9998376369476318, 0.9994141459465027, 0.9998645782470703, 0.9992422461509705, 0.9989357590675354, 0.9985663294792175, 0.9999125003814697, 0.999936580657959, 0.9972739219665527, 0.9977301955223083, 0.9980834126472473, 0.9992853999137878, 0.9988119602203369, 0.9998741149902344, 0.99958735704422, 0.9984702467918396, 0.9987640380859375, 0.9999242424964905, 0.9990903735160828, 0.9992088675498962, 0.9984036087989807, 0.9990531206130981, 0.998544454574585, 0.9998831748962402, 0.9992117285728455, 0.9982457160949707, 0.9999539852142334, 0.9982817769050598, 0.9987715482711792, 0.9984518885612488, 0.9991710782051086, 0.9998692274093628, 0.9985731840133667, 0.9999052286148071, 0.9994497895240784, 0.999923825263977, 0.9987510442733765, 0.9998525977134705, 0.9978250861167908, 0.9998922944068909, 0.9994509220123291, 0.9996563196182251, 0.9999204874038696, 0.9998689889907837, 0.9995000958442688, 0.9999189376831055, 0.9987067580223083, 0.9999024868011475, 0.9999179840087891, 0.9999545216560364, 0.9997506737709045, 0.9999492764472961, 0.9998953938484192, 0.9998354911804199, 0.9997395873069763, 0.9986818432807922, 0.9995999336242676, 0.9996951818466187, 0.9992684125900269, 0.9998819231987, 0.9992703199386597, 0.9996917843818665, 0.999075174331665, 0.9990720152854919, 0.9998598098754883, 0.9971967935562134, 0.999259352684021, 0.9966624975204468, 0.9998165369033813, 0.9994876384735107]\n",
      "[0.9999089241027832, 0.999285101890564, 0.9961562156677246, 0.9997745752334595, 0.9998099207878113, 0.9986923336982727, 0.9975904226303101, 0.9991785287857056, 0.9992237091064453, 0.9977243542671204, 0.9996947050094604, 0.9985675811767578, 0.9994972944259644, 0.9967268705368042, 0.998982310295105, 0.9987103939056396, 0.999686062335968, 0.99936842918396, 0.9995971918106079, 0.9987425804138184, 0.9995119571685791, 0.9998229742050171, 0.999143123626709, 0.9997204542160034, 0.9999063014984131, 0.9995117783546448, 0.9997541904449463, 0.998908519744873, 0.9997545480728149, 0.9983089566230774, 0.9996664524078369, 0.999915599822998, 0.9980307221412659, 0.9993945360183716, 0.9990576505661011, 0.9991996884346008, 0.9998634457588196, 0.9966586232185364, 0.9976720213890076, 0.9988579154014587, 0.999933123588562, 0.9999350309371948, 0.9992150664329529, 0.999915599822998, 0.9998444318771362, 0.9985254406929016, 0.999858558177948, 0.9998957514762878, 0.9998723268508911, 0.9998601078987122, 0.998482882976532, 0.9981573224067688, 0.9999313950538635, 0.9993045330047607, 0.9982016682624817, 0.9993658065795898, 0.998514711856842, 0.9999423027038574, 0.9995182156562805, 0.9989251494407654, 0.9990123510360718, 0.9999278783798218, 0.9982234835624695, 0.9996331930160522, 0.9998315572738647, 0.9997204542160034, 0.9996595978736877, 0.9986838102340698, 0.9994027018547058, 0.9998105764389038, 0.998917818069458, 0.9968555569648743, 0.9984457492828369, 0.9989526271820068, 0.999858021736145, 0.9980345964431763, 0.9998780488967896, 0.9982426762580872, 0.9981828927993774, 0.998776376247406, 0.9991439580917358, 0.9999099969863892, 0.9982585906982422, 0.9998764991760254, 0.9987943172454834, 0.9999467730522156, 0.9993930459022522, 0.9985359907150269, 0.9991141557693481, 0.9993525147438049, 0.9970576763153076, 0.9992823004722595, 0.9997702240943909, 0.9998089075088501, 0.9998804926872253, 0.9985265731811523, 0.9999367594718933, 0.9986688494682312, 0.9987311363220215, 0.9998981952667236, 0.9978398680686951, 0.9996343851089478, 0.999515950679779, 0.9983991980552673, 0.9997822642326355, 0.9999186396598816, 0.9988452196121216, 0.9996182918548584, 0.9985821843147278, 0.9998377561569214, 0.999316394329071, 0.9969753623008728, 0.9999040365219116, 0.9989945292472839, 0.999282717704773, 0.9993417859077454, 0.999332070350647, 0.9985164999961853, 0.9980705380439758, 0.9973596334457397, 0.9998751282691956, 0.9997208118438721, 0.9986632466316223, 0.9999307990074158, 0.9998113512992859, 0.999768853187561, 0.9999244213104248, 0.9986768364906311, 0.9983035922050476, 0.999184250831604, 0.9999086856842041, 0.9997715353965759, 0.9999004006385803, 0.9999045133590698, 0.9984018802642822, 0.9991917014122009, 0.9999176859855652, 0.9996258020401001, 0.9993152618408203, 0.9998157024383545, 0.9997798204421997, 0.9999110698699951, 0.999287486076355, 0.9980764985084534, 0.9998835325241089, 0.9997250437736511, 0.9997448325157166, 0.9995691180229187, 0.9997997283935547, 0.9986832141876221, 0.9959900379180908, 0.9999459981918335, 0.9993494153022766, 0.9998337626457214, 0.9995501041412354, 0.9999438524246216, 0.9982824325561523, 0.9976228475570679, 0.9984570741653442, 0.9984099864959717, 0.9991849660873413, 0.9990109205245972, 0.9989849328994751, 0.9989650249481201, 0.999875545501709, 0.9993035793304443, 0.9985314607620239, 0.9998967051506042, 0.9998176097869873, 0.9987595081329346, 0.9999141693115234, 0.9980599284172058, 0.9978657960891724, 0.9989122152328491, 0.9976470470428467, 0.9997164011001587, 0.9999083876609802, 0.9993702173233032, 0.9999024868011475, 0.9998803734779358, 0.9972990155220032, 0.9965035319328308, 0.9999165534973145, 0.9988492727279663, 0.9984325766563416, 0.9983766078948975, 0.9994134902954102, 0.9977994561195374, 0.9998690485954285, 0.9994794726371765, 0.9972975254058838, 0.9994909167289734, 0.9997285008430481, 0.9990196228027344, 0.9987356662750244, 0.9997393488883972, 0.9997960329055786, 0.9997660517692566, 0.9973458647727966, 0.9999563097953796, 0.9986557960510254, 0.9999068379402161, 0.9997719526290894, 0.9996538162231445, 0.9991283416748047, 0.9999271631240845, 0.9995143413543701, 0.9999250769615173, 0.998807966709137, 0.9991875886917114, 0.999123215675354, 0.9995055794715881, 0.998550534248352, 0.9999361634254456, 0.9982309341430664, 0.9997825026512146, 0.9998575448989868, 0.9996943473815918, 0.9987677335739136, 0.9997987747192383, 0.996653139591217, 0.9967737793922424, 0.9984732866287231, 0.9989924430847168, 0.9996688961982727, 0.9997866153717041, 0.999933660030365, 0.9980464577674866, 0.9997733235359192, 0.9999150037765503, 0.9976136684417725, 0.9999239444732666, 0.9986313581466675, 0.9999085664749146, 0.9985329508781433, 0.9997953176498413, 0.998723030090332, 0.9988219738006592, 0.9993786811828613, 0.9998913407325745, 0.9987865686416626, 0.9998981952667236, 0.9989430904388428, 0.9998198747634888, 0.9998940229415894, 0.999775767326355, 0.9998990297317505, 0.9996408224105835, 0.9995651245117188, 0.9999628067016602, 0.9986646175384521, 0.9996455311775208, 0.9984173774719238, 0.9990680813789368, 0.9987184405326843, 0.9982728362083435]\n",
      "[0.9991840124130249, 0.9972879886627197, 0.999753475189209, 0.9988601207733154, 0.9989839792251587, 0.9997110962867737, 0.9969950914382935, 0.9987953305244446, 0.9993005394935608, 0.9983367323875427, 0.9992662668228149, 0.9996216893196106, 0.9973354339599609, 0.9984493255615234, 0.9996014833450317, 0.9993526935577393, 0.9997041821479797, 0.9998098015785217, 0.998814046382904, 0.9987322688102722, 0.9997339248657227, 0.9984198212623596, 0.9998186826705933, 0.9982779026031494, 0.9990788698196411, 0.9986093640327454, 0.9995911121368408, 0.9998868107795715, 0.999924898147583, 0.9975630640983582, 0.9999173283576965, 0.9989629983901978, 0.9997020363807678, 0.9991316795349121, 0.999626874923706, 0.9962708950042725, 0.9968379139900208, 0.9980692863464355, 0.9998931884765625, 0.9990107417106628, 0.9998948574066162, 0.9997793436050415, 0.9999068975448608, 0.9983687400817871, 0.9986453056335449, 0.9998204708099365, 0.9986904859542847, 0.9998079538345337, 0.9999057054519653, 0.9997501969337463, 0.9973933100700378, 0.9999578595161438, 0.999907910823822, 0.9985837936401367, 0.9984067678451538, 0.9999145865440369, 0.9993122816085815, 0.9987822771072388, 0.9998515248298645, 0.9980258941650391, 0.9998767971992493, 0.9987161159515381, 0.9982296228408813, 0.9993934035301208, 0.9977689981460571, 0.9990436434745789, 0.9985411763191223, 0.9989358186721802, 0.9987273812294006, 0.997414231300354, 0.9997515082359314, 0.9998942613601685, 0.9998646974563599, 0.9991576671600342, 0.9997817873954773, 0.999893307685852, 0.9999464750289917, 0.998428225517273, 0.9985273480415344, 0.9998756647109985, 0.9990953803062439, 0.9987740516662598, 0.9996364116668701, 0.9991675019264221, 0.9997683167457581, 0.9991464614868164, 0.9998795390129089, 0.9990119934082031, 0.9998559951782227, 0.9988129734992981, 0.9992639422416687, 0.998958170413971, 0.9999111890792847, 0.9999616146087646, 0.999536395072937, 0.9996716976165771, 0.9998820424079895, 0.9980688095092773, 0.9990127682685852, 0.9998806118965149, 0.999117910861969, 0.9987110495567322, 0.9998633861541748, 0.9991147518157959, 0.9995081424713135, 0.9998937249183655, 0.9990232586860657, 0.9997340440750122, 0.9994556307792664, 0.9997785687446594, 0.9992365837097168, 0.9998158812522888, 0.9998826384544373, 0.9998944401741028, 0.9998504519462585, 0.9993897676467896, 0.9987617135047913, 0.999401330947876, 0.9997682571411133, 0.9997831583023071, 0.9969304800033569, 0.99994957447052, 0.9992959499359131, 0.9995399713516235, 0.9975691437721252, 0.9999517798423767, 0.9981755018234253, 0.9998245239257812, 0.9987475872039795, 0.9992128014564514, 0.9998379349708557, 0.9993950128555298, 0.9986753463745117, 0.9979684948921204, 0.9990721344947815, 0.9989818334579468, 0.9997987747192383, 0.9999347925186157, 0.9995733499526978, 0.9988692998886108, 0.9999402165412903, 0.9988177418708801, 0.9976276755332947, 0.9997825026512146, 0.9998674988746643, 0.9994648694992065, 0.9991101622581482, 0.999763011932373, 0.9987639784812927, 0.9976691603660583, 0.9998408555984497, 0.9991174340248108, 0.9997633695602417, 0.9993472695350647, 0.9986371994018555, 0.9966471195220947, 0.9989249110221863, 0.9992630481719971, 0.9987906217575073, 0.9998705983161926, 0.9997992515563965, 0.9981897473335266, 0.9988065361976624, 0.9999329447746277, 0.999546229839325, 0.9991136789321899, 0.9998984336853027, 0.9977855086326599, 0.9988614320755005, 0.9989410042762756, 0.9978317022323608, 0.9996945858001709, 0.9997882843017578, 0.999907910823822, 0.9981369376182556, 0.9968757033348083, 0.9998553395271301, 0.9989051818847656, 0.9988178014755249, 0.9996136426925659, 0.9997393488883972, 0.9998411536216736, 0.9995968341827393, 0.9990185499191284, 0.9983498454093933, 0.9999202489852905, 0.999904453754425, 0.9998745918273926, 0.9998887181282043, 0.9994067549705505, 0.998943567276001, 0.9980454444885254, 0.999732494354248, 0.998965859413147, 0.9998055100440979, 0.9985601902008057, 0.9997843503952026, 0.9998356103897095, 0.9978770017623901, 0.9997870922088623, 0.9992882013320923, 0.9998751878738403, 0.9968625903129578, 0.9993506669998169, 0.999075174331665, 0.9998908042907715, 0.999641478061676, 0.9999029636383057, 0.9995813369750977, 0.999755859375, 0.9998681545257568, 0.9993349313735962, 0.9999674558639526, 0.9997727870941162, 0.9997111558914185, 0.9984316825866699, 0.9986727237701416, 0.9959431886672974, 0.9999136924743652, 0.9994819760322571, 0.999905526638031, 0.999782919883728, 0.9985747337341309, 0.9993918538093567, 0.9998446702957153, 0.9998650550842285, 0.9993352890014648, 0.9998120665550232, 0.9999016523361206, 0.9966787695884705, 0.9971175193786621, 0.9999253749847412, 0.9998854398727417, 0.9981653690338135]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "535"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = prediction(model, test_loader, device)\n",
    "sum(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fecf184-57e8-4969-bce1-46da090bdd32",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "05157612-68bf-4102-a723-8c0ad0f76755",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv(path+'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "38b5a428-5c17-4056-a3fe-4f50e30412ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAMPLE_ID</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_0001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_0002</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_0003</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_0004</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1509</th>\n",
       "      <td>TEST_1509</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1510</th>\n",
       "      <td>TEST_1510</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1511</th>\n",
       "      <td>TEST_1511</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1512</th>\n",
       "      <td>TEST_1512</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1513</th>\n",
       "      <td>TEST_1513</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1514 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      SAMPLE_ID  LABEL\n",
       "0     TEST_0000      0\n",
       "1     TEST_0001      0\n",
       "2     TEST_0002      0\n",
       "3     TEST_0003      1\n",
       "4     TEST_0004      0\n",
       "...         ...    ...\n",
       "1509  TEST_1509      1\n",
       "1510  TEST_1510      1\n",
       "1511  TEST_1511      0\n",
       "1512  TEST_1512      0\n",
       "1513  TEST_1513      1\n",
       "\n",
       "[1514 rows x 2 columns]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit['LABEL'] = preds\n",
    "submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6c1c8567-eacd-4de3-b5e2-490102224edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('submit.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2fe3fa16-acc3-4628-a401-bc752544e41b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "535"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit['LABEL'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe23aea0-10f0-4d16-a1a8-67544c34adea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298ca0c6-67df-40b9-a2f0-3b67b25b77bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
