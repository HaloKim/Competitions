{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2bcba5f-002e-4f49-9622-ada6117faf0a",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b0d9b68-7102-4eca-9543-3b9b8acafc6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hist\\miniconda3\\envs\\ml\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import librosa\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba7851e-f369-4b1b-b5e3-b45233535157",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7c0d625-02ae-42bf-b87c-bc60423e2284",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1000\n",
    "LR = 1e-1\n",
    "SR = 16000\n",
    "SEED = 42\n",
    "N_MFCC = 128\n",
    "BATCH = 256\n",
    "device = 'cuda'\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(SEED) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a4172e-5791-446f-9616-35c09d8bf25a",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a62c78cd-4f40-4e98-b8a6-1b6f1d906b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'open/'\n",
    "train_df = pd.read_csv(path+'train.csv') # 모두 정상 Sample\n",
    "test_df = pd.read_csv(path+'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33f37236-257c-44ab-8a9f-b2daf6c50ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAMPLE_ID</th>\n",
       "      <th>SAMPLE_PATH</th>\n",
       "      <th>FAN_TYPE</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>./train/TRAIN_0000.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>./train/TRAIN_0001.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>./train/TRAIN_0002.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>./train/TRAIN_0003.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>./train/TRAIN_0004.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274</th>\n",
       "      <td>TRAIN_1274</td>\n",
       "      <td>./train/TRAIN_1274.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275</th>\n",
       "      <td>TRAIN_1275</td>\n",
       "      <td>./train/TRAIN_1275.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1276</th>\n",
       "      <td>TRAIN_1276</td>\n",
       "      <td>./train/TRAIN_1276.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1277</th>\n",
       "      <td>TRAIN_1277</td>\n",
       "      <td>./train/TRAIN_1277.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1278</th>\n",
       "      <td>TRAIN_1278</td>\n",
       "      <td>./train/TRAIN_1278.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1279 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       SAMPLE_ID             SAMPLE_PATH  FAN_TYPE  LABEL\n",
       "0     TRAIN_0000  ./train/TRAIN_0000.wav         2      0\n",
       "1     TRAIN_0001  ./train/TRAIN_0001.wav         0      0\n",
       "2     TRAIN_0002  ./train/TRAIN_0002.wav         0      0\n",
       "3     TRAIN_0003  ./train/TRAIN_0003.wav         2      0\n",
       "4     TRAIN_0004  ./train/TRAIN_0004.wav         2      0\n",
       "...          ...                     ...       ...    ...\n",
       "1274  TRAIN_1274  ./train/TRAIN_1274.wav         2      0\n",
       "1275  TRAIN_1275  ./train/TRAIN_1275.wav         2      0\n",
       "1276  TRAIN_1276  ./train/TRAIN_1276.wav         2      0\n",
       "1277  TRAIN_1277  ./train/TRAIN_1277.wav         2      0\n",
       "1278  TRAIN_1278  ./train/TRAIN_1278.wav         0      0\n",
       "\n",
       "[1279 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee800463-995c-43ac-a05d-f3988923add9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mfcc_feature(df):\n",
    "    features = []\n",
    "    features2 = []\n",
    "    for path in tqdm(df['SAMPLE_PATH']):\n",
    "        # librosa패키지를 사용하여 wav 파일 load\n",
    "        y, sr = librosa.load(path, sr=SR)\n",
    "        \n",
    "        # melspectrogram\n",
    "        mels = librosa.feature.melspectrogram(y, sr=sr, n_mels=N_MFCC)\n",
    "        mels = librosa.power_to_db(mels, ref=np.max)\n",
    "        \n",
    "        # librosa패키지를 사용하여 mfcc 추출\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=N_MFCC)\n",
    "        \n",
    "        y_feature2 = []\n",
    "        # 추출된 MFCC들의 평균을 Feature로 사용\n",
    "        for e in mels:\n",
    "            y_feature2.append(np.mean(e))\n",
    "        features2.append(y_feature2)\n",
    "        \n",
    "        y_feature = []\n",
    "        # 추출된 MFCC들의 평균을 Feature로 사용\n",
    "        for e in mfcc:\n",
    "            y_feature.append(np.mean(e))\n",
    "        features.append(y_feature)\n",
    "    return features, features2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5b4be22-d948-4407-afe9-cf16c4281826",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1279/1279 [00:27<00:00, 46.36it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1514/1514 [00:32<00:00, 46.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 21s\n",
      "Wall time: 1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_features, train_features2 = get_mfcc_feature(train_df)\n",
    "test_features, test_features2 = get_mfcc_feature(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c86bd05-d873-4943-885c-b7efffe41048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 반복문으로 전체 컬럼명 변경하기\n",
    "def rename(df):\n",
    "    flag = 0\n",
    "    for col_name in df.columns:\n",
    "        if col_name == 0:\n",
    "            flag = 1\n",
    "        if flag == 1:\n",
    "            df.rename(columns = {col_name : 128+col_name}, inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03d6602e-38a0-4a7e-99b4-ba10b6d03d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAMPLE_ID</th>\n",
       "      <th>SAMPLE_PATH</th>\n",
       "      <th>FAN_TYPE</th>\n",
       "      <th>LABEL</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>...</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>./train/TRAIN_0000.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.952273</td>\n",
       "      <td>0.284621</td>\n",
       "      <td>0.295843</td>\n",
       "      <td>0.363586</td>\n",
       "      <td>0.357530</td>\n",
       "      <td>0.294327</td>\n",
       "      <td>...</td>\n",
       "      <td>0.833846</td>\n",
       "      <td>0.842291</td>\n",
       "      <td>0.818590</td>\n",
       "      <td>0.817896</td>\n",
       "      <td>0.799984</td>\n",
       "      <td>0.799645</td>\n",
       "      <td>0.811261</td>\n",
       "      <td>0.816584</td>\n",
       "      <td>0.825692</td>\n",
       "      <td>0.805742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>./train/TRAIN_0001.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.081981</td>\n",
       "      <td>0.935459</td>\n",
       "      <td>0.514880</td>\n",
       "      <td>0.613406</td>\n",
       "      <td>0.691659</td>\n",
       "      <td>0.825306</td>\n",
       "      <td>...</td>\n",
       "      <td>0.148846</td>\n",
       "      <td>0.144862</td>\n",
       "      <td>0.149996</td>\n",
       "      <td>0.165012</td>\n",
       "      <td>0.156853</td>\n",
       "      <td>0.160930</td>\n",
       "      <td>0.163661</td>\n",
       "      <td>0.162874</td>\n",
       "      <td>0.158788</td>\n",
       "      <td>0.084692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>./train/TRAIN_0002.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.240260</td>\n",
       "      <td>0.664368</td>\n",
       "      <td>0.724483</td>\n",
       "      <td>0.354632</td>\n",
       "      <td>0.625930</td>\n",
       "      <td>0.695975</td>\n",
       "      <td>...</td>\n",
       "      <td>0.454258</td>\n",
       "      <td>0.520886</td>\n",
       "      <td>0.454087</td>\n",
       "      <td>0.465858</td>\n",
       "      <td>0.423989</td>\n",
       "      <td>0.497721</td>\n",
       "      <td>0.504808</td>\n",
       "      <td>0.502295</td>\n",
       "      <td>0.487844</td>\n",
       "      <td>0.409240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>./train/TRAIN_0003.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.943729</td>\n",
       "      <td>0.295295</td>\n",
       "      <td>0.312391</td>\n",
       "      <td>0.371455</td>\n",
       "      <td>0.329344</td>\n",
       "      <td>0.268686</td>\n",
       "      <td>...</td>\n",
       "      <td>0.808817</td>\n",
       "      <td>0.827032</td>\n",
       "      <td>0.815166</td>\n",
       "      <td>0.787913</td>\n",
       "      <td>0.779337</td>\n",
       "      <td>0.781981</td>\n",
       "      <td>0.771998</td>\n",
       "      <td>0.779565</td>\n",
       "      <td>0.796365</td>\n",
       "      <td>0.798277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>./train/TRAIN_0004.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.949608</td>\n",
       "      <td>0.188729</td>\n",
       "      <td>0.179787</td>\n",
       "      <td>0.154144</td>\n",
       "      <td>0.007228</td>\n",
       "      <td>0.055841</td>\n",
       "      <td>...</td>\n",
       "      <td>0.877542</td>\n",
       "      <td>0.909439</td>\n",
       "      <td>0.894097</td>\n",
       "      <td>0.863824</td>\n",
       "      <td>0.849838</td>\n",
       "      <td>0.867351</td>\n",
       "      <td>0.865360</td>\n",
       "      <td>0.861068</td>\n",
       "      <td>0.868706</td>\n",
       "      <td>0.875541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274</th>\n",
       "      <td>TRAIN_1274</td>\n",
       "      <td>./train/TRAIN_1274.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.964989</td>\n",
       "      <td>0.096119</td>\n",
       "      <td>0.129592</td>\n",
       "      <td>0.222531</td>\n",
       "      <td>0.039493</td>\n",
       "      <td>0.075616</td>\n",
       "      <td>...</td>\n",
       "      <td>0.857574</td>\n",
       "      <td>0.875432</td>\n",
       "      <td>0.862135</td>\n",
       "      <td>0.853636</td>\n",
       "      <td>0.852105</td>\n",
       "      <td>0.854758</td>\n",
       "      <td>0.852087</td>\n",
       "      <td>0.851688</td>\n",
       "      <td>0.860304</td>\n",
       "      <td>0.881918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275</th>\n",
       "      <td>TRAIN_1275</td>\n",
       "      <td>./train/TRAIN_1275.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.959506</td>\n",
       "      <td>0.283203</td>\n",
       "      <td>0.293961</td>\n",
       "      <td>0.390142</td>\n",
       "      <td>0.322176</td>\n",
       "      <td>0.239990</td>\n",
       "      <td>...</td>\n",
       "      <td>0.842018</td>\n",
       "      <td>0.853391</td>\n",
       "      <td>0.822730</td>\n",
       "      <td>0.825484</td>\n",
       "      <td>0.822317</td>\n",
       "      <td>0.809976</td>\n",
       "      <td>0.820805</td>\n",
       "      <td>0.835689</td>\n",
       "      <td>0.836974</td>\n",
       "      <td>0.819587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1276</th>\n",
       "      <td>TRAIN_1276</td>\n",
       "      <td>./train/TRAIN_1276.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.930908</td>\n",
       "      <td>0.223856</td>\n",
       "      <td>0.210020</td>\n",
       "      <td>0.151861</td>\n",
       "      <td>0.134018</td>\n",
       "      <td>0.149416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.918239</td>\n",
       "      <td>0.932157</td>\n",
       "      <td>0.898576</td>\n",
       "      <td>0.905386</td>\n",
       "      <td>0.894843</td>\n",
       "      <td>0.879353</td>\n",
       "      <td>0.900221</td>\n",
       "      <td>0.910439</td>\n",
       "      <td>0.903620</td>\n",
       "      <td>0.895840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1277</th>\n",
       "      <td>TRAIN_1277</td>\n",
       "      <td>./train/TRAIN_1277.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.932890</td>\n",
       "      <td>0.247222</td>\n",
       "      <td>0.265837</td>\n",
       "      <td>0.223214</td>\n",
       "      <td>0.170546</td>\n",
       "      <td>0.141445</td>\n",
       "      <td>...</td>\n",
       "      <td>0.922436</td>\n",
       "      <td>0.946884</td>\n",
       "      <td>0.920099</td>\n",
       "      <td>0.916849</td>\n",
       "      <td>0.900980</td>\n",
       "      <td>0.903257</td>\n",
       "      <td>0.919087</td>\n",
       "      <td>0.922190</td>\n",
       "      <td>0.920950</td>\n",
       "      <td>0.932029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1278</th>\n",
       "      <td>TRAIN_1278</td>\n",
       "      <td>./train/TRAIN_1278.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.407188</td>\n",
       "      <td>0.620077</td>\n",
       "      <td>0.356668</td>\n",
       "      <td>0.769474</td>\n",
       "      <td>0.548411</td>\n",
       "      <td>0.806605</td>\n",
       "      <td>...</td>\n",
       "      <td>0.306191</td>\n",
       "      <td>0.317135</td>\n",
       "      <td>0.329551</td>\n",
       "      <td>0.308712</td>\n",
       "      <td>0.285871</td>\n",
       "      <td>0.289114</td>\n",
       "      <td>0.280076</td>\n",
       "      <td>0.270611</td>\n",
       "      <td>0.257248</td>\n",
       "      <td>0.177652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1279 rows × 260 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       SAMPLE_ID             SAMPLE_PATH  FAN_TYPE  LABEL       128       129  \\\n",
       "0     TRAIN_0000  ./train/TRAIN_0000.wav         2      0  0.952273  0.284621   \n",
       "1     TRAIN_0001  ./train/TRAIN_0001.wav         0      0  0.081981  0.935459   \n",
       "2     TRAIN_0002  ./train/TRAIN_0002.wav         0      0  0.240260  0.664368   \n",
       "3     TRAIN_0003  ./train/TRAIN_0003.wav         2      0  0.943729  0.295295   \n",
       "4     TRAIN_0004  ./train/TRAIN_0004.wav         2      0  0.949608  0.188729   \n",
       "...          ...                     ...       ...    ...       ...       ...   \n",
       "1274  TRAIN_1274  ./train/TRAIN_1274.wav         2      0  0.964989  0.096119   \n",
       "1275  TRAIN_1275  ./train/TRAIN_1275.wav         2      0  0.959506  0.283203   \n",
       "1276  TRAIN_1276  ./train/TRAIN_1276.wav         2      0  0.930908  0.223856   \n",
       "1277  TRAIN_1277  ./train/TRAIN_1277.wav         2      0  0.932890  0.247222   \n",
       "1278  TRAIN_1278  ./train/TRAIN_1278.wav         0      0  0.407188  0.620077   \n",
       "\n",
       "           130       131       132       133  ...       118       119  \\\n",
       "0     0.295843  0.363586  0.357530  0.294327  ...  0.833846  0.842291   \n",
       "1     0.514880  0.613406  0.691659  0.825306  ...  0.148846  0.144862   \n",
       "2     0.724483  0.354632  0.625930  0.695975  ...  0.454258  0.520886   \n",
       "3     0.312391  0.371455  0.329344  0.268686  ...  0.808817  0.827032   \n",
       "4     0.179787  0.154144  0.007228  0.055841  ...  0.877542  0.909439   \n",
       "...        ...       ...       ...       ...  ...       ...       ...   \n",
       "1274  0.129592  0.222531  0.039493  0.075616  ...  0.857574  0.875432   \n",
       "1275  0.293961  0.390142  0.322176  0.239990  ...  0.842018  0.853391   \n",
       "1276  0.210020  0.151861  0.134018  0.149416  ...  0.918239  0.932157   \n",
       "1277  0.265837  0.223214  0.170546  0.141445  ...  0.922436  0.946884   \n",
       "1278  0.356668  0.769474  0.548411  0.806605  ...  0.306191  0.317135   \n",
       "\n",
       "           120       121       122       123       124       125       126  \\\n",
       "0     0.818590  0.817896  0.799984  0.799645  0.811261  0.816584  0.825692   \n",
       "1     0.149996  0.165012  0.156853  0.160930  0.163661  0.162874  0.158788   \n",
       "2     0.454087  0.465858  0.423989  0.497721  0.504808  0.502295  0.487844   \n",
       "3     0.815166  0.787913  0.779337  0.781981  0.771998  0.779565  0.796365   \n",
       "4     0.894097  0.863824  0.849838  0.867351  0.865360  0.861068  0.868706   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1274  0.862135  0.853636  0.852105  0.854758  0.852087  0.851688  0.860304   \n",
       "1275  0.822730  0.825484  0.822317  0.809976  0.820805  0.835689  0.836974   \n",
       "1276  0.898576  0.905386  0.894843  0.879353  0.900221  0.910439  0.903620   \n",
       "1277  0.920099  0.916849  0.900980  0.903257  0.919087  0.922190  0.920950   \n",
       "1278  0.329551  0.308712  0.285871  0.289114  0.280076  0.270611  0.257248   \n",
       "\n",
       "           127  \n",
       "0     0.805742  \n",
       "1     0.084692  \n",
       "2     0.409240  \n",
       "3     0.798277  \n",
       "4     0.875541  \n",
       "...        ...  \n",
       "1274  0.881918  \n",
       "1275  0.819587  \n",
       "1276  0.895840  \n",
       "1277  0.932029  \n",
       "1278  0.177652  \n",
       "\n",
       "[1279 rows x 260 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = pd.concat([train_df, pd.DataFrame(train_features)], axis=1)\n",
    "tmp = rename(tmp)\n",
    "tmp = pd.concat([tmp, pd.DataFrame(train_features2)], axis=1)\n",
    "\n",
    "test = pd.concat([test_df, pd.DataFrame(test_features)], axis=1)\n",
    "test = rename(test)\n",
    "test = pd.concat([test, pd.DataFrame(test_features2)], axis=1)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "tmp.iloc[:,range(4,len(tmp.columns))] = scaler.fit_transform(tmp.iloc[:,range(4,len(tmp.columns))])\n",
    "test.iloc[:,range(3,len(test.columns))] = scaler.transform(test.iloc[:,range(3,len(test.columns))])\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09100fa5-d563-42aa-9575-1798d8ebbd40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAMPLE_ID</th>\n",
       "      <th>SAMPLE_PATH</th>\n",
       "      <th>FAN_TYPE</th>\n",
       "      <th>LABEL</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>...</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>./train/TRAIN_0000.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.952273</td>\n",
       "      <td>0.284621</td>\n",
       "      <td>0.295843</td>\n",
       "      <td>0.363586</td>\n",
       "      <td>0.357530</td>\n",
       "      <td>0.294327</td>\n",
       "      <td>...</td>\n",
       "      <td>0.833846</td>\n",
       "      <td>0.842291</td>\n",
       "      <td>0.818590</td>\n",
       "      <td>0.817896</td>\n",
       "      <td>0.799984</td>\n",
       "      <td>0.799645</td>\n",
       "      <td>0.811261</td>\n",
       "      <td>0.816584</td>\n",
       "      <td>0.825692</td>\n",
       "      <td>0.805742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>./train/TRAIN_0001.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.081981</td>\n",
       "      <td>0.935459</td>\n",
       "      <td>0.514880</td>\n",
       "      <td>0.613406</td>\n",
       "      <td>0.691659</td>\n",
       "      <td>0.825306</td>\n",
       "      <td>...</td>\n",
       "      <td>0.148846</td>\n",
       "      <td>0.144862</td>\n",
       "      <td>0.149996</td>\n",
       "      <td>0.165012</td>\n",
       "      <td>0.156853</td>\n",
       "      <td>0.160930</td>\n",
       "      <td>0.163661</td>\n",
       "      <td>0.162874</td>\n",
       "      <td>0.158788</td>\n",
       "      <td>0.084692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>./train/TRAIN_0002.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.240260</td>\n",
       "      <td>0.664368</td>\n",
       "      <td>0.724483</td>\n",
       "      <td>0.354632</td>\n",
       "      <td>0.625930</td>\n",
       "      <td>0.695975</td>\n",
       "      <td>...</td>\n",
       "      <td>0.454258</td>\n",
       "      <td>0.520886</td>\n",
       "      <td>0.454087</td>\n",
       "      <td>0.465858</td>\n",
       "      <td>0.423989</td>\n",
       "      <td>0.497721</td>\n",
       "      <td>0.504808</td>\n",
       "      <td>0.502295</td>\n",
       "      <td>0.487844</td>\n",
       "      <td>0.409240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>./train/TRAIN_0003.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.943729</td>\n",
       "      <td>0.295295</td>\n",
       "      <td>0.312391</td>\n",
       "      <td>0.371455</td>\n",
       "      <td>0.329344</td>\n",
       "      <td>0.268686</td>\n",
       "      <td>...</td>\n",
       "      <td>0.808817</td>\n",
       "      <td>0.827032</td>\n",
       "      <td>0.815166</td>\n",
       "      <td>0.787913</td>\n",
       "      <td>0.779337</td>\n",
       "      <td>0.781981</td>\n",
       "      <td>0.771998</td>\n",
       "      <td>0.779565</td>\n",
       "      <td>0.796365</td>\n",
       "      <td>0.798277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>./train/TRAIN_0004.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.949608</td>\n",
       "      <td>0.188729</td>\n",
       "      <td>0.179787</td>\n",
       "      <td>0.154144</td>\n",
       "      <td>0.007228</td>\n",
       "      <td>0.055841</td>\n",
       "      <td>...</td>\n",
       "      <td>0.877542</td>\n",
       "      <td>0.909439</td>\n",
       "      <td>0.894097</td>\n",
       "      <td>0.863824</td>\n",
       "      <td>0.849838</td>\n",
       "      <td>0.867351</td>\n",
       "      <td>0.865360</td>\n",
       "      <td>0.861068</td>\n",
       "      <td>0.868706</td>\n",
       "      <td>0.875541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274</th>\n",
       "      <td>TRAIN_1274</td>\n",
       "      <td>./train/TRAIN_1274.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.964989</td>\n",
       "      <td>0.096119</td>\n",
       "      <td>0.129592</td>\n",
       "      <td>0.222531</td>\n",
       "      <td>0.039493</td>\n",
       "      <td>0.075616</td>\n",
       "      <td>...</td>\n",
       "      <td>0.857574</td>\n",
       "      <td>0.875432</td>\n",
       "      <td>0.862135</td>\n",
       "      <td>0.853636</td>\n",
       "      <td>0.852105</td>\n",
       "      <td>0.854758</td>\n",
       "      <td>0.852087</td>\n",
       "      <td>0.851688</td>\n",
       "      <td>0.860304</td>\n",
       "      <td>0.881918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275</th>\n",
       "      <td>TRAIN_1275</td>\n",
       "      <td>./train/TRAIN_1275.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.959506</td>\n",
       "      <td>0.283203</td>\n",
       "      <td>0.293961</td>\n",
       "      <td>0.390142</td>\n",
       "      <td>0.322176</td>\n",
       "      <td>0.239990</td>\n",
       "      <td>...</td>\n",
       "      <td>0.842018</td>\n",
       "      <td>0.853391</td>\n",
       "      <td>0.822730</td>\n",
       "      <td>0.825484</td>\n",
       "      <td>0.822317</td>\n",
       "      <td>0.809976</td>\n",
       "      <td>0.820805</td>\n",
       "      <td>0.835689</td>\n",
       "      <td>0.836974</td>\n",
       "      <td>0.819587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1276</th>\n",
       "      <td>TRAIN_1276</td>\n",
       "      <td>./train/TRAIN_1276.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.930908</td>\n",
       "      <td>0.223856</td>\n",
       "      <td>0.210020</td>\n",
       "      <td>0.151861</td>\n",
       "      <td>0.134018</td>\n",
       "      <td>0.149416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.918239</td>\n",
       "      <td>0.932157</td>\n",
       "      <td>0.898576</td>\n",
       "      <td>0.905386</td>\n",
       "      <td>0.894843</td>\n",
       "      <td>0.879353</td>\n",
       "      <td>0.900221</td>\n",
       "      <td>0.910439</td>\n",
       "      <td>0.903620</td>\n",
       "      <td>0.895840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1277</th>\n",
       "      <td>TRAIN_1277</td>\n",
       "      <td>./train/TRAIN_1277.wav</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.932890</td>\n",
       "      <td>0.247222</td>\n",
       "      <td>0.265837</td>\n",
       "      <td>0.223214</td>\n",
       "      <td>0.170546</td>\n",
       "      <td>0.141445</td>\n",
       "      <td>...</td>\n",
       "      <td>0.922436</td>\n",
       "      <td>0.946884</td>\n",
       "      <td>0.920099</td>\n",
       "      <td>0.916849</td>\n",
       "      <td>0.900980</td>\n",
       "      <td>0.903257</td>\n",
       "      <td>0.919087</td>\n",
       "      <td>0.922190</td>\n",
       "      <td>0.920950</td>\n",
       "      <td>0.932029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1278</th>\n",
       "      <td>TRAIN_1278</td>\n",
       "      <td>./train/TRAIN_1278.wav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.407188</td>\n",
       "      <td>0.620077</td>\n",
       "      <td>0.356668</td>\n",
       "      <td>0.769474</td>\n",
       "      <td>0.548411</td>\n",
       "      <td>0.806605</td>\n",
       "      <td>...</td>\n",
       "      <td>0.306191</td>\n",
       "      <td>0.317135</td>\n",
       "      <td>0.329551</td>\n",
       "      <td>0.308712</td>\n",
       "      <td>0.285871</td>\n",
       "      <td>0.289114</td>\n",
       "      <td>0.280076</td>\n",
       "      <td>0.270611</td>\n",
       "      <td>0.257248</td>\n",
       "      <td>0.177652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1279 rows × 260 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       SAMPLE_ID             SAMPLE_PATH  FAN_TYPE  LABEL       128       129  \\\n",
       "0     TRAIN_0000  ./train/TRAIN_0000.wav         2      0  0.952273  0.284621   \n",
       "1     TRAIN_0001  ./train/TRAIN_0001.wav         0      0  0.081981  0.935459   \n",
       "2     TRAIN_0002  ./train/TRAIN_0002.wav         0      0  0.240260  0.664368   \n",
       "3     TRAIN_0003  ./train/TRAIN_0003.wav         2      0  0.943729  0.295295   \n",
       "4     TRAIN_0004  ./train/TRAIN_0004.wav         2      0  0.949608  0.188729   \n",
       "...          ...                     ...       ...    ...       ...       ...   \n",
       "1274  TRAIN_1274  ./train/TRAIN_1274.wav         2      0  0.964989  0.096119   \n",
       "1275  TRAIN_1275  ./train/TRAIN_1275.wav         2      0  0.959506  0.283203   \n",
       "1276  TRAIN_1276  ./train/TRAIN_1276.wav         2      0  0.930908  0.223856   \n",
       "1277  TRAIN_1277  ./train/TRAIN_1277.wav         2      0  0.932890  0.247222   \n",
       "1278  TRAIN_1278  ./train/TRAIN_1278.wav         0      0  0.407188  0.620077   \n",
       "\n",
       "           130       131       132       133  ...       118       119  \\\n",
       "0     0.295843  0.363586  0.357530  0.294327  ...  0.833846  0.842291   \n",
       "1     0.514880  0.613406  0.691659  0.825306  ...  0.148846  0.144862   \n",
       "2     0.724483  0.354632  0.625930  0.695975  ...  0.454258  0.520886   \n",
       "3     0.312391  0.371455  0.329344  0.268686  ...  0.808817  0.827032   \n",
       "4     0.179787  0.154144  0.007228  0.055841  ...  0.877542  0.909439   \n",
       "...        ...       ...       ...       ...  ...       ...       ...   \n",
       "1274  0.129592  0.222531  0.039493  0.075616  ...  0.857574  0.875432   \n",
       "1275  0.293961  0.390142  0.322176  0.239990  ...  0.842018  0.853391   \n",
       "1276  0.210020  0.151861  0.134018  0.149416  ...  0.918239  0.932157   \n",
       "1277  0.265837  0.223214  0.170546  0.141445  ...  0.922436  0.946884   \n",
       "1278  0.356668  0.769474  0.548411  0.806605  ...  0.306191  0.317135   \n",
       "\n",
       "           120       121       122       123       124       125       126  \\\n",
       "0     0.818590  0.817896  0.799984  0.799645  0.811261  0.816584  0.825692   \n",
       "1     0.149996  0.165012  0.156853  0.160930  0.163661  0.162874  0.158788   \n",
       "2     0.454087  0.465858  0.423989  0.497721  0.504808  0.502295  0.487844   \n",
       "3     0.815166  0.787913  0.779337  0.781981  0.771998  0.779565  0.796365   \n",
       "4     0.894097  0.863824  0.849838  0.867351  0.865360  0.861068  0.868706   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1274  0.862135  0.853636  0.852105  0.854758  0.852087  0.851688  0.860304   \n",
       "1275  0.822730  0.825484  0.822317  0.809976  0.820805  0.835689  0.836974   \n",
       "1276  0.898576  0.905386  0.894843  0.879353  0.900221  0.910439  0.903620   \n",
       "1277  0.920099  0.916849  0.900980  0.903257  0.919087  0.922190  0.920950   \n",
       "1278  0.329551  0.308712  0.285871  0.289114  0.280076  0.270611  0.257248   \n",
       "\n",
       "           127  \n",
       "0     0.805742  \n",
       "1     0.084692  \n",
       "2     0.409240  \n",
       "3     0.798277  \n",
       "4     0.875541  \n",
       "...        ...  \n",
       "1274  0.881918  \n",
       "1275  0.819587  \n",
       "1276  0.895840  \n",
       "1277  0.932029  \n",
       "1278  0.177652  \n",
       "\n",
       "[1279 rows x 260 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a8fecd3-b50a-4c2a-9cb2-301a69345620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['FAN_TYPE',        128,        129,        130,        131,        132,\n",
       "              133,        134,        135,        136,\n",
       "       ...\n",
       "              118,        119,        120,        121,        122,        123,\n",
       "              124,        125,        126,        127],\n",
       "      dtype='object', length=257)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = tmp.columns.drop(['SAMPLE_ID', 'SAMPLE_PATH', 'LABEL'])\n",
    "cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e967190-c87e-458a-b9cb-4399574fa696",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0bdc4b0-54e4-45c1-8bca-57c252dc591c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, eval_mode):\n",
    "        self.eval_mode = eval_mode\n",
    "        if self.eval_mode:\n",
    "            self.labels = df['LABEL'].values\n",
    "        self.df = df[cols].values\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if self.eval_mode:\n",
    "            self.x = self.df[index]\n",
    "            self.y = self.labels[index]\n",
    "            return torch.Tensor(self.x).reshape(-1,1), self.y\n",
    "        else:\n",
    "            self.x = self.df[index]\n",
    "            return torch.Tensor(self.x).reshape(-1,1)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2afb059-203b-4ac7-aaec-79a0380abadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, tolerance=5, min_delta=0):\n",
    "        self.tolerance = tolerance\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.min_loss = np.inf\n",
    "\n",
    "    def __call__(self, train_loss, validation_loss=None):\n",
    "        if train_loss < self.min_loss:\n",
    "            self.counter = 0\n",
    "            self.min_loss = train_loss\n",
    "            print(f'counter : set 0 min loss : {self.min_loss}')\n",
    "        elif train_loss > self.min_loss:\n",
    "            self.counter += 1\n",
    "            print(f'counter : {self.counter}')\n",
    "        if self.counter >= self.tolerance:  \n",
    "            self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20032097-fc78-4ae7-bf1a-bcd831ecca78",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(tmp, test_size=0.1, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d381b860-662d-4cc1-a144-5410e9554877",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(df=train_df, eval_mode=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH, shuffle=True)\n",
    "\n",
    "val_dataset = CustomDataset(df = val_df, eval_mode=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b146980-5d90-4639-ac2c-df6ea0e4fd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, model, optimizer, train_loader, val_loader, scheduler, device):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.scheduler = scheduler\n",
    "        self.device = device\n",
    "        self.early_stopping = EarlyStopping(tolerance=100, min_delta=10)\n",
    "        \n",
    "        # Loss Function\n",
    "        self.criterion = nn.KLDivLoss(reduction='batchmean', log_target=True).to(self.device)\n",
    "        # self.criterion = nn.L1Loss().to(self.device)\n",
    "        \n",
    "    def fit(self):\n",
    "        self.model.to(self.device)\n",
    "        best_score = 0\n",
    "        for epoch in range(EPOCHS):\n",
    "            self.model.train()\n",
    "            train_loss = []\n",
    "            for x in iter(self.train_loader):\n",
    "                self.optimizer.zero_grad()\n",
    "                x = x.float().to(self.device)\n",
    "                _x = self.model(x)\n",
    "\n",
    "                log_target = F.log_softmax(_x, dim=1)\n",
    "                log_input = F.log_softmax(x.reshape(-1,257), dim=1)\n",
    "                \n",
    "                loss = self.criterion(log_input, log_target)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                train_loss.append(loss.item())\n",
    "            \n",
    "            score = self.validation(self.model)\n",
    "            print(f'Epoch : [{epoch}] Train loss : [{np.mean(train_loss)}] Val Score : [{score}])')\n",
    "            \n",
    "            if best_score < score:\n",
    "                best_score = score\n",
    "                torch.save(model.state_dict(), './best_model.pth', _use_new_zipfile_serialization=False)\n",
    "                \n",
    "            # early stopping\n",
    "            self.early_stopping(np.mean(train_loss))\n",
    "            if self.early_stopping.early_stop:\n",
    "                print(\"early_stopping:\", epoch)\n",
    "                break\n",
    "            \n",
    "            if self.scheduler is not None:\n",
    "                self.scheduler.step(score)\n",
    "                            \n",
    "    def validation(self, eval_model, thr=0.999):\n",
    "        cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        eval_model.eval()\n",
    "        pred = []\n",
    "        true = []\n",
    "        with torch.no_grad():\n",
    "            for x, y in iter(self.val_loader):\n",
    "                x = x.float().to(self.device)\n",
    "                _x = self.model(x)\n",
    "                \n",
    "                log_target = F.log_softmax(_x, dim=1)\n",
    "                log_input = F.log_softmax(x.reshape(-1,257), dim=1)\n",
    "                \n",
    "                diff = cos(log_input, log_target).cpu().tolist()\n",
    "                # print(diff)\n",
    "                batch_pred = np.where(np.array(diff) > thr, 0, 1).tolist()\n",
    "\n",
    "                pred += batch_pred\n",
    "                true += y.tolist()\n",
    "\n",
    "        return f1_score(true, pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "638207fa-027e-4b11-ab15-38ea0a9222df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, block):\n",
    "        super().__init__()\n",
    "        self.block = block\n",
    "    def forward(self, x):\n",
    "        return self.block(x) + x #f(x) + x\n",
    "    \n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        def Conv(in_channels, out_channels):\n",
    "            layers = []\n",
    "            layers += [nn.Conv1d(in_channels = in_channels, out_channels = out_channels, \n",
    "                                 kernel_size = 1, stride = 1)]\n",
    "            layers += [nn.BatchNorm1d(num_features = out_channels)]\n",
    "            layers += [nn.GELU()]\n",
    "            return nn.Sequential(*layers)\n",
    "        in_channels, out_channels = out_channels, \n",
    "                                 kernel_size = 1, stride = 1)filters[0]),\n",
    "            nn.in_channels, out_channels = out_channels, \n",
    "                                 kernel_size = 1, stride = 1self.input_skip = nn.Sequential(\n",
    "            nn.Conv\n",
    "        self.Encoder = nn.Sequential(\n",
    "            Conv(257,512),\n",
    "            nn.MaxPool1d(kernel_size=1, stride=1),\n",
    "            Conv(512,512),\n",
    "            nn.MaxPool1d(kernel_size=1, stride=1),\n",
    "            \n",
    "            Conv(512,1024),\n",
    "            nn.MaxPool1d(kernel_size=1, stride=1),\n",
    "            Conv(1024,1024),\n",
    "        )\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=1, stride=1)\n",
    "        self.unpool1 = nn.ConvTranspose1d(in_channels=1024, out_channels=1024, kernel_size=1, stride=1)\n",
    "\n",
    "        self.Decoder1 = nn.Sequential(\n",
    "            Conv(2048,1024),\n",
    "            nn.MaxPool1d(kernel_size=1, stride=1),\n",
    "            Conv(1024,512),\n",
    "        )\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=1, stride=1)\n",
    "        self.unpool2 = nn.ConvTranspose1d(in_channels=512, out_channels=512, kernel_size=1, stride=1)\n",
    "        \n",
    "        self.Decoder2 = nn.Sequential(\n",
    "            Conv(1024,512),\n",
    "            nn.MaxPool1d(kernel_size=1, stride=1),\n",
    "            Conv(512,512),\n",
    "        )\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=1, stride=1)\n",
    "        self.unpool3 = nn.ConvTranspose1d(in_channels=512, out_channels=512, kernel_size=1, stride=1)\n",
    "        \n",
    "        self.flat = nn.Flatten()\n",
    "        self.fc = nn.Conv1d(in_channels = 1024, out_channels = 257, kernel_size = 1, stride = 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self_x = self.Encoder(x)\n",
    "        cat1 = torch.cat((self.unpool1(_x),self.pool1(_x)), dim=1)\n",
    "        \n",
    "        _x = self.Decoder1(cat1)\n",
    "        cat2 = torch.cat((self.unpool2(_x),self.pool2(_x)), dim=1)\n",
    "        \n",
    "        _x = self.Decoder2(cat2)\n",
    "        cat3 = torch.cat((self.unpool3(_x),self.pool3(_x)), dim=1)\n",
    "        \n",
    "        x = self.flat(self.fc(cat3))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5c4ed34d-cf4f-46ac-a7dc-e0af29698759",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 512, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (512) must match the size of tensor b (256) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[123], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(optimizer, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, threshold_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabs\u001b[39m\u001b[38;5;124m'\u001b[39m, min_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-12\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model, optimizer, train_loader, val_loader, scheduler, device)\n\u001b[1;32m----> 7\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 29\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     26\u001b[0m log_target \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlog_softmax(_x, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     27\u001b[0m log_input \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlog_softmax(x\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m257\u001b[39m), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 29\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_target\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml\\lib\\site-packages\\torch\\nn\\modules\\loss.py:471\u001b[0m, in \u001b[0;36mKLDivLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 471\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkl_div\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_target\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_target\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml\\lib\\site-packages\\torch\\nn\\functional.py:2928\u001b[0m, in \u001b[0;36mkl_div\u001b[1;34m(input, target, size_average, reduce, reduction, log_target)\u001b[0m\n\u001b[0;32m   2925\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2926\u001b[0m         reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m-> 2928\u001b[0m reduced \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkl_div\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_enum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_target\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_target\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2930\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduction \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatchmean\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2931\u001b[0m     reduced \u001b[38;5;241m=\u001b[39m reduced \u001b[38;5;241m/\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (512) must match the size of tensor b (256) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "model = CNN()\n",
    "model.eval()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=10, threshold_mode='abs', min_lr=1e-12, verbose=True)\n",
    "\n",
    "trainer = Trainer(model, optimizer, train_loader, val_loader, scheduler, device)\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "96ca587b-6af7-444c-8396-0387d8b8f450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (Encoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv1d(257, 512, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): GELU(approximate='none')\n",
       "    )\n",
       "    (1): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (2): Sequential(\n",
       "      (0): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): GELU(approximate='none')\n",
       "    )\n",
       "    (3): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): GELU(approximate='none')\n",
       "    )\n",
       "    (5): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Sequential(\n",
       "      (0): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): GELU(approximate='none')\n",
       "    )\n",
       "    (7): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Sequential(\n",
       "      (0): Conv1d(1024, 2048, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): GELU(approximate='none')\n",
       "    )\n",
       "    (9): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Sequential(\n",
       "      (0): Conv1d(2048, 2048, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): GELU(approximate='none')\n",
       "    )\n",
       "  )\n",
       "  (pool1): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  (unpool1): ConvTranspose1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "  (Decoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv1d(2048, 1024, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): GELU(approximate='none')\n",
       "    )\n",
       "  )\n",
       "  (Decoder1): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv1d(2048, 1024, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): GELU(approximate='none')\n",
       "    )\n",
       "    (1): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (2): Sequential(\n",
       "      (0): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): GELU(approximate='none')\n",
       "    )\n",
       "  )\n",
       "  (pool2): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  (unpool2): ConvTranspose1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "  (Decoder2): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): GELU(approximate='none')\n",
       "    )\n",
       "    (1): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (2): Sequential(\n",
       "      (0): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): GELU(approximate='none')\n",
       "    )\n",
       "  )\n",
       "  (pool3): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  (unpool3): ConvTranspose1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc): Conv1d(1024, 257, kernel_size=(1,), stride=(1,))\n",
       ")"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNN()\n",
    "model.load_state_dict(torch.load('./best_model.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a634dee-9e6d-4332-898e-c564866d09c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "762db097-20e2-47db-a5b0-21e2b97b1553",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(test, False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e6872caa-8a20-45a4-8ded-76124adae2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(model, test_loader, device, thr=0.999):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    pred = []\n",
    "    with torch.no_grad():\n",
    "        for x in iter(test_loader):\n",
    "            x = x.float().to(device)\n",
    "            _x = model(x)\n",
    "            log_target = F.log_softmax(_x, dim=1)\n",
    "            log_input = F.log_softmax(x.reshape(-1,257), dim=1)\n",
    "                \n",
    "            diff = cos(log_input, log_target).cpu().tolist()\n",
    "            print(diff)\n",
    "            batch_pred = np.where(np.array(diff) > thr, 0, 1).tolist()\n",
    "            pred += batch_pred\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cf0f701d-7862-42bc-a8df-a235785dd876",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9995537400245667, 0.9999197125434875, 0.9994228482246399, 0.9963836669921875, 0.9992743730545044, 0.9999178647994995, 0.99991774559021, 0.9996844530105591, 0.9982609748840332, 0.9993952512741089, 0.9990406036376953, 0.99971604347229, 0.9992411136627197, 0.9988492727279663, 0.9998130798339844, 0.9993667602539062, 0.9999433159828186, 0.9994677305221558, 0.9997826218605042, 0.9959637522697449, 0.9954495429992676, 0.9992697238922119, 0.9992820024490356, 0.9989156723022461, 0.9996443390846252, 0.9996000528335571, 0.9989124536514282, 0.9996083974838257, 0.9989909529685974, 0.9998980760574341, 0.9998732209205627, 0.9997262954711914, 0.9998555183410645, 0.9995408654212952, 0.9989401698112488, 0.9999208450317383, 0.9998863935470581, 0.9989365339279175, 0.9991800785064697, 0.9998806715011597, 0.9990048408508301, 0.9998503923416138, 0.9998046159744263, 0.999340295791626, 0.9995889663696289, 0.9996730089187622, 0.9996721148490906, 0.9998587369918823, 0.9999300241470337, 0.9983798265457153, 0.9998633861541748, 0.9998505115509033, 0.9995214939117432, 0.9995730519294739, 0.9987758994102478, 0.9998414516448975, 0.9991499781608582, 0.9998387098312378, 0.9995623826980591, 0.9998915791511536, 0.9997876286506653, 0.9993900656700134, 0.9995673298835754, 0.9999191761016846, 0.9996581077575684, 0.999846875667572, 0.998809278011322, 0.9997756481170654, 0.9992471933364868, 0.9988434314727783, 0.9999386072158813, 0.9999415278434753, 0.9998752474784851, 0.9999150037765503, 0.9988083839416504, 0.9999285936355591, 0.9977645874023438, 0.9999330043792725, 0.9984820485115051, 0.9998542666435242, 0.9988832473754883, 0.998956561088562, 0.9994627237319946, 0.9993739128112793, 0.999151885509491, 0.9989583492279053, 0.9998201131820679, 0.9995959997177124, 0.9994012117385864, 0.999679684638977, 0.9999135136604309, 0.9987822771072388, 0.999714732170105, 0.9993568658828735, 0.9990726113319397, 0.9960650205612183, 0.9966511726379395, 0.9998670816421509, 0.9997825622558594, 0.9997912645339966, 0.9992997050285339, 0.9997478723526001, 0.9999465942382812, 0.9966777563095093, 0.9985498785972595, 0.9992055892944336, 0.9998721480369568, 0.999481737613678, 0.9991613626480103, 0.9998811483383179, 0.9996849298477173, 0.998995304107666, 0.9995956420898438, 0.9990678429603577, 0.9986851811408997, 0.999140739440918, 0.9998868703842163, 0.9990480542182922, 0.9988234043121338, 0.9988794326782227, 0.9998738765716553, 0.999193012714386, 0.9994363784790039, 0.9973991513252258, 0.9994418621063232, 0.9996229410171509, 0.9998559355735779, 0.99947190284729, 0.9993273019790649, 0.9990721940994263, 0.999782383441925, 0.9993412494659424, 0.9997028112411499, 0.9990426301956177, 0.9991183280944824, 0.9999231100082397, 0.9991492629051208, 0.9997947812080383, 0.9990524053573608, 0.9998289346694946, 0.9998416900634766, 0.9989463090896606, 0.9978296756744385, 0.9998844265937805, 0.9984882473945618, 0.9982479810714722, 0.9998346567153931, 0.9990403652191162, 0.9970372915267944, 0.9997130632400513, 0.999487578868866, 0.9998881220817566, 0.9997934103012085, 0.9994751214981079, 0.9997464418411255, 0.9989786148071289, 0.9998928308486938, 0.9987407922744751, 0.9986140727996826, 0.9997414350509644, 0.9989441633224487, 0.9995529651641846, 0.9987838268280029, 0.999825119972229, 0.9999086260795593, 0.9994181394577026, 0.9938164949417114, 0.9989007711410522, 0.9960442185401917, 0.9994529485702515, 0.9993319511413574, 0.9989047646522522, 0.9991739392280579, 0.9978269934654236, 0.999324381351471, 0.9989932179450989, 0.9999215006828308, 0.9985482096672058, 0.9998495578765869, 0.9986643195152283, 0.9990159273147583, 0.9997278451919556, 0.9978012442588806, 0.9997939467430115, 0.9998502731323242, 0.9996789693832397, 0.999913215637207, 0.9973952174186707, 0.9986435174942017, 0.9997559785842896, 0.997944712638855, 0.999215304851532, 0.9989954829216003, 0.9984861612319946, 0.9995450973510742, 0.9995966553688049, 0.9989577531814575, 0.9990181922912598, 0.9983785152435303, 0.9998767375946045, 0.9993938207626343, 0.9998741149902344, 0.9993703365325928, 0.9997060894966125, 0.9998544454574585, 0.9999070167541504, 0.9997843503952026, 0.999523937702179, 0.9999015927314758, 0.9999213218688965, 0.998781144618988, 0.9987888336181641, 0.9997925162315369, 0.9987636804580688, 0.9999169111251831, 0.9998546838760376, 0.9971410632133484, 0.999607503414154, 0.999021053314209, 0.9985923767089844, 0.9989913702011108, 0.9991050958633423, 0.9996300935745239, 0.9986387491226196, 0.9998520612716675, 0.9965139627456665, 0.9983501434326172, 0.9989051222801208, 0.9999070167541504, 0.9996629953384399, 0.9999330043792725, 0.9980679750442505, 0.9991202354431152, 0.999903678894043, 0.9995734691619873, 0.999880313873291, 0.9996019601821899, 0.9999000430107117, 0.9993627071380615, 0.9998302459716797, 0.9994737505912781, 0.998098611831665, 0.9998847246170044, 0.9996387362480164, 0.9990218877792358, 0.9998779892921448, 0.998912513256073, 0.9999389052391052, 0.9989296793937683, 0.9997819662094116, 0.9998215436935425, 0.9978781342506409, 0.9991292357444763, 0.9998108148574829, 0.9988391399383545, 0.9988425970077515]\n",
      "[0.9986171126365662, 0.9985471963882446, 0.9993237853050232, 0.9981303215026855, 0.9998500347137451, 0.9999269843101501, 0.9993444085121155, 0.9999201893806458, 0.9997997283935547, 0.9990640878677368, 0.9975517392158508, 0.9999158978462219, 0.999629020690918, 0.9997707605361938, 0.999678909778595, 0.9996999502182007, 0.9999270439147949, 0.9993987083435059, 0.9970859289169312, 0.9993959665298462, 0.9962302446365356, 0.9987748265266418, 0.9994447827339172, 0.9997718334197998, 0.9998388886451721, 0.9997282028198242, 0.9996911287307739, 0.9994339942932129, 0.9988499283790588, 0.9991416931152344, 0.9998477101325989, 0.9997128248214722, 0.999846875667572, 0.9998858571052551, 0.9997706413269043, 0.9996888637542725, 0.9997196197509766, 0.9994335174560547, 0.9964108467102051, 0.9986041188240051, 0.9983119964599609, 0.9999333024024963, 0.9997589588165283, 0.9996281862258911, 0.9998171925544739, 0.9988988041877747, 0.9996854066848755, 0.9994306564331055, 0.998572051525116, 0.999411404132843, 0.9998710751533508, 0.9996601939201355, 0.9995558261871338, 0.9984623789787292, 0.9996954798698425, 0.999028742313385, 0.9989911913871765, 0.9987555146217346, 0.9990886449813843, 0.9998047351837158, 0.9993319511413574, 0.9993728399276733, 0.9998471736907959, 0.9994317293167114, 0.9989537000656128, 0.9984753131866455, 0.9999191761016846, 0.9984372854232788, 0.9961019158363342, 0.9998359680175781, 0.9987276196479797, 0.9982167482376099, 0.9999187588691711, 0.9982333183288574, 0.9999203085899353, 0.9998562932014465, 0.9998593926429749, 0.9997544884681702, 0.9999085664749146, 0.9991699457168579, 0.9994601011276245, 0.9995418787002563, 0.9989891052246094, 0.9999161958694458, 0.9999040961265564, 0.9992775321006775, 0.9991707801818848, 0.9984060525894165, 0.9995677471160889, 0.9988123178482056, 0.9996445178985596, 0.9981423616409302, 0.9997598528862, 0.9987277984619141, 0.9989656209945679, 0.9987218379974365, 0.9971302151679993, 0.9996170997619629, 0.9978439807891846, 0.9983291625976562, 0.9999003410339355, 0.9998899102210999, 0.999853253364563, 0.9998712539672852, 0.9996029734611511, 0.9982567429542542, 0.9999315142631531, 0.9999163150787354, 0.9991136789321899, 0.9997198581695557, 0.9994505047798157, 0.9997479915618896, 0.9993157386779785, 0.9999414682388306, 0.9991447925567627, 0.9999059438705444, 0.9994841814041138, 0.999651312828064, 0.998855710029602, 0.9999368190765381, 0.9998957514762878, 0.9996188879013062, 0.9996441602706909, 0.9990434646606445, 0.998934268951416, 0.9987677931785583, 0.9991779923439026, 0.9997377991676331, 0.9992572069168091, 0.9998207092285156, 0.9999341368675232, 0.9953346848487854, 0.9991562962532043, 0.9978492259979248, 0.9996978640556335, 0.9993710517883301, 0.9993773698806763, 0.9993892908096313, 0.9984567165374756, 0.9995773434638977, 0.999699592590332, 0.9999197125434875, 0.9997102618217468, 0.9996243119239807, 0.9994280338287354, 0.9991039037704468, 0.9994279742240906, 0.9973388910293579, 0.9959473609924316, 0.9995564222335815, 0.998735785484314, 0.9999132752418518, 0.9995462894439697, 0.9994162917137146, 0.9998805522918701, 0.9981796741485596, 0.9982554912567139, 0.9955024123191833, 0.9987278580665588, 0.9995813369750977, 0.9998726844787598, 0.9984779357910156, 0.9972221851348877, 0.9989694356918335, 0.9984093904495239, 0.9996693134307861, 0.9999362826347351, 0.998912513256073, 0.9997308254241943, 0.9992746710777283, 0.9991088509559631, 0.999789297580719, 0.9995740056037903, 0.9997802972793579, 0.9992247223854065, 0.9998493194580078, 0.9999351501464844, 0.9994088411331177, 0.9991616010665894, 0.9998758435249329, 0.9996925592422485, 0.9998536109924316, 0.9990237951278687, 0.9998210072517395, 0.9986131191253662, 0.9993868470191956, 0.9993522763252258, 0.9999126195907593, 0.9985893368721008, 0.9995865225791931, 0.9996113777160645, 0.9996907711029053, 0.9996969103813171, 0.9997901916503906, 0.9958382844924927, 0.9987940788269043, 0.9989379644393921, 0.9997622966766357, 0.9996780157089233, 0.9997233152389526, 0.9999282360076904, 0.9968670606613159, 0.9998406171798706, 0.9990592002868652, 0.9996218681335449, 0.9996694326400757, 0.9991097450256348, 0.9998450875282288, 0.99947190284729, 0.9992425441741943, 0.9997485876083374, 0.9990156292915344, 0.9984030723571777, 0.9997897744178772, 0.9996695518493652, 0.9992579817771912, 0.9995524883270264, 0.9998874664306641, 0.9994927644729614, 0.9989334344863892, 0.9986721277236938, 0.9996410012245178, 0.9997724294662476, 0.9964075684547424, 0.9988943934440613, 0.999671220779419, 0.9990925788879395, 0.9981062412261963, 0.9996623992919922, 0.9963234066963196, 0.9996436238288879, 0.999323844909668, 0.999475359916687, 0.9989498257637024, 0.9998775720596313, 0.9995571970939636, 0.9998937845230103, 0.9998433589935303, 0.9992984533309937, 0.9996689558029175, 0.9990509152412415, 0.9997162818908691, 0.9995123147964478, 0.9998310804367065, 0.9999023675918579, 0.999763011932373, 0.9989097118377686, 0.9978053569793701, 0.9982872605323792, 0.9999279975891113, 0.9984497427940369, 0.9987581372261047, 0.99990314245224, 0.9999469518661499, 0.9999251961708069, 0.9999282360076904]\n",
      "[0.9998849630355835, 0.9998383522033691, 0.9985316395759583, 0.9988610744476318, 0.9994845390319824, 0.9998593926429749, 0.9999364614486694, 0.999144434928894, 0.999926745891571, 0.9995582699775696, 0.9991360306739807, 0.9994214177131653, 0.9990555644035339, 0.9981322884559631, 0.997854471206665, 0.99757319688797, 0.9985076189041138, 0.999690055847168, 0.9997420310974121, 0.9962931871414185, 0.9998233914375305, 0.9999194145202637, 0.9983924627304077, 0.9996148347854614, 0.999833881855011, 0.9981913566589355, 0.9974542260169983, 0.9996407628059387, 0.9999188780784607, 0.9988116025924683, 0.9998400211334229, 0.9998739361763, 0.9988770484924316, 0.9989969730377197, 0.9959565997123718, 0.9998997449874878, 0.9992473721504211, 0.9997636079788208, 0.9979792833328247, 0.996876060962677, 0.9990824460983276, 0.9997169375419617, 0.9997000098228455, 0.999508798122406, 0.9991147518157959, 0.9998742938041687, 0.9998807907104492, 0.9999408721923828, 0.9974190592765808, 0.9998825788497925, 0.9993002414703369, 0.9991776943206787, 0.9996217489242554, 0.9998751878738403, 0.9992749094963074, 0.9998048543930054, 0.9970517158508301, 0.9994426965713501, 0.9999163150787354, 0.9990326166152954, 0.9997311234474182, 0.999871551990509, 0.999376654624939, 0.9997721314430237, 0.9991087913513184, 0.9999051690101624, 0.9983164072036743, 0.9963671565055847, 0.9989762306213379, 0.9999093413352966, 0.9993857741355896, 0.9954038858413696, 0.9990372657775879, 0.9997758865356445, 0.9994676113128662, 0.9980112314224243, 0.9992711544036865, 0.999319314956665, 0.999887228012085, 0.9996066689491272, 0.9993566274642944, 0.9990646839141846, 0.9983334541320801, 0.9990098476409912, 0.9989112615585327, 0.9989786148071289, 0.9998161792755127, 0.999677836894989, 0.998572826385498, 0.9999211430549622, 0.9998356699943542, 0.999674916267395, 0.9988117218017578, 0.9987258911132812, 0.9987255334854126, 0.9993736743927002, 0.9999129772186279, 0.9992520809173584, 0.9992179870605469, 0.9997377991676331, 0.9991486668586731, 0.9997290968894958, 0.9984145760536194, 0.9997808933258057, 0.9997819662094116, 0.999181866645813, 0.9998910427093506, 0.9975275993347168, 0.9999176263809204, 0.998226523399353, 0.9998383522033691, 0.9985469579696655, 0.9985450506210327, 0.9997352361679077, 0.9998408555984497, 0.9996792078018188, 0.9998111724853516, 0.9992725253105164, 0.9995081424713135, 0.9997578859329224, 0.9993366003036499, 0.9999184608459473, 0.9989891052246094, 0.9987368583679199, 0.9994269013404846, 0.9994752407073975, 0.9990977048873901, 0.9982032775878906, 0.9995266199111938, 0.9989662766456604, 0.9995791912078857, 0.9998267889022827, 0.9994019269943237, 0.9990320205688477, 0.9996170997619629, 0.9982903599739075, 0.9979516863822937, 0.9990478754043579, 0.998633861541748, 0.9998195171356201, 0.9988246560096741, 0.9994187355041504, 0.9991316795349121, 0.9992474317550659, 0.9979043006896973, 0.9993898868560791, 0.9999021887779236, 0.9986204504966736, 0.9999338388442993, 0.9990348219871521, 0.9995210766792297, 0.9999226331710815, 0.9996126294136047, 0.9996492266654968, 0.9995922446250916, 0.9989763498306274, 0.9975255727767944, 0.9999327063560486, 0.9943077564239502, 0.9985083937644958, 0.9998867511749268, 0.9966452717781067, 0.9989503622055054, 0.9995001554489136, 0.9992123246192932, 0.9984209537506104, 0.9982751607894897, 0.9998669624328613, 0.9991582632064819, 0.9996716380119324, 0.9995951652526855, 0.9986380934715271, 0.9996364116668701, 0.9994474053382874, 0.9992766976356506, 0.9991139769554138, 0.9996364116668701, 0.9986998438835144, 0.9998642802238464, 0.9991610646247864, 0.9996013045310974, 0.9997619390487671, 0.9993007183074951, 0.9999035000801086, 0.9992576837539673, 0.999466061592102, 0.9995509386062622, 0.9992225766181946, 0.9994152784347534, 0.9998924732208252, 0.998760461807251, 0.9997472167015076, 0.9994111657142639, 0.9991410374641418, 0.9995777010917664, 0.9987465143203735, 0.9988722801208496, 0.99758380651474, 0.999311089515686, 0.9993342161178589, 0.9972963333129883, 0.9987255334854126, 0.9994522929191589, 0.9992649555206299, 0.9998570084571838, 0.9999219179153442, 0.9996950626373291, 0.9986940622329712, 0.9992800951004028, 0.999737024307251, 0.9993168711662292, 0.9998396635055542, 0.998138964176178, 0.9999010562896729, 0.9985362887382507, 0.9996954798698425, 0.9994868040084839, 0.999869704246521, 0.9989138841629028, 0.9989646077156067, 0.9996078014373779, 0.9983718395233154, 0.9989190101623535, 0.9987869262695312, 0.9996199011802673, 0.9998108744621277, 0.997846245765686, 0.9992727637290955, 0.9998120665550232, 0.9997410774230957, 0.9986510872840881, 0.9998173713684082, 0.9997376799583435, 0.9993196725845337, 0.9999408721923828, 0.9998635053634644, 0.9969506859779358, 0.9989482164382935, 0.9994450807571411, 0.9992917776107788, 0.9995939135551453, 0.9997150897979736, 0.9990912079811096, 0.9999079704284668, 0.9985178709030151, 0.9981945753097534, 0.999933123588562, 0.9971350431442261, 0.9998294711112976, 0.9994664788246155, 0.9999121427536011, 0.9994938373565674, 0.9980576634407043, 0.9995098114013672, 0.9999377727508545, 0.9985775351524353]\n",
      "[0.9988126754760742, 0.9990891814231873, 0.9989397525787354, 0.9997430443763733, 0.9942673444747925, 0.9997808933258057, 0.9954798817634583, 0.9998289346694946, 0.9989850521087646, 0.9998918771743774, 0.9986046552658081, 0.9997372627258301, 0.9993091821670532, 0.999943196773529, 0.9990839958190918, 0.9998713731765747, 0.9993585348129272, 0.9978100061416626, 0.9986286163330078, 0.9995924234390259, 0.9980985522270203, 0.9998809099197388, 0.9992998838424683, 0.9990093111991882, 0.9990450143814087, 0.9986555576324463, 0.9995484948158264, 0.99947190284729, 0.9992944002151489, 0.9996150732040405, 0.9998226165771484, 0.9994730949401855, 0.9990856647491455, 0.999040961265564, 0.9996342062950134, 0.9999397993087769, 0.9986860156059265, 0.9999340772628784, 0.998932957649231, 0.9996263980865479, 0.9997702836990356, 0.9998906850814819, 0.9989929795265198, 0.9999396800994873, 0.9983404874801636, 0.9992192387580872, 0.9996243119239807, 0.999785304069519, 0.9990504384040833, 0.9999064207077026, 0.9998384118080139, 0.9999227523803711, 0.9995315074920654, 0.99942547082901, 0.9985113143920898, 0.9989741444587708, 0.9989781975746155, 0.9989296197891235, 0.999434232711792, 0.9997933506965637, 0.9991300702095032, 0.9989390969276428, 0.9997767210006714, 0.9998140335083008, 0.9996605515480042, 0.9996070265769958, 0.9998795986175537, 0.9997951984405518, 0.996435821056366, 0.9987057447433472, 0.9992177486419678, 0.999764084815979, 0.9993266463279724, 0.9998894333839417, 0.9996652007102966, 0.9993301033973694, 0.9990536570549011, 0.9997288584709167, 0.9954255819320679, 0.9993904829025269, 0.9950685501098633, 0.9971201419830322, 0.9998166561126709, 0.9995517730712891, 0.9996405839920044, 0.9990682601928711, 0.9994013905525208, 0.9995372891426086, 0.999588131904602, 0.9994173049926758, 0.9999107122421265, 0.9998786449432373, 0.9994238018989563, 0.9997122287750244, 0.9999072551727295, 0.997671902179718, 0.9992643594741821, 0.9997774362564087, 0.9997394680976868, 0.9994603991508484, 0.9994639158248901, 0.9967799186706543, 0.9970554113388062, 0.9999062418937683, 0.9995377063751221, 0.9997310042381287, 0.9988516569137573, 0.9995602369308472, 0.9977941513061523, 0.9990499019622803, 0.9994226098060608, 0.9994946718215942, 0.9995278120040894, 0.9997541904449463, 0.9995256662368774, 0.9996898174285889, 0.9991326332092285, 0.9998064041137695, 0.9996949434280396, 0.9997867941856384, 0.9999198317527771, 0.9986576437950134, 0.9998247623443604, 0.9995914697647095, 0.9998235702514648, 0.9992848634719849, 0.9998502731323242, 0.9994107484817505, 0.9997044801712036, 0.998847246170044, 0.999656081199646, 0.9995883703231812, 0.9984225034713745, 0.999829888343811, 0.9997314810752869, 0.9967271685600281, 0.9992075562477112, 0.9997561573982239, 0.9997997283935547, 0.9988874197006226, 0.9987649321556091, 0.9996793270111084, 0.9995001554489136, 0.9999257326126099, 0.9997647404670715, 0.9986632466316223, 0.9988792538642883, 0.9997400045394897, 0.9994611740112305, 0.9952106475830078, 0.9998489618301392, 0.9991168975830078, 0.995699942111969, 0.9999179840087891, 0.9998042583465576, 0.9985750913619995, 0.9987037777900696, 0.9988356232643127, 0.9980945587158203, 0.9996691942214966, 0.9966102242469788, 0.9996833205223083, 0.998386025428772, 0.9995428323745728, 0.9991687536239624, 0.9990043640136719, 0.9991874098777771, 0.9971739053726196, 0.9996532201766968, 0.9987491965293884, 0.9997662305831909, 0.9997050762176514, 0.9958868026733398, 0.9986905455589294, 0.9968295693397522, 0.9997798800468445, 0.9998989701271057, 0.9995548129081726, 0.9990941286087036, 0.9961830973625183, 0.9999465942382812, 0.9997352361679077, 0.9995764493942261, 0.9999315738677979, 0.9992545247077942, 0.9987587332725525, 0.9996176958084106, 0.9981822967529297, 0.9997024536132812, 0.9992563128471375, 0.9988884925842285, 0.9987062811851501, 0.9998940825462341, 0.9999186992645264, 0.995411217212677, 0.9970576763153076, 0.9989843368530273, 0.9992244243621826, 0.9990144968032837, 0.9997417330741882, 0.9995604753494263, 0.998857319355011, 0.9993396997451782, 0.9999186992645264, 0.9990257620811462, 0.9990160465240479, 0.9989048838615417, 0.9988789558410645, 0.9977128505706787, 0.9998536705970764, 0.9994937181472778, 0.99913090467453, 0.999934732913971, 0.9989438056945801, 0.9991001486778259, 0.9991291761398315, 0.9994558095932007, 0.9998537302017212, 0.9989355206489563, 0.9998925924301147, 0.9990254640579224, 0.9999091029167175, 0.9994076490402222, 0.9997127056121826, 0.9975486993789673, 0.9999123215675354, 0.9993561506271362, 0.9997252225875854, 0.9999271035194397, 0.9998451471328735, 0.9995943307876587, 0.9999274015426636, 0.9993784427642822, 0.9999117851257324, 0.9998945593833923, 0.9999504089355469, 0.9997055530548096, 0.9999265670776367, 0.9998916983604431, 0.9998403787612915, 0.9997124671936035, 0.9982672929763794, 0.9994048476219177, 0.9991637468338013, 0.9992926120758057, 0.9999094605445862, 0.9992761611938477, 0.999547004699707, 0.9991201758384705, 0.9990732669830322, 0.999802827835083, 0.9982332587242126, 0.9993930459022522, 0.9958216547966003, 0.9998373985290527, 0.9996539354324341]\n",
      "[0.9999016523361206, 0.99955153465271, 0.9925847053527832, 0.9996923208236694, 0.9994490146636963, 0.9990872144699097, 0.9969053268432617, 0.9988558292388916, 0.9985774755477905, 0.996894359588623, 0.999431848526001, 0.9990605711936951, 0.9994297027587891, 0.9966530799865723, 0.999281108379364, 0.9984699487686157, 0.999667227268219, 0.9993746280670166, 0.9992849826812744, 0.9993727803230286, 0.9986605644226074, 0.9997133016586304, 0.9992110729217529, 0.9997639656066895, 0.999873161315918, 0.9995787143707275, 0.9997369050979614, 0.998970627784729, 0.9997587203979492, 0.9990440607070923, 0.9991354942321777, 0.9999134540557861, 0.9970659613609314, 0.9995983839035034, 0.9979403018951416, 0.9993373155593872, 0.9998512268066406, 0.9981767535209656, 0.9965814352035522, 0.9992706179618835, 0.9999234080314636, 0.9999452233314514, 0.9994020462036133, 0.9999279975891113, 0.9998456835746765, 0.9989565014839172, 0.9998540878295898, 0.9999035000801086, 0.9998958110809326, 0.99981689453125, 0.9992029666900635, 0.9969536066055298, 0.9999284744262695, 0.9996570944786072, 0.9987659454345703, 0.9992609024047852, 0.999197244644165, 0.999929666519165, 0.9995869994163513, 0.9994920492172241, 0.9996107816696167, 0.9999182224273682, 0.9989262819290161, 0.9989500045776367, 0.9997988343238831, 0.9996135830879211, 0.9996234178543091, 0.9994445443153381, 0.9996438026428223, 0.999830961227417, 0.9993457198143005, 0.99824458360672, 0.9992245435714722, 0.9992320537567139, 0.9998809695243835, 0.9977867603302002, 0.9998775720596313, 0.9989429712295532, 0.9974365234375, 0.999122142791748, 0.9986933469772339, 0.9999133348464966, 0.9990888833999634, 0.9998723268508911, 0.9990732073783875, 0.9999231100082397, 0.9993715286254883, 0.9982568025588989, 0.9991962909698486, 0.9995856285095215, 0.9977500438690186, 0.9995980858802795, 0.9997507333755493, 0.9997773170471191, 0.9998473525047302, 0.999167799949646, 0.9999168515205383, 0.9988544583320618, 0.9984351992607117, 0.9998883605003357, 0.9985442161560059, 0.9995335340499878, 0.9996901750564575, 0.9985402822494507, 0.9997754096984863, 0.9999259114265442, 0.9994733929634094, 0.999034583568573, 0.9992000460624695, 0.9997554421424866, 0.9996125102043152, 0.9957458972930908, 0.9998118281364441, 0.9990643858909607, 0.9995301961898804, 0.9995702505111694, 0.9994763731956482, 0.9989185929298401, 0.9971916079521179, 0.9942816495895386, 0.9997683763504028, 0.9993879199028015, 0.9986724853515625, 0.9999291300773621, 0.9998105764389038, 0.9997494220733643, 0.9998999834060669, 0.998395562171936, 0.9989005327224731, 0.9992009997367859, 0.9999011754989624, 0.9997886419296265, 0.9999049305915833, 0.99989253282547, 0.9985954761505127, 0.9993088245391846, 0.9998857975006104, 0.9996997714042664, 0.9995664954185486, 0.9998339414596558, 0.9997364282608032, 0.9998880624771118, 0.9994642734527588, 0.9977248907089233, 0.9999120831489563, 0.9995512366294861, 0.9995607137680054, 0.9990745782852173, 0.999769926071167, 0.9989623427391052, 0.9970524311065674, 0.9999324083328247, 0.9993174076080322, 0.9998525381088257, 0.9997056126594543, 0.9999379515647888, 0.9990639686584473, 0.9961196780204773, 0.9983586072921753, 0.9982262849807739, 0.9993110299110413, 0.998680830001831, 0.9988184571266174, 0.9993565678596497, 0.9998822212219238, 0.9995982646942139, 0.9988924264907837, 0.9997939467430115, 0.9997376203536987, 0.9992291331291199, 0.9999135136604309, 0.9975019693374634, 0.9976505637168884, 0.9983856081962585, 0.9974451065063477, 0.9997391104698181, 0.9999094009399414, 0.9996127486228943, 0.9998745918273926, 0.9998825192451477, 0.998529314994812, 0.9978330731391907, 0.9998900890350342, 0.999540388584137, 0.9984965920448303, 0.9979466795921326, 0.9991347789764404, 0.9983553886413574, 0.9998882412910461, 0.9996063709259033, 0.9984449148178101, 0.9996124505996704, 0.9997548460960388, 0.999444842338562, 0.9982995986938477, 0.9997560977935791, 0.9998424649238586, 0.999305009841919, 0.99717116355896, 0.999947190284729, 0.9988652467727661, 0.9998490810394287, 0.9997563362121582, 0.9988563060760498, 0.9994918704032898, 0.999906063079834, 0.9994956254959106, 0.9999387264251709, 0.9992634057998657, 0.9992727637290955, 0.9987316131591797, 0.9996556639671326, 0.9990109801292419, 0.9999418258666992, 0.9979063868522644, 0.9996588230133057, 0.9997353553771973, 0.9996966123580933, 0.9993040561676025, 0.9997661113739014, 0.9981728196144104, 0.9985718131065369, 0.9990547299385071, 0.9993407726287842, 0.9996742606163025, 0.9997692704200745, 0.9999394416809082, 0.9975308775901794, 0.9997326135635376, 0.9999108910560608, 0.996643602848053, 0.9999227523803711, 0.9990402460098267, 0.9999114871025085, 0.998356282711029, 0.9997003674507141, 0.9989157915115356, 0.9990682601928711, 0.9992777109146118, 0.9998741149902344, 0.9992425441741943, 0.9998972415924072, 0.9991705417633057, 0.999830961227417, 0.9998865723609924, 0.9997435808181763, 0.9998947978019714, 0.9997381567955017, 0.9988535046577454, 0.9999577403068542, 0.9990140199661255, 0.9991053342819214, 0.9987281560897827, 0.999162495136261, 0.9990440607070923, 0.9991358518600464]\n",
      "[0.9994759559631348, 0.9957289695739746, 0.9995308518409729, 0.9988746047019958, 0.9989620447158813, 0.9996203184127808, 0.9972559213638306, 0.9984544515609741, 0.9996539354324341, 0.9990285634994507, 0.9995750188827515, 0.9989216327667236, 0.9982954263687134, 0.999031662940979, 0.9986019134521484, 0.9996088743209839, 0.9993990063667297, 0.9996643662452698, 0.9993076920509338, 0.9977487921714783, 0.9995908737182617, 0.9983361959457397, 0.9998035430908203, 0.9989761114120483, 0.9991593360900879, 0.9986592531204224, 0.9995542764663696, 0.9998961091041565, 0.9999390244483948, 0.997814953327179, 0.9999038577079773, 0.999015212059021, 0.9996694326400757, 0.999505877494812, 0.9995549917221069, 0.9967019557952881, 0.9969105124473572, 0.9978705644607544, 0.9998982548713684, 0.9992706775665283, 0.9996116161346436, 0.9997832775115967, 0.999885082244873, 0.9990929961204529, 0.9991884231567383, 0.9996936321258545, 0.9988499283790588, 0.9998590350151062, 0.999910295009613, 0.9992810487747192, 0.9977691173553467, 0.9999510645866394, 0.9998904466629028, 0.9983837008476257, 0.9988138675689697, 0.9999110698699951, 0.9995145201683044, 0.9992510080337524, 0.9998922944068909, 0.9979430437088013, 0.9998807311058044, 0.999267578125, 0.9980612993240356, 0.9996006488800049, 0.9980249404907227, 0.999143660068512, 0.9981785416603088, 0.9995125532150269, 0.998881459236145, 0.9985209107398987, 0.9997819662094116, 0.9998822212219238, 0.9998853802680969, 0.9994707107543945, 0.9995945692062378, 0.999874472618103, 0.9999232292175293, 0.9985606074333191, 0.9976667165756226, 0.9998396635055542, 0.9991910457611084, 0.9986934065818787, 0.9997656345367432, 0.9991006851196289, 0.999835729598999, 0.999477207660675, 0.9998864531517029, 0.9990541934967041, 0.9998157620429993, 0.9979947805404663, 0.9992583394050598, 0.9993869066238403, 0.9999233484268188, 0.9999411702156067, 0.9996296763420105, 0.9997878074645996, 0.9998782873153687, 0.9973926544189453, 0.998363733291626, 0.9998776912689209, 0.9991130828857422, 0.9982414245605469, 0.9998766183853149, 0.9993535876274109, 0.9993000030517578, 0.999900758266449, 0.9990553259849548, 0.9993098378181458, 0.9995219111442566, 0.9994989037513733, 0.999345064163208, 0.9997486472129822, 0.9998763203620911, 0.9998895525932312, 0.99990314245224, 0.999549388885498, 0.9989551901817322, 0.9995659589767456, 0.9997615814208984, 0.9997178316116333, 0.9973442554473877, 0.9999415278434753, 0.9994635581970215, 0.9997062087059021, 0.9986064434051514, 0.9999393820762634, 0.9976115226745605, 0.9998931288719177, 0.9994399547576904, 0.9993808269500732, 0.9998269081115723, 0.9996612071990967, 0.9984747767448425, 0.9978867173194885, 0.998805046081543, 0.9989263415336609, 0.9996958374977112, 0.9999308586120605, 0.9995006918907166, 0.99945467710495, 0.9999316930770874, 0.9994916915893555, 0.9980629682540894, 0.999761700630188, 0.9998780488967896, 0.9994534850120544, 0.998871922492981, 0.999634325504303, 0.9994534254074097, 0.9973253607749939, 0.9998108148574829, 0.9985454082489014, 0.9996968507766724, 0.9993429183959961, 0.999169111251831, 0.9984415769577026, 0.9996206760406494, 0.9994761943817139, 0.998428463935852, 0.9998920559883118, 0.999815046787262, 0.9976533055305481, 0.9986852407455444, 0.999895453453064, 0.9992635250091553, 0.9987116456031799, 0.999905526638031, 0.9978794455528259, 0.999201774597168, 0.9994295835494995, 0.9989743828773499, 0.9991472363471985, 0.9997463226318359, 0.9998835921287537, 0.9990403056144714, 0.9978992938995361, 0.9998878836631775, 0.9986742734909058, 0.9983887672424316, 0.9993023872375488, 0.9996521472930908, 0.9997326135635376, 0.9988154172897339, 0.9984280467033386, 0.9992305040359497, 0.9999067783355713, 0.9999187588691711, 0.9998952746391296, 0.9999023079872131, 0.9996305108070374, 0.9989606142044067, 0.9980694055557251, 0.9993898272514343, 0.9993352293968201, 0.9998195767402649, 0.9990806579589844, 0.9997687935829163, 0.9998806715011597, 0.9980698823928833, 0.9997228384017944, 0.9992021322250366, 0.9998658299446106, 0.9967973232269287, 0.999545693397522, 0.9995477795600891, 0.9998156428337097, 0.9996379613876343, 0.9999013543128967, 0.9996285438537598, 0.9997296929359436, 0.999763011932373, 0.9996061325073242, 0.9999473094940186, 0.9992144107818604, 0.9989745616912842, 0.9989917874336243, 0.9991148710250854, 0.9948220252990723, 0.9998815655708313, 0.9991108179092407, 0.9998992681503296, 0.9995121955871582, 0.9991098642349243, 0.9995748996734619, 0.999925434589386, 0.9998124837875366, 0.9994806051254272, 0.9997506141662598, 0.9998273849487305, 0.994560718536377, 0.9986716508865356, 0.9999106526374817, 0.999883770942688, 0.9984821081161499]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "427"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = prediction(model, test_loader, device)\n",
    "sum(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fecf184-57e8-4969-bce1-46da090bdd32",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "05157612-68bf-4102-a723-8c0ad0f76755",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv(path+'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "38b5a428-5c17-4056-a3fe-4f50e30412ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAMPLE_ID</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_0001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_0002</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_0003</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_0004</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1509</th>\n",
       "      <td>TEST_1509</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1510</th>\n",
       "      <td>TEST_1510</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1511</th>\n",
       "      <td>TEST_1511</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1512</th>\n",
       "      <td>TEST_1512</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1513</th>\n",
       "      <td>TEST_1513</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1514 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      SAMPLE_ID  LABEL\n",
       "0     TEST_0000      0\n",
       "1     TEST_0001      0\n",
       "2     TEST_0002      0\n",
       "3     TEST_0003      1\n",
       "4     TEST_0004      0\n",
       "...         ...    ...\n",
       "1509  TEST_1509      1\n",
       "1510  TEST_1510      1\n",
       "1511  TEST_1511      0\n",
       "1512  TEST_1512      0\n",
       "1513  TEST_1513      1\n",
       "\n",
       "[1514 rows x 2 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit['LABEL'] = preds\n",
    "submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6c1c8567-eacd-4de3-b5e2-490102224edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('submit.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2fe3fa16-acc3-4628-a401-bc752544e41b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "427"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit['LABEL'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731ccaef-e266-4079-8101-126bb441a031",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
