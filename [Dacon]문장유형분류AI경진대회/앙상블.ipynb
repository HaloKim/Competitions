{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a84908-231a-4f60-8f97-feb197a8e4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import numpy as np\n",
    "from typing import Optional, Sequence\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, f1_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from torch.nn import functional as F\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification, AutoTokenizer, EarlyStoppingCallback, AutoModel, AutoConfig\n",
    "\n",
    "import gc\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d093495-74ef-492a-9a58-465814b4f43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv').drop(['ID'], axis=1)\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c6844e-4f82-4739-9789-0aad9434e2fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7301db-513f-4620-b4bf-003dcf2ec91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            st_type = self.labels['type'][idx]\n",
    "            st_polarity = self.labels['polarity'][idx]\n",
    "            st_tense = self.labels['tense'][idx]\n",
    "            st_certainty = self.labels['certainty'][idx]\n",
    "            item[\"labels\"] = torch.tensor(st_type), torch.tensor(st_polarity), torch.tensor(st_tense), torch.tensor(st_certainty)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f124b81-8db7-4397-904f-042e35c9a9b5",
   "metadata": {},
   "source": [
    "# HuggingFace Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91380cf8-3197-42f8-b11b-60a4f08bea76",
   "metadata": {},
   "source": [
    "## trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1d46c2-1a0e-4805-b7da-bfb1c9ea2940",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define trainer\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # forward pass\n",
    "        labels = inputs.pop(\"labels\").to(torch.int64)\n",
    "        \n",
    "        type_logit, polarity_logit, tense_logit, certainty_logit = model(**inputs)\n",
    "        \n",
    "        # # simple loss\n",
    "        # criterion = {\n",
    "        #     'type' : nn.CrossEntropyLoss().to(device),\n",
    "        #     'polarity' : nn.CrossEntropyLoss().to(device),\n",
    "        #     'tense' : nn.CrossEntropyLoss().to(device),\n",
    "        #     'certainty' : nn.CrossEntropyLoss().to(device)\n",
    "        # }\n",
    "        # loss = criterion['type'](type_logit, labels[::, 0]) + \\\n",
    "        #             criterion['polarity'](polarity_logit, labels[::, 1]) + \\\n",
    "        #             criterion['tense'](tense_logit,labels[::, 2]) + \\\n",
    "        #             criterion['certainty'](certainty_logit, labels[::, 3])\n",
    "        \n",
    "        # focal loss\n",
    "        criterion = {\n",
    "            'type' : FocalLoss().to(device),\n",
    "            'polarity' : FocalLoss().to(device),\n",
    "            'tense' : FocalLoss().to(device),\n",
    "            'certainty' : FocalLoss().to(device)\n",
    "        }\n",
    "        # labels = labels.type(torch.float).clone().detach()\n",
    "        loss = criterion['type'](type_logit, labels[::, 0]) + \\\n",
    "                    criterion['polarity'](polarity_logit, labels[::, 1]) + \\\n",
    "                    criterion['tense'](tense_logit, labels[::, 2]) + \\\n",
    "                    criterion['certainty'](certainty_logit, labels[::, 3])\n",
    "\n",
    "        outputs = None, \\\n",
    "                    torch.argmax(type_logit, dim = 1), \\\n",
    "                    torch.argmax(polarity_logit, dim = 1),\\\n",
    "                    torch.argmax(tense_logit, dim = 1),\\\n",
    "                    torch.argmax(certainty_logit, dim = 1)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27185269-59bb-4d68-ae89-a8d1bddd5846",
   "metadata": {},
   "outputs": [],
   "source": [
    "유형 = LabelEncoder()\n",
    "유형.fit(train['유형'])\n",
    "\n",
    "극성 = LabelEncoder()\n",
    "극성.fit(train['극성'])\n",
    "\n",
    "시제 = LabelEncoder()\n",
    "시제.fit(train['시제'])\n",
    "\n",
    "확실성 = LabelEncoder()\n",
    "확실성.fit(train['확실성'])\n",
    "\n",
    "def encoding(X_train, X_val):\n",
    "    X_train['유형'] = 유형.transform(X_train['유형'])\n",
    "    X_val['유형'] = 유형.transform(X_val['유형'])\n",
    "\n",
    "    X_train['극성'] = 극성.transform(X_train['극성'])\n",
    "    X_val['극성'] = 극성.transform(X_val['극성'])\n",
    "\n",
    "    X_train['시제'] = 시제.transform(X_train['시제'])\n",
    "    X_val['시제'] = 시제.transform(X_val['시제'])\n",
    "\n",
    "    X_train['확실성'] = 확실성.transform(X_train['확실성'])\n",
    "    X_val['확실성'] = 확실성.transform(X_val['확실성'])\n",
    "\n",
    "    train_labels = {\n",
    "        'type' : X_train['유형'].values,\n",
    "        'polarity' : X_train['극성'].values,\n",
    "        'tense' : X_train['시제'].values,\n",
    "        'certainty' : X_train['확실성'].values\n",
    "    }\n",
    "\n",
    "    val_labels = {\n",
    "        'type' : X_val['유형'].values,\n",
    "        'polarity' : X_val['극성'].values,\n",
    "        'tense' : X_val['시제'].values,\n",
    "        'certainty' : X_val['확실성'].values\n",
    "    }\n",
    "    return train_labels, val_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec38659-4ebc-4649-a747-b24b5270269c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f059d9-e265-4f7c-8bba-a484968574ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recent_file(path):\n",
    "    file_name_and_time_lst = []\n",
    "    # 해당 경로에 있는 파일들의 생성시간을 함께 리스트로 넣어줌. \n",
    "    for f_name in os.listdir(f\"{path}\"):\n",
    "        written_time = os.path.getctime(f\"{path}/{f_name}\")\n",
    "        file_name_and_time_lst.append((f_name, written_time))\n",
    "    # 생성시간 역순으로 정렬하고, \n",
    "    sorted_file_lst = sorted(file_name_and_time_lst, key=lambda x: x[1], reverse=True)\n",
    "    # 가장 앞에 이는 놈을 넣어준다.\n",
    "    recent_file = sorted_file_lst[0]\n",
    "    recent_file_name = recent_file[0]\n",
    "    return f\"{path}/{recent_file_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69df98a0-79ff-4393-9c80-bf29421fdd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2098bb42-0189-4101-bbed-52f17b8dd23a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 743\n",
    "device = torch.device(\"cuda\")\n",
    "model_path = \"beomi/KcELECTRA-base-v2022\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "length = train['문장'].str.len().max()\n",
    "config=AutoConfig.from_pretrained(model_path)\n",
    "config._name_or_path = 'kr.kim'\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomModel, self).__init__()\n",
    "        if model_path == 'monologg/kobigbird-bert-base':\n",
    "            config.attention_type = \"original_full\"\n",
    "        self.base_model = AutoModel.from_pretrained(model_path, config=config)\n",
    "        try:\n",
    "            self.out = self.base_model.encoder.layer[-1].output.dense.out_features\n",
    "        except:\n",
    "            self.out = 768\n",
    "        # self.linear = nn.Linear(768, 768//2)\n",
    "\n",
    "        self.type_classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=self.out, out_features=4),\n",
    "        )\n",
    "        self.polarity_classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=self.out, out_features=3),\n",
    "        )\n",
    "        self.tense_classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=self.out, out_features=3),\n",
    "        )\n",
    "        self.certainty_classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=self.out, out_features=2),\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels=None, token_type_ids=None):\n",
    "        x = self.base_model(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
    "        # x = self.linear(x)\n",
    "        # 문장 유형, 극성, 시제, 확실성을 각각 분류\n",
    "        type_output = self.type_classifier(x[:,0,:].view(-1,self.out))\n",
    "        polarity_output = self.polarity_classifier(x[:,0,:].view(-1,self.out))\n",
    "        tense_output = self.tense_classifier(x[:,0,:].view(-1,self.out))\n",
    "        certainty_output = self.certainty_classifier(x[:,0,:].view(-1,self.out))\n",
    "        return type_output, polarity_output, tense_output, certainty_output\n",
    "\n",
    "gc.collect() # python 자원 관리 \n",
    "torch.cuda.empty_cache() # gpu 자원관리\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenized = tokenizer(test.문장.tolist(), padding=True, truncation=True, max_length=length, return_tensors=\"pt\")\n",
    "test_dataset = CustomDataset(tokenized, None)\n",
    "test_args = TrainingArguments(\n",
    "    output_dir = './',\n",
    "    do_train = False,\n",
    "    do_predict = True,\n",
    "    per_device_eval_batch_size = 256,   \n",
    "    dataloader_drop_last = False    \n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(5):\n",
    "        print(f'Round {i}')\n",
    "        # model = AutoModel.from_pretrained(recent_file('custom_model'), config=config)\n",
    "        model = CustomModel().to(device)\n",
    "        model.load_state_dict(torch.load(f\"{recent_file(f'743/fold_{i}')}/pytorch_model.bin\"))\n",
    "        trainer = CustomTrainer(\n",
    "                      model = model, \n",
    "                      args = test_args,)\n",
    "        test_results.append(trainer.predict(test_dataset))\n",
    "        del model\n",
    "        del trainer\n",
    "        gc.collect() # python 자원 관리 \n",
    "        torch.cuda.empty_cache() # gpu 자원관리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9712c6e-4b46-47a0-8940-146a69d42d20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 7474\n",
    "device = torch.device(\"cuda\")\n",
    "model_path = \"monologg/kobigbird-bert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "length = train['문장'].str.len().max()\n",
    "config=AutoConfig.from_pretrained(model_path)\n",
    "config._name_or_path = 'kr.kim'\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomModel, self).__init__()\n",
    "        if model_path == 'monologg/kobigbird-bert-base':\n",
    "            config.attention_type = \"original_full\"\n",
    "        self.base_model = AutoModel.from_pretrained(model_path, config=config)\n",
    "        try:\n",
    "            self.out = self.base_model.encoder.layer[-1].output.dense.out_features\n",
    "        except:\n",
    "            self.out = 768\n",
    "        # self.linear = nn.Linear(768, 768//2)\n",
    "\n",
    "        self.type_classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=self.out, out_features=4),\n",
    "        )\n",
    "        self.polarity_classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=self.out, out_features=3),\n",
    "        )\n",
    "        self.tense_classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=self.out, out_features=3),\n",
    "        )\n",
    "        self.certainty_classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=self.out, out_features=2),\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels=None, token_type_ids=None):\n",
    "        x = self.base_model(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
    "        # x = self.linear(x)\n",
    "        # 문장 유형, 극성, 시제, 확실성을 각각 분류\n",
    "        type_output = self.type_classifier(x[:,0,:].view(-1,self.out))\n",
    "        polarity_output = self.polarity_classifier(x[:,0,:].view(-1,self.out))\n",
    "        tense_output = self.tense_classifier(x[:,0,:].view(-1,self.out))\n",
    "        certainty_output = self.certainty_classifier(x[:,0,:].view(-1,self.out))\n",
    "        return type_output, polarity_output, tense_output, certainty_output\n",
    "\n",
    "gc.collect() # python 자원 관리 \n",
    "torch.cuda.empty_cache() # gpu 자원관리\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenized = tokenizer(test.문장.tolist(), padding=True, truncation=True, max_length=length, return_tensors=\"pt\")\n",
    "test_dataset = CustomDataset(tokenized, None)\n",
    "test_args = TrainingArguments(\n",
    "    output_dir = './',\n",
    "    do_train = False,\n",
    "    do_predict = True,\n",
    "    per_device_eval_batch_size = 256,   \n",
    "    dataloader_drop_last = False    \n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(5):\n",
    "        print(f'Round {i}')\n",
    "        # model = AutoModel.from_pretrained(recent_file('custom_model'), config=config)\n",
    "        model = CustomModel().to(device)\n",
    "        model.load_state_dict(torch.load(f\"{recent_file(f'7474/fold_{i}')}/pytorch_model.bin\"))\n",
    "        trainer = CustomTrainer(\n",
    "                      model = model, \n",
    "                      args = test_args)\n",
    "        test_results.append(trainer.predict(test_dataset))\n",
    "        del model\n",
    "        del trainer\n",
    "        gc.collect() # python 자원 관리 \n",
    "        torch.cuda.empty_cache() # gpu 자원관리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418eefef-4412-43ec-9cd5-02a06cc7755a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 741\n",
    "device = torch.device(\"cuda\")\n",
    "model_path = \"monologg/kobigbird-bert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "length = train['문장'].str.len().max()\n",
    "config=AutoConfig.from_pretrained(model_path)\n",
    "config._name_or_path = 'kr.kim'\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomModel, self).__init__()\n",
    "        if model_path == 'monologg/kobigbird-bert-base':\n",
    "            config.attention_type = \"original_full\"\n",
    "        self.base_model = AutoModel.from_pretrained(model_path, config=config)\n",
    "        self.out = self.base_model.encoder.layer[-1].output.dense.out_features//2\n",
    "        self.norm = 384\n",
    "        \n",
    "        self.Linear1 = nn.Sequential(\n",
    "            nn.Linear(768, 384),\n",
    "            nn.BatchNorm1d(self.norm))\n",
    "        self.Linear2 = nn.Sequential(\n",
    "            nn.Linear(768, 384),\n",
    "            nn.BatchNorm1d(self.norm))\n",
    "        self.Linear3 = nn.Sequential(\n",
    "            nn.Linear(768, 384),\n",
    "            nn.BatchNorm1d(self.norm))\n",
    "        self.Linear4 = nn.Sequential(\n",
    "            nn.Linear(768, 384),\n",
    "            nn.BatchNorm1d(self.norm))\n",
    "        \n",
    "        self.type_classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=self.out, out_features=4),\n",
    "        )\n",
    "        self.polarity_classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=self.out, out_features=3),\n",
    "        )\n",
    "        self.tense_classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=self.out, out_features=3),\n",
    "        )\n",
    "        self.certainty_classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=self.out, out_features=2),\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels=None, token_type_ids=None):\n",
    "        x = self.base_model(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
    "        x = x[:,0,:]\n",
    "\n",
    "        out4 = self.Linear4(x)\n",
    "        certainty_output = self.certainty_classifier(out4)\n",
    "        \n",
    "        out3 = self.Linear3(x)\n",
    "        tense_output = self.tense_classifier((out3+out4))\n",
    "        \n",
    "        out2 = self.Linear2(x)\n",
    "        polarity_output = self.polarity_classifier((out2+out3+out4))\n",
    "        \n",
    "        out1 = self.Linear1(x) \n",
    "        type_output = self.type_classifier((out1+out2+out3+out4))\n",
    "        \n",
    "        return type_output, polarity_output, tense_output, certainty_output\n",
    "\n",
    "gc.collect() # python 자원 관리 \n",
    "torch.cuda.empty_cache() # gpu 자원관리\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenized = tokenizer(test.문장.tolist(), padding=True, truncation=True, max_length=length, return_tensors=\"pt\")\n",
    "test_dataset = CustomDataset(tokenized, None)\n",
    "test_args = TrainingArguments(\n",
    "    output_dir = './',\n",
    "    do_train = False,\n",
    "    do_predict = True,\n",
    "    per_device_eval_batch_size = 256,   \n",
    "    dataloader_drop_last = False    \n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(5):\n",
    "        print(f'Round {i}')\n",
    "        # model = AutoModel.from_pretrained(recent_file('custom_model'), config=config)\n",
    "        model = CustomModel().to(device)\n",
    "        model.load_state_dict(torch.load(f\"{recent_file(f'741/fold_{i}')}/pytorch_model.bin\"))\n",
    "        trainer = CustomTrainer(\n",
    "                      model = model, \n",
    "                      args = test_args)\n",
    "        test_results.append(trainer.predict(test_dataset))\n",
    "        del model\n",
    "        del trainer\n",
    "        gc.collect() # python 자원 관리 \n",
    "        torch.cuda.empty_cache() # gpu 자원관리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cc2144-20c3-4c9f-b639-dd0c44b287ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 740\n",
    "device = torch.device(\"cuda\")\n",
    "model_path = \"kykim/electra-kor-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "length = train['문장'].str.len().max()\n",
    "config=AutoConfig.from_pretrained(model_path)\n",
    "config._name_or_path = 'kr.kim'\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomModel, self).__init__()\n",
    "        if model_path == 'monologg/kobigbird-bert-base':\n",
    "            config.attention_type = \"original_full\"\n",
    "        self.base_model = AutoModel.from_pretrained(model_path, config=config)\n",
    "        try:\n",
    "            self.out = self.base_model.encoder.layer[-1].output.dense.out_features\n",
    "        except:\n",
    "            self.out = 768\n",
    "        # self.linear = nn.Linear(768, 768//2)\n",
    "\n",
    "        self.type_classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=self.out, out_features=4),\n",
    "        )\n",
    "        self.polarity_classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=self.out, out_features=3),\n",
    "        )\n",
    "        self.tense_classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=self.out, out_features=3),\n",
    "        )\n",
    "        self.certainty_classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=self.out, out_features=2),\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels=None, token_type_ids=None):\n",
    "        x = self.base_model(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
    "        # x = self.linear(x)\n",
    "        # 문장 유형, 극성, 시제, 확실성을 각각 분류\n",
    "        type_output = self.type_classifier(x[:,0,:].view(-1,self.out))\n",
    "        polarity_output = self.polarity_classifier(x[:,0,:].view(-1,self.out))\n",
    "        tense_output = self.tense_classifier(x[:,0,:].view(-1,self.out))\n",
    "        certainty_output = self.certainty_classifier(x[:,0,:].view(-1,self.out))\n",
    "        return type_output, polarity_output, tense_output, certainty_output\n",
    "\n",
    "gc.collect() # python 자원 관리 \n",
    "torch.cuda.empty_cache() # gpu 자원관리\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenized = tokenizer(test.문장.tolist(), padding=True, truncation=True, max_length=length, return_tensors=\"pt\")\n",
    "test_dataset = CustomDataset(tokenized, None)\n",
    "test_args = TrainingArguments(\n",
    "    output_dir = './',\n",
    "    do_train = False,\n",
    "    do_predict = True,\n",
    "    per_device_eval_batch_size = 256,   \n",
    "    dataloader_drop_last = False    \n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(1, 5):\n",
    "        print(f'Round {i}')\n",
    "        # model = AutoModel.from_pretrained(recent_file('custom_model'), config=config)\n",
    "        model = CustomModel().to(device)\n",
    "        model.load_state_dict(torch.load(f\"{recent_file(f'740/fold_{i}')}/pytorch_model.bin\"))\n",
    "        trainer = CustomTrainer(\n",
    "                      model = model, \n",
    "                      args = test_args)\n",
    "        test_results.append(trainer.predict(test_dataset))\n",
    "        del model\n",
    "        del trainer\n",
    "        gc.collect() # python 자원 관리 \n",
    "        torch.cuda.empty_cache() # gpu 자원관리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53bd65a-e454-4296-8562-20270f27e0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 744\n",
    "device = torch.device(\"cuda\")\n",
    "model_path = \"monologg/kobigbird-bert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "length = train['문장'].str.len().max()\n",
    "config=AutoConfig.from_pretrained(model_path)\n",
    "config._name_or_path = 'kr.kim'\n",
    "\n",
    "gc.collect() # python 자원 관리 \n",
    "torch.cuda.empty_cache() # gpu 자원관리\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenized = tokenizer(test.문장.tolist(), padding=True, truncation=True, max_length=length, return_tensors=\"pt\")\n",
    "test_dataset = CustomDataset(tokenized, None)\n",
    "test_args = TrainingArguments(\n",
    "    output_dir = './',\n",
    "    do_train = False,\n",
    "    do_predict = True,\n",
    "    per_device_eval_batch_size = 512,   \n",
    "    dataloader_drop_last = False    \n",
    ")\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomModel, self).__init__()\n",
    "        if model_path == 'monologg/kobigbird-bert-base':\n",
    "            config.attention_type = \"original_full\"\n",
    "        self.base_model = AutoModel.from_pretrained(model_path, config=config)\n",
    "        self.out = self.base_model.encoder.layer[-1].output.dense.out_features\n",
    "        # self.linear = nn.Linear(768, 768//2)\n",
    "\n",
    "        self.type_classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=self.out, out_features=4),\n",
    "        )\n",
    "        self.polarity_classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=self.out, out_features=3),\n",
    "        )\n",
    "        self.tense_classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=self.out, out_features=3),\n",
    "        )\n",
    "        self.certainty_classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=self.out, out_features=2),\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels=None, token_type_ids=None):\n",
    "        x = self.base_model(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
    "        # x = self.linear(x)\n",
    "        # 문장 유형, 극성, 시제, 확실성을 각각 분류\n",
    "        type_output = self.type_classifier(x[:,0,:].view(-1,self.out))\n",
    "        polarity_output = self.polarity_classifier(x[:,0,:].view(-1,self.out))\n",
    "        tense_output = self.tense_classifier(x[:,0,:].view(-1,self.out))\n",
    "        certainty_output = self.certainty_classifier(x[:,0,:].view(-1,self.out))\n",
    "        return type_output, polarity_output, tense_output, certainty_output\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(5):\n",
    "        print(f'Round {i}')\n",
    "        # model = AutoModel.from_pretrained(recent_file('custom_model'), config=config)\n",
    "        model = CustomModel().to(device)\n",
    "        model.load_state_dict(torch.load(f\"{recent_file(f'744/fold_{i}')}/pytorch_model.bin\"))\n",
    "        trainer = CustomTrainer(\n",
    "                      model = model, \n",
    "                      args = test_args)\n",
    "        test_results.append(trainer.predict(test_dataset))\n",
    "        del model\n",
    "        del trainer\n",
    "        gc.collect() # python 자원 관리 \n",
    "        torch.cuda.empty_cache() # gpu 자원관리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617bb5a0-0aad-4869-a10b-aba9acd9223c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 749\n",
    "device = torch.device(\"cuda\")\n",
    "model_path = \"monologg/kobigbird-bert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "length = train['문장'].str.len().max()\n",
    "config=AutoConfig.from_pretrained(model_path)\n",
    "config._name_or_path = 'kr.kim'\n",
    "\n",
    "gc.collect() # python 자원 관리 \n",
    "torch.cuda.empty_cache() # gpu 자원관리\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenized = tokenizer(test.문장.tolist(), padding=True, truncation=True, max_length=length, return_tensors=\"pt\")\n",
    "test_dataset = CustomDataset(tokenized, None)\n",
    "test_args = TrainingArguments(\n",
    "    output_dir = './',\n",
    "    do_train = False,\n",
    "    do_predict = True,\n",
    "    per_device_eval_batch_size = 512,   \n",
    "    dataloader_drop_last = False    \n",
    ")\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomModel, self).__init__()\n",
    "        if model_path == 'monologg/kobigbird-bert-base':\n",
    "            config.attention_type = \"original_full\"\n",
    "        self.base_model = AutoModel.from_pretrained(model_path, config=config)\n",
    "        self.out = self.base_model.encoder.layer[-1].output.dense.out_features\n",
    "        # self.linear = nn.Linear(768, 768//2)\n",
    "\n",
    "        self.type_classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=self.out, out_features=4),\n",
    "        )\n",
    "        self.polarity_classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=self.out, out_features=3),\n",
    "        )\n",
    "        self.tense_classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=self.out, out_features=3),\n",
    "        )\n",
    "        self.certainty_classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=self.out, out_features=2),\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels=None, token_type_ids=None):\n",
    "        x = self.base_model(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
    "        # x = self.linear(x)\n",
    "        # 문장 유형, 극성, 시제, 확실성을 각각 분류\n",
    "        type_output = self.type_classifier(x[:,0,:].view(-1,self.out))\n",
    "        polarity_output = self.polarity_classifier(x[:,0,:].view(-1,self.out))\n",
    "        tense_output = self.tense_classifier(x[:,0,:].view(-1,self.out))\n",
    "        certainty_output = self.certainty_classifier(x[:,0,:].view(-1,self.out))\n",
    "        return type_output, polarity_output, tense_output, certainty_output\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(5):\n",
    "        print(f'Round {i}')\n",
    "        # model = AutoModel.from_pretrained(recent_file('custom_model'), config=config)\n",
    "        model = CustomModel().to(device)\n",
    "        model.load_state_dict(torch.load(f\"{recent_file(f'749/fold_{i}')}/pytorch_model.bin\"))\n",
    "        trainer = CustomTrainer(\n",
    "                      model = model, \n",
    "                      args = test_args)\n",
    "        test_results.append(trainer.predict(test_dataset))\n",
    "        del model\n",
    "        del trainer\n",
    "        gc.collect() # python 자원 관리 \n",
    "        torch.cuda.empty_cache() # gpu 자원관리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884e41e1-bc95-400d-922d-f01f4c73f770",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9558dc5-3db7-4275-8d2c-18ba58783256",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test['유형'] = list(map(lambda x : 유형.inverse_transform([np.argmax(x)]), sum(list(map(lambda x: x.predictions[0], test_results)))/len(test_results)))\n",
    "test['극성'] = list(map(lambda x : 극성.inverse_transform([np.argmax(x)]), sum(list(map(lambda x: x.predictions[1], test_results)))/len(test_results)))\n",
    "test['시제'] = list(map(lambda x : 시제.inverse_transform([np.argmax(x)]), sum(list(map(lambda x: x.predictions[2], test_results)))/len(test_results)))\n",
    "test['확실성'] = list(map(lambda x : 확실성.inverse_transform([np.argmax(x)]), sum(list(map(lambda x: x.predictions[3], test_results)))/len(test_results)))\n",
    "\n",
    "test['유형'] = list(map(lambda x : x[0], test['유형']))\n",
    "test['극성'] = list(map(lambda x : x[0], test['극성']))\n",
    "test['시제'] = list(map(lambda x : x[0], test['시제']))\n",
    "test['확실성'] = list(map(lambda x : x[0], test['확실성']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db41568b-37eb-4e1f-878b-ffa7df37e6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['label'] = test['유형'] + '-' + test['극성'] + '-' + test['시제'] + '-' + test['확실성']\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a5530a-a3cb-4576-ba2c-87a39e813e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('C:\\\\Users\\\\hist\\\\Documents\\\\GitHub\\\\competition\\\\dacon\\\\문장분류대회\\\\sample_submission.csv')\n",
    "sub['label'] = test['label']\n",
    "sub.to_csv(f'앙상블5.csv', index=False, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497e84cd-0ab6-4be7-8c4e-8860062e61de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
