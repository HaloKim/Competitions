{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1a84908-231a-4f60-8f97-feb197a8e4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\halo\\miniconda3\\envs\\ml\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import numpy as np\n",
    "from typing import Optional, Sequence\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, f1_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from torch.nn import functional as F\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification, AutoTokenizer, EarlyStoppingCallback, AutoModel, AutoConfig\n",
    "\n",
    "import gc\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d093495-74ef-492a-9a58-465814b4f43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv').drop(['ID'], axis=1)\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2589fae-8239-4b0c-bef7-a20cadaaaeed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>문장</th>\n",
       "      <th>유형</th>\n",
       "      <th>극성</th>\n",
       "      <th>시제</th>\n",
       "      <th>확실성</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.75%포인트 금리 인상은 1994년 이후 28년 만에 처음이다.</td>\n",
       "      <td>사실형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>현재</td>\n",
       "      <td>확실</td>\n",
       "      <td>사실형-긍정-현재-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>이어 ＂앞으로 전문가들과 함께 4주 단위로 상황을 재평가할 예정＂이라며 ＂그 이전이...</td>\n",
       "      <td>사실형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>과거</td>\n",
       "      <td>확실</td>\n",
       "      <td>사실형-긍정-과거-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>정부가 고유가 대응을 위해 7월부터 연말까지 유류세 인하 폭을 30%에서 37%까지...</td>\n",
       "      <td>사실형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>미래</td>\n",
       "      <td>확실</td>\n",
       "      <td>사실형-긍정-미래-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>서울시는 올해 3월 즉시 견인 유예시간 60분을 제공하겠다고 밝혔지만, 하루 만에 ...</td>\n",
       "      <td>사실형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>과거</td>\n",
       "      <td>확실</td>\n",
       "      <td>사실형-긍정-과거-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>익사한 자는 사다리에 태워 거꾸로 놓고 소금으로 코를 막아 가득 채운다.</td>\n",
       "      <td>사실형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>현재</td>\n",
       "      <td>확실</td>\n",
       "      <td>사실형-긍정-현재-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16536</th>\n",
       "      <td>＇신동덤＇은 ＇신비한 동물사전＇과 ＇해리 포터＇ 시리즈를 잇는 마법 어드벤처물로, ...</td>\n",
       "      <td>사실형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>과거</td>\n",
       "      <td>확실</td>\n",
       "      <td>사실형-긍정-과거-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16537</th>\n",
       "      <td>수족냉증은 어릴 때부터 심했으며 관절은 어디 한 곳이 아니고 목, 어깨, 팔꿈치, ...</td>\n",
       "      <td>사실형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>과거</td>\n",
       "      <td>확실</td>\n",
       "      <td>사실형-긍정-과거-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16538</th>\n",
       "      <td>김금희 소설가는 ＂계약서 조정이 그리 어려운가 작가를 격려한다면서 그런 문구 하나 ...</td>\n",
       "      <td>사실형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>과거</td>\n",
       "      <td>확실</td>\n",
       "      <td>사실형-긍정-과거-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16539</th>\n",
       "      <td>1만명이 넘는 방문자수를 기록한 이번 전시회는 총 77개 작품을 넥슨 사옥을 그대로...</td>\n",
       "      <td>사실형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>과거</td>\n",
       "      <td>불확실</td>\n",
       "      <td>사실형-긍정-과거-불확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16540</th>\n",
       "      <td>《목민심서》의 내용이다.</td>\n",
       "      <td>사실형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>현재</td>\n",
       "      <td>확실</td>\n",
       "      <td>사실형-긍정-현재-확실</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16541 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      문장   유형  극성  시제  확실성  \\\n",
       "0                  0.75%포인트 금리 인상은 1994년 이후 28년 만에 처음이다.  사실형  긍정  현재   확실   \n",
       "1      이어 ＂앞으로 전문가들과 함께 4주 단위로 상황을 재평가할 예정＂이라며 ＂그 이전이...  사실형  긍정  과거   확실   \n",
       "2      정부가 고유가 대응을 위해 7월부터 연말까지 유류세 인하 폭을 30%에서 37%까지...  사실형  긍정  미래   확실   \n",
       "3      서울시는 올해 3월 즉시 견인 유예시간 60분을 제공하겠다고 밝혔지만, 하루 만에 ...  사실형  긍정  과거   확실   \n",
       "4               익사한 자는 사다리에 태워 거꾸로 놓고 소금으로 코를 막아 가득 채운다.  사실형  긍정  현재   확실   \n",
       "...                                                  ...  ...  ..  ..  ...   \n",
       "16536  ＇신동덤＇은 ＇신비한 동물사전＇과 ＇해리 포터＇ 시리즈를 잇는 마법 어드벤처물로, ...  사실형  긍정  과거   확실   \n",
       "16537  수족냉증은 어릴 때부터 심했으며 관절은 어디 한 곳이 아니고 목, 어깨, 팔꿈치, ...  사실형  긍정  과거   확실   \n",
       "16538  김금희 소설가는 ＂계약서 조정이 그리 어려운가 작가를 격려한다면서 그런 문구 하나 ...  사실형  긍정  과거   확실   \n",
       "16539  1만명이 넘는 방문자수를 기록한 이번 전시회는 총 77개 작품을 넥슨 사옥을 그대로...  사실형  긍정  과거  불확실   \n",
       "16540                                      《목민심서》의 내용이다.  사실형  긍정  현재   확실   \n",
       "\n",
       "               label  \n",
       "0       사실형-긍정-현재-확실  \n",
       "1       사실형-긍정-과거-확실  \n",
       "2       사실형-긍정-미래-확실  \n",
       "3       사실형-긍정-과거-확실  \n",
       "4       사실형-긍정-현재-확실  \n",
       "...              ...  \n",
       "16536   사실형-긍정-과거-확실  \n",
       "16537   사실형-긍정-과거-확실  \n",
       "16538   사실형-긍정-과거-확실  \n",
       "16539  사실형-긍정-과거-불확실  \n",
       "16540   사실형-긍정-현재-확실  \n",
       "\n",
       "[16541 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a7cfca6-de9c-48ae-b335-2c0a204be19b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['사실형', '추론형', '예측형', '대화형'], dtype=object),\n",
       " array(['긍정', '부정', '미정'], dtype=object),\n",
       " array(['현재', '과거', '미래'], dtype=object),\n",
       " array(['확실', '불확실'], dtype=object))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.유형.unique(), train.극성.unique(), train.시제.unique(), train.확실성.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb369936-4700-40bc-94ea-03ecd847c874",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['문장'] = train['문장'].apply(lambda x: re.sub(\"[^ A-Za-z0-9가-힣]\", \"\", x))\n",
    "train['문장'] = train['문장'].apply(lambda x: re.sub(\"[ +]\", \" \", x))\n",
    "\n",
    "test['문장'] = test['문장'].apply(lambda x: re.sub(\"[^ A-Za-z0-9가-힣]\", \"\", x))\n",
    "test['문장'] = test['문장'].apply(lambda x: re.sub(\"[ +]\", \" \", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "121bac3f-5ea6-457c-99d6-4a271c21c2a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                       075포인트 금리 인상은 1994년 이후 28년 만에 처음이다\n",
       "1        이어 앞으로 전문가들과 함께 4주 단위로 상황을 재평가할 예정이라며 그 이전이라도 ...\n",
       "2        정부가 고유가 대응을 위해 7월부터 연말까지 유류세 인하 폭을 30에서 37까지 확대한다\n",
       "3        서울시는 올해 3월 즉시 견인 유예시간 60분을 제공하겠다고 밝혔지만 하루 만에 차...\n",
       "4                  익사한 자는 사다리에 태워 거꾸로 놓고 소금으로 코를 막아 가득 채운다\n",
       "                               ...                        \n",
       "16536    신동덤은 신비한 동물사전과 해리 포터 시리즈를 잇는 마법 어드벤처물로 전편에 이어 ...\n",
       "16537    수족냉증은 어릴 때부터 심했으며 관절은 어디 한 곳이 아니고 목 어깨 팔꿈치 등 허...\n",
       "16538    김금희 소설가는 계약서 조정이 그리 어려운가 작가를 격려한다면서 그런 문구 하나 고...\n",
       "16539    1만명이 넘는 방문자수를 기록한 이번 전시회는 총 77개 작품을 넥슨 사옥을 그대로...\n",
       "16540                                           목민심서의 내용이다\n",
       "Name: 문장, Length: 16541, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['문장']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b049d8bd-0259-4c01-9411-0b9d79d2d43c",
   "metadata": {},
   "source": [
    "# Text Aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a555162-1bee-4cec-a1db-e82b065faed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, X_val, _, _ = train_test_split(train, train.label, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "298af6ab-a1ac-4b46-bcf8-cdbd3709c5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/catSirup/KorEDA/blob/master/eda.py\n",
    "def swap_word(new_words):\n",
    "    random_idx_1 = random.randint(0, len(new_words)-1)\n",
    "    random_idx_2 = random_idx_1\n",
    "    counter = 0\n",
    "\n",
    "    while random_idx_2 == random_idx_1:\n",
    "        random_idx_2 = random.randint(0, len(new_words)-1)\n",
    "        counter += 1\n",
    "        if counter > 3:\n",
    "            return new_words\n",
    "\n",
    "    new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1]\n",
    "    return new_words\n",
    "\n",
    "def random_swap(words, n):\n",
    "    new_words = words.copy()\n",
    "    for _ in range(n):\n",
    "        new_words = swap_word(new_words)\n",
    "    return new_words\n",
    "\n",
    "def text_aug(sentence, alpha_rs = 0.1, num_aug=3):\n",
    "    words = sentence.split(' ')\n",
    "    words = [word for word in words if word != \"\"]\n",
    "    num_words = len(words)\n",
    "\n",
    "    augmented_sentences = []\n",
    "    num_new_per_technique = num_aug\n",
    "\n",
    "    n_rs = max(1, int(alpha_rs*num_words))\n",
    "\n",
    "    for _ in range(num_new_per_technique):\n",
    "        a_words = random_swap(words, n_rs)\n",
    "        augmented_sentences.append(\" \".join(a_words))\n",
    "\n",
    "    augmented_sentences = [sentence for sentence in augmented_sentences]\n",
    "    random.shuffle(augmented_sentences)\n",
    "\n",
    "    if num_aug >= 1:\n",
    "        augmented_sentences = augmented_sentences[:num_aug]\n",
    "    else:\n",
    "        keep_prob = num_aug / len(augmented_sentences)\n",
    "        augmented_sentences = [s for s in augmented_sentences if random.uniform(0, 1) < keep_prob]\n",
    "    return augmented_sentences\n",
    "\n",
    "aug = train['문장'].apply(lambda x: text_aug(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1c2c457-c286-4489-acd4-f0bb866e8f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1 = train.copy()\n",
    "tmp1['문장'] = list(map(lambda x: x[0], aug))\n",
    "\n",
    "tmp2 = train.copy()\n",
    "tmp2['문장'] = list(map(lambda x: x[1], aug))\n",
    "\n",
    "tmp3 = train.copy()\n",
    "tmp3['문장'] = list(map(lambda x: x[2], aug))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "601cec89-8c2e-4fa6-86ec-8ad2e4180cea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>문장</th>\n",
       "      <th>유형</th>\n",
       "      <th>극성</th>\n",
       "      <th>시제</th>\n",
       "      <th>확실성</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>또한 김종직 14311492은 점필재집에서 안동의 유명한 누각 영호루는 공민왕이 손...</td>\n",
       "      <td>사실형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>과거</td>\n",
       "      <td>확실</td>\n",
       "      <td>사실형-긍정-과거-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>호가는 17억원 수준이다</td>\n",
       "      <td>사실형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>현재</td>\n",
       "      <td>확실</td>\n",
       "      <td>사실형-긍정-현재-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>국가를 역사는 비행기의 출현과 동시에 시작되어 비행기 속도가 느린 초창기에는 하늘에...</td>\n",
       "      <td>사실형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>현재</td>\n",
       "      <td>확실</td>\n",
       "      <td>사실형-긍정-현재-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5일 금융권에 따르면 은행들은 우한 폐렴으로 피해를 본 기업 및 소상공인에게 대출 ...</td>\n",
       "      <td>사실형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>현재</td>\n",
       "      <td>확실</td>\n",
       "      <td>사실형-긍정-현재-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>지금 순위로 올림픽 국가대표에 나설 수 있는 고진영1위 박성현2위 13위 4명이 과...</td>\n",
       "      <td>추론형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>현재</td>\n",
       "      <td>확실</td>\n",
       "      <td>추론형-긍정-현재-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49030</th>\n",
       "      <td>2관왕에 부문에 이름을 올린 조커는 남우주연상과 음악상 11개 그쳤다</td>\n",
       "      <td>사실형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>과거</td>\n",
       "      <td>확실</td>\n",
       "      <td>사실형-긍정-과거-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49031</th>\n",
       "      <td>지난 20년간 한국 못했다는 영향력이 커졌음에도 한 번도 오스카상 후보에 오르지 영...</td>\n",
       "      <td>사실형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>과거</td>\n",
       "      <td>확실</td>\n",
       "      <td>사실형-긍정-과거-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49032</th>\n",
       "      <td>불안한 10대들과 공감하며 소통했던 라디오 문제 모습부터 사회적 DJ의 제기와 해결...</td>\n",
       "      <td>사실형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>미래</td>\n",
       "      <td>확실</td>\n",
       "      <td>사실형-긍정-미래-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49033</th>\n",
       "      <td>재판부는 피고인은 여호와의 증인 신자로서 진지한 판단했다 양심 때문에 입영을 하지 ...</td>\n",
       "      <td>사실형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>과거</td>\n",
       "      <td>확실</td>\n",
       "      <td>사실형-긍정-과거-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49034</th>\n",
       "      <td>꽃구경이 그립다고요 그래서 컷 신구대식물원에 환하게 피어난 꽃 사진을 한 서비스로 ...</td>\n",
       "      <td>대화형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>과거</td>\n",
       "      <td>확실</td>\n",
       "      <td>대화형-긍정-과거-확실</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49035 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      문장   유형  극성  시제 확실성  \\\n",
       "0      또한 김종직 14311492은 점필재집에서 안동의 유명한 누각 영호루는 공민왕이 손...  사실형  긍정  과거  확실   \n",
       "1                                          호가는 17억원 수준이다  사실형  긍정  현재  확실   \n",
       "2      국가를 역사는 비행기의 출현과 동시에 시작되어 비행기 속도가 느린 초창기에는 하늘에...  사실형  긍정  현재  확실   \n",
       "3      5일 금융권에 따르면 은행들은 우한 폐렴으로 피해를 본 기업 및 소상공인에게 대출 ...  사실형  긍정  현재  확실   \n",
       "4      지금 순위로 올림픽 국가대표에 나설 수 있는 고진영1위 박성현2위 13위 4명이 과...  추론형  긍정  현재  확실   \n",
       "...                                                  ...  ...  ..  ..  ..   \n",
       "49030             2관왕에 부문에 이름을 올린 조커는 남우주연상과 음악상 11개 그쳤다  사실형  긍정  과거  확실   \n",
       "49031  지난 20년간 한국 못했다는 영향력이 커졌음에도 한 번도 오스카상 후보에 오르지 영...  사실형  긍정  과거  확실   \n",
       "49032  불안한 10대들과 공감하며 소통했던 라디오 문제 모습부터 사회적 DJ의 제기와 해결...  사실형  긍정  미래  확실   \n",
       "49033  재판부는 피고인은 여호와의 증인 신자로서 진지한 판단했다 양심 때문에 입영을 하지 ...  사실형  긍정  과거  확실   \n",
       "49034  꽃구경이 그립다고요 그래서 컷 신구대식물원에 환하게 피어난 꽃 사진을 한 서비스로 ...  대화형  긍정  과거  확실   \n",
       "\n",
       "              label  \n",
       "0      사실형-긍정-과거-확실  \n",
       "1      사실형-긍정-현재-확실  \n",
       "2      사실형-긍정-현재-확실  \n",
       "3      사실형-긍정-현재-확실  \n",
       "4      추론형-긍정-현재-확실  \n",
       "...             ...  \n",
       "49030  사실형-긍정-과거-확실  \n",
       "49031  사실형-긍정-과거-확실  \n",
       "49032  사실형-긍정-미래-확실  \n",
       "49033  사실형-긍정-과거-확실  \n",
       "49034  대화형-긍정-과거-확실  \n",
       "\n",
       "[49035 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.concat([train,tmp1,tmp2,tmp3]).drop_duplicates(keep='first').sample(frac=1).reset_index(drop=True)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "154018d4-1701-437b-83aa-f06d0c56e133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(496, 378)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['문장'].str.len().max(), test['문장'].str.len().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c6844e-4f82-4739-9789-0aad9434e2fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16ed3b7b-6258-4b28-b830-95fde97bb649",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|████████████████████████████████████████████████████████████████████████████| 373/373 [00:00<?, ?B/s]\n",
      "C:\\Users\\halo\\miniconda3\\envs\\ml\\lib\\site-packages\\huggingface_hub\\file_download.py:127: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\halo\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading: 100%|███████████████████████████████████████████████████████████████████| 241k/241k [00:00<00:00, 321kB/s]\n",
      "Downloading: 100%|███████████████████████████████████████████████████████████████████| 492k/492k [00:01<00:00, 492kB/s]\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████| 169/169 [00:00<00:00, 382kB/s]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model_path = \"monologg/kobigbird-bert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "length = train['문장'].str.len().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a7301db-513f-4620-b4bf-003dcf2ec91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            st_type = self.labels['type'][idx]\n",
    "            st_polarity = self.labels['polarity'][idx]\n",
    "            st_tense = self.labels['tense'][idx]\n",
    "            st_certainty = self.labels['certainty'][idx]\n",
    "            item[\"labels\"] = torch.tensor(st_type), torch.tensor(st_polarity), torch.tensor(st_tense), torch.tensor(st_certainty)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f124b81-8db7-4397-904f-042e35c9a9b5",
   "metadata": {},
   "source": [
    "# HuggingFace Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ae53b1-754c-47a3-8105-f630114df803",
   "metadata": {},
   "source": [
    "## config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39fe649c-cf27-4a52-82fe-0d735332feec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|████████████████████████████████████████████████████████████████████████████| 870/870 [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BigBirdConfig {\n",
       "  \"_name_or_path\": \"kr.kim\",\n",
       "  \"architectures\": [\n",
       "    \"BigBirdForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"attention_type\": \"block_sparse\",\n",
       "  \"block_size\": 64,\n",
       "  \"bos_token_id\": 5,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"eos_token_id\": 6,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu_new\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 4096,\n",
       "  \"model_type\": \"big_bird\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"num_random_blocks\": 3,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"rescale_embeddings\": false,\n",
       "  \"sep_token_id\": 3,\n",
       "  \"tokenizer_class\": \"BertTokenizer\",\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.25.1\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_bias\": true,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32500\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config=AutoConfig.from_pretrained(model_path)\n",
    "config._name_or_path = 'kr.kim'\n",
    "print(config.num_hidden_layers)\n",
    "# config.num_hidden_layers = 17\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb1806a-bd54-4d41-ba21-36ea1a04de41",
   "metadata": {},
   "source": [
    "## custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4171e3a9-06a8-4bdc-ba7c-c0c489058619",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomModel, self).__init__()\n",
    "        if model_path == 'monologg/kobigbird-bert-base':\n",
    "            config.attention_type = \"original_full\"\n",
    "        self.base_model = AutoModel.from_pretrained(model_path, config=config)\n",
    "        self.out = 768\n",
    "        # self.linear = nn.Linear(768, 768//2)\n",
    "\n",
    "        self.type_classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=self.out, out_features=4),\n",
    "        )\n",
    "        self.polarity_classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=self.out, out_features=3),\n",
    "        )\n",
    "        self.tense_classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=self.out, out_features=3),\n",
    "        )\n",
    "        self.certainty_classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=self.out, out_features=2),\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels=None, token_type_ids=None):\n",
    "        x = self.base_model(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
    "        # x = self.linear(x)\n",
    "        # 문장 유형, 극성, 시제, 확실성을 각각 분류\n",
    "        type_output = self.type_classifier(x[:,0,:].view(-1,self.out))\n",
    "        polarity_output = self.polarity_classifier(x[:,0,:].view(-1,self.out))\n",
    "        tense_output = self.tense_classifier(x[:,0,:].view(-1,self.out))\n",
    "        certainty_output = self.certainty_classifier(x[:,0,:].view(-1,self.out))\n",
    "        return type_output, polarity_output, tense_output, certainty_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7229301a-53c2-4907-adc0-a838d5c63594",
   "metadata": {},
   "source": [
    "## arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "620c41d3-e492-4e97-958e-b420ad350890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer arguments\n",
    "lr = 1e-4\n",
    "stop = 3\n",
    "epoch = 1000\n",
    "batch = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af99029a-eb0e-458c-97c1-cd1f2d52aa8d",
   "metadata": {},
   "source": [
    "## loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff558037-0c56-4be6-aae5-b16793f1e262",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\" Focal Loss, as described in https://arxiv.org/abs/1708.02002.\n",
    "    It is essentially an enhancement to cross entropy loss and is\n",
    "    useful for classification tasks when there is a large class imbalance.\n",
    "    x is expected to contain raw, unnormalized scores for each class.\n",
    "    y is expected to contain class labels.\n",
    "    Shape:\n",
    "        - x: (batch_size, C) or (batch_size, C, d1, d2, ..., dK), K > 0.\n",
    "        - y: (batch_size,) or (batch_size, d1, d2, ..., dK), K > 0.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 alpha: Optional[Tensor] = None,\n",
    "                 gamma: float = 0.,\n",
    "                 reduction: str = 'mean',\n",
    "                 ignore_index: int = -100):\n",
    "        \"\"\"Constructor.\n",
    "        Args:\n",
    "            alpha (Tensor, optional): Weights for each class. Defaults to None.\n",
    "            gamma (float, optional): A constant, as described in the paper.\n",
    "                Defaults to 0.\n",
    "            reduction (str, optional): 'mean', 'sum' or 'none'.\n",
    "                Defaults to 'mean'.\n",
    "            ignore_index (int, optional): class label to ignore.\n",
    "                Defaults to -100.\n",
    "        \"\"\"\n",
    "        if reduction not in ('mean', 'sum', 'none'):\n",
    "            raise ValueError(\n",
    "                'Reduction must be one of: \"mean\", \"sum\", \"none\".')\n",
    "\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.ignore_index = ignore_index\n",
    "        self.reduction = reduction\n",
    "\n",
    "        self.nll_loss = nn.NLLLoss(\n",
    "            weight=alpha, reduction='none', ignore_index=ignore_index)\n",
    "\n",
    "    def __repr__(self):\n",
    "        arg_keys = ['alpha', 'gamma', 'ignore_index', 'reduction']\n",
    "        arg_vals = [self.__dict__[k] for k in arg_keys]\n",
    "        arg_strs = [f'{k}={v!r}' for k, v in zip(arg_keys, arg_vals)]\n",
    "        arg_str = ', '.join(arg_strs)\n",
    "        return f'{type(self).__name__}({arg_str})'\n",
    "\n",
    "    def forward(self, x: Tensor, y: Tensor) -> Tensor:\n",
    "        if x.ndim > 2:\n",
    "            # (N, C, d1, d2, ..., dK) --> (N * d1 * ... * dK, C)\n",
    "            c = x.shape[1]\n",
    "            x = x.permute(0, *range(2, x.ndim), 1).reshape(-1, c)\n",
    "            # (N, d1, d2, ..., dK) --> (N * d1 * ... * dK,)\n",
    "            y = y.view(-1)\n",
    "\n",
    "        unignored_mask = y != self.ignore_index\n",
    "        y = y[unignored_mask]\n",
    "        if len(y) == 0:\n",
    "            return torch.tensor(0.)\n",
    "        x = x[unignored_mask]\n",
    "\n",
    "        # compute weighted cross entropy term: -alpha * log(pt)\n",
    "        # (alpha is already part of self.nll_loss)\n",
    "        log_p = F.log_softmax(x, dim=-1)\n",
    "        ce = self.nll_loss(log_p, y)\n",
    "\n",
    "        # get true class column from each row\n",
    "        all_rows = torch.arange(len(x))\n",
    "        log_pt = log_p[all_rows, y]\n",
    "\n",
    "        # compute focal term: (1 - pt)^gamma\n",
    "        pt = log_pt.exp()\n",
    "        focal_term = (1 - pt)**self.gamma\n",
    "\n",
    "        # the full loss: -alpha * ((1 - pt)^gamma) * log(pt)\n",
    "        loss = focal_term * ce\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "def focal_loss(alpha: Optional[Sequence] = None,\n",
    "               gamma: float = 0.,\n",
    "               reduction: str = 'mean',\n",
    "               ignore_index: int = -100,\n",
    "               device='cpu',\n",
    "               dtype=torch.float32) -> FocalLoss:\n",
    "    \"\"\"Factory function for FocalLoss.\n",
    "    Args:\n",
    "        alpha (Sequence, optional): Weights for each class. Will be converted\n",
    "            to a Tensor if not None. Defaults to None.\n",
    "        gamma (float, optional): A constant, as described in the paper.\n",
    "            Defaults to 0.\n",
    "        reduction (str, optional): 'mean', 'sum' or 'none'.\n",
    "            Defaults to 'mean'.\n",
    "        ignore_index (int, optional): class label to ignore.\n",
    "            Defaults to -100.\n",
    "        device (str, optional): Device to move alpha to. Defaults to 'cpu'.\n",
    "        dtype (torch.dtype, optional): dtype to cast alpha to.\n",
    "            Defaults to torch.float32.\n",
    "    Returns:\n",
    "        A FocalLoss object\n",
    "    \"\"\"\n",
    "    if alpha is not None:\n",
    "        if not isinstance(alpha, Tensor):\n",
    "            alpha = torch.tensor(alpha)\n",
    "        alpha = alpha.to(device=device, dtype=dtype)\n",
    "\n",
    "    fl = FocalLoss(\n",
    "        alpha=alpha,\n",
    "        gamma=gamma,\n",
    "        reduction=reduction,\n",
    "        ignore_index=ignore_index)\n",
    "    return fl\n",
    "        \n",
    "def compute_metrics(pred):\n",
    "    # label = [[cls1,cls2,...],]\n",
    "    # preds = n list\n",
    "    focal_loss = FocalLoss()\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions\n",
    "    f1 = []\n",
    "    focal = []\n",
    "    for i in range(4):\n",
    "        # focal.append(focal_loss(torch.tensor(preds[i], dtype=torch.float), torch.tensor(labels[::, i],dtype=torch.float)))\n",
    "        f1.append(f1_score(y_true = labels[::, i], y_pred = preds[i], average='weighted'))\n",
    "    return {\n",
    "        #'focal': sum(focal),\n",
    "        'f1-sum': sum(f1)/4\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bc8d8f-d686-458e-af87-e26502b8201c",
   "metadata": {},
   "source": [
    "## scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cab4f7cb-b726-4e28-8a0b-d1c5eb941fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineAnnealingWarmUpRestarts(_LRScheduler):\n",
    "    def __init__(self, optimizer, T_0, T_mult=1, eta_max=0.1, T_up=0, gamma=1., last_epoch=-1):\n",
    "        if T_0 <= 0 or not isinstance(T_0, int):\n",
    "            raise ValueError(\"Expected positive integer T_0, but got {}\".format(T_0))\n",
    "        if T_mult < 1 or not isinstance(T_mult, int):\n",
    "            raise ValueError(\"Expected integer T_mult >= 1, but got {}\".format(T_mult))\n",
    "        if T_up < 0 or not isinstance(T_up, int):\n",
    "            raise ValueError(\"Expected positive integer T_up, but got {}\".format(T_up))\n",
    "        self.T_0 = T_0\n",
    "        self.T_mult = T_mult\n",
    "        self.base_eta_max = eta_max\n",
    "        self.eta_max = eta_max\n",
    "        self.T_up = T_up\n",
    "        self.T_i = T_0\n",
    "        self.gamma = gamma\n",
    "        self.cycle = 0\n",
    "        self.T_cur = last_epoch\n",
    "        super(CosineAnnealingWarmUpRestarts, self).__init__(optimizer, last_epoch)\n",
    "    \n",
    "    def get_lr(self):\n",
    "        if self.T_cur == -1:\n",
    "            return self.base_lrs\n",
    "        elif self.T_cur < self.T_up:\n",
    "            return [(self.eta_max - base_lr)*self.T_cur / self.T_up + base_lr for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr + (self.eta_max - base_lr) * (1 + math.cos(math.pi * (self.T_cur-self.T_up) / (self.T_i - self.T_up))) / 2\n",
    "                    for base_lr in self.base_lrs]\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "            self.T_cur = self.T_cur + 1\n",
    "            if self.T_cur >= self.T_i:\n",
    "                self.cycle += 1\n",
    "                self.T_cur = self.T_cur - self.T_i\n",
    "                self.T_i = (self.T_i - self.T_up) * self.T_mult + self.T_up\n",
    "        else:\n",
    "            if epoch >= self.T_0:\n",
    "                if self.T_mult == 1:\n",
    "                    self.T_cur = epoch % self.T_0\n",
    "                    self.cycle = epoch // self.T_0\n",
    "                else:\n",
    "                    n = int(math.log((epoch / self.T_0 * (self.T_mult - 1) + 1), self.T_mult))\n",
    "                    self.cycle = n\n",
    "                    self.T_cur = epoch - self.T_0 * (self.T_mult ** n - 1) / (self.T_mult - 1)\n",
    "                    self.T_i = self.T_0 * self.T_mult ** (n)\n",
    "            else:\n",
    "                self.T_i = self.T_0\n",
    "                self.T_cur = epoch\n",
    "                \n",
    "        self.eta_max = self.base_eta_max * (self.gamma**self.cycle)\n",
    "        self.last_epoch = math.floor(epoch)\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr = 0)\n",
    "# scheduler = CosineAnnealingWarmUpRestarts(optimizer, T_0=150, T_mult=1, eta_max=0.1,  T_up=10, gamma=0.5)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=1, \n",
    "#                                                                  T_mult=2, eta_min=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91380cf8-3197-42f8-b11b-60a4f08bea76",
   "metadata": {},
   "source": [
    "## trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f1d46c2-1a0e-4805-b7da-bfb1c9ea2940",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define trainer\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # forward pass\n",
    "        labels = inputs.pop(\"labels\").to(torch.int64)\n",
    "        \n",
    "        type_logit, polarity_logit, tense_logit, certainty_logit = model(**inputs)\n",
    "        \n",
    "        # # simple loss\n",
    "        # criterion = {\n",
    "        #     'type' : nn.CrossEntropyLoss().to(device),\n",
    "        #     'polarity' : nn.CrossEntropyLoss().to(device),\n",
    "        #     'tense' : nn.CrossEntropyLoss().to(device),\n",
    "        #     'certainty' : nn.CrossEntropyLoss().to(device)\n",
    "        # }\n",
    "        # loss = criterion['type'](type_logit, labels[::, 0]) + \\\n",
    "        #             criterion['polarity'](polarity_logit, labels[::, 1]) + \\\n",
    "        #             criterion['tense'](tense_logit,labels[::, 2]) + \\\n",
    "        #             criterion['certainty'](certainty_logit, labels[::, 3])\n",
    "        \n",
    "        # focal loss\n",
    "        criterion = {\n",
    "            'type' : FocalLoss().to(device),\n",
    "            'polarity' : FocalLoss().to(device),\n",
    "            'tense' : FocalLoss().to(device),\n",
    "            'certainty' : FocalLoss().to(device)\n",
    "        }\n",
    "        # labels = labels.type(torch.float).clone().detach()\n",
    "        loss = criterion['type'](type_logit, labels[::, 0]) + \\\n",
    "                    criterion['polarity'](polarity_logit, labels[::, 1]) + \\\n",
    "                    criterion['tense'](tense_logit, labels[::, 2]) + \\\n",
    "                    criterion['certainty'](certainty_logit, labels[::, 3])\n",
    "\n",
    "        outputs = None, \\\n",
    "                    torch.argmax(type_logit, dim = 1), \\\n",
    "                    torch.argmax(polarity_logit, dim = 1),\\\n",
    "                    torch.argmax(tense_logit, dim = 1),\\\n",
    "                    torch.argmax(certainty_logit, dim = 1)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65a375b-dfcf-4954-9257-7e9dfe3ca30b",
   "metadata": {},
   "source": [
    "# Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27185269-59bb-4d68-ae89-a8d1bddd5846",
   "metadata": {},
   "outputs": [],
   "source": [
    "유형 = LabelEncoder()\n",
    "유형.fit(train['유형'])\n",
    "\n",
    "극성 = LabelEncoder()\n",
    "극성.fit(train['극성'])\n",
    "\n",
    "시제 = LabelEncoder()\n",
    "시제.fit(train['시제'])\n",
    "\n",
    "확실성 = LabelEncoder()\n",
    "확실성.fit(train['확실성'])\n",
    "\n",
    "def encoding(X_train, X_val):\n",
    "    X_train['유형'] = 유형.transform(X_train['유형'])\n",
    "    X_val['유형'] = 유형.transform(X_val['유형'])\n",
    "\n",
    "    X_train['극성'] = 극성.transform(X_train['극성'])\n",
    "    X_val['극성'] = 극성.transform(X_val['극성'])\n",
    "\n",
    "    X_train['시제'] = 시제.transform(X_train['시제'])\n",
    "    X_val['시제'] = 시제.transform(X_val['시제'])\n",
    "\n",
    "    X_train['확실성'] = 확실성.transform(X_train['확실성'])\n",
    "    X_val['확실성'] = 확실성.transform(X_val['확실성'])\n",
    "\n",
    "    train_labels = {\n",
    "        'type' : X_train['유형'].values,\n",
    "        'polarity' : X_train['극성'].values,\n",
    "        'tense' : X_train['시제'].values,\n",
    "        'certainty' : X_train['확실성'].values\n",
    "    }\n",
    "\n",
    "    val_labels = {\n",
    "        'type' : X_val['유형'].values,\n",
    "        'polarity' : X_val['극성'].values,\n",
    "        'tense' : X_val['시제'].values,\n",
    "        'certainty' : X_val['확실성'].values\n",
    "    }\n",
    "    return train_labels, val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de904e23-8948-4b06-b68b-649f51ef2ea7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_layers : 12\n",
      "Round 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████████████████████████████████████████████████████████████| 458M/458M [00:39<00:00, 11.6MB/s]\n",
      "Some weights of the model checkpoint at monologg/kobigbird-bert-base were not used when initializing BigBirdModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BigBirdModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Using cuda_amp half precision backend\n",
      "C:\\Users\\halo\\miniconda3\\envs\\ml\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 39228\n",
      "  Num Epochs = 1000\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total optimization steps = 153000\n",
      "  Number of trainable parameters = 113763084\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='550' max='153000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   550/153000 1:49:55 < 509:42:29, 0.08 it/s, Epoch 3/1000]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1-sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.288000</td>\n",
       "      <td>1.700518</td>\n",
       "      <td>0.827423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.522200</td>\n",
       "      <td>1.353398</td>\n",
       "      <td>0.844360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.343500</td>\n",
       "      <td>1.152042</td>\n",
       "      <td>0.880859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.131300</td>\n",
       "      <td>1.000579</td>\n",
       "      <td>0.900315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.019900</td>\n",
       "      <td>0.887559</td>\n",
       "      <td>0.920604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.861000</td>\n",
       "      <td>0.872052</td>\n",
       "      <td>0.917347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.892600</td>\n",
       "      <td>0.815355</td>\n",
       "      <td>0.922123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.834000</td>\n",
       "      <td>0.822298</td>\n",
       "      <td>0.917181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.820100</td>\n",
       "      <td>0.791586</td>\n",
       "      <td>0.923965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.805600</td>\n",
       "      <td>0.759099</td>\n",
       "      <td>0.927396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.762500</td>\n",
       "      <td>0.737632</td>\n",
       "      <td>0.929671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.794900</td>\n",
       "      <td>0.733732</td>\n",
       "      <td>0.928846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.743500</td>\n",
       "      <td>0.709261</td>\n",
       "      <td>0.935144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.735700</td>\n",
       "      <td>0.707965</td>\n",
       "      <td>0.931266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.757200</td>\n",
       "      <td>0.730143</td>\n",
       "      <td>0.925986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.704600</td>\n",
       "      <td>0.697097</td>\n",
       "      <td>0.931278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.641300</td>\n",
       "      <td>0.672345</td>\n",
       "      <td>0.937298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.634100</td>\n",
       "      <td>0.653416</td>\n",
       "      <td>0.940514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.618600</td>\n",
       "      <td>0.638181</td>\n",
       "      <td>0.936387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.610100</td>\n",
       "      <td>0.631622</td>\n",
       "      <td>0.942122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.614100</td>\n",
       "      <td>0.627654</td>\n",
       "      <td>0.943290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.592000</td>\n",
       "      <td>0.599574</td>\n",
       "      <td>0.943260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.560700</td>\n",
       "      <td>0.604586</td>\n",
       "      <td>0.943953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.575100</td>\n",
       "      <td>0.550479</td>\n",
       "      <td>0.948319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.532000</td>\n",
       "      <td>0.551963</td>\n",
       "      <td>0.950696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.551800</td>\n",
       "      <td>0.552675</td>\n",
       "      <td>0.951487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.474900</td>\n",
       "      <td>0.531311</td>\n",
       "      <td>0.952753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.496800</td>\n",
       "      <td>0.517154</td>\n",
       "      <td>0.953576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.515200</td>\n",
       "      <td>0.511710</td>\n",
       "      <td>0.951125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.527400</td>\n",
       "      <td>0.483319</td>\n",
       "      <td>0.955995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.451300</td>\n",
       "      <td>0.467062</td>\n",
       "      <td>0.957360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.354300</td>\n",
       "      <td>0.448412</td>\n",
       "      <td>0.962519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.323400</td>\n",
       "      <td>0.444727</td>\n",
       "      <td>0.961794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.329200</td>\n",
       "      <td>0.437796</td>\n",
       "      <td>0.963282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.353100</td>\n",
       "      <td>0.434280</td>\n",
       "      <td>0.963240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.320900</td>\n",
       "      <td>0.413986</td>\n",
       "      <td>0.965531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.327600</td>\n",
       "      <td>0.408159</td>\n",
       "      <td>0.964989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.315500</td>\n",
       "      <td>0.402013</td>\n",
       "      <td>0.966291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.336800</td>\n",
       "      <td>0.373858</td>\n",
       "      <td>0.969494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.319100</td>\n",
       "      <td>0.354243</td>\n",
       "      <td>0.970776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.318000</td>\n",
       "      <td>0.370357</td>\n",
       "      <td>0.969302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.316000</td>\n",
       "      <td>0.350591</td>\n",
       "      <td>0.970995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.264500</td>\n",
       "      <td>0.334475</td>\n",
       "      <td>0.973248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.267800</td>\n",
       "      <td>0.325140</td>\n",
       "      <td>0.974838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.269500</td>\n",
       "      <td>0.308688</td>\n",
       "      <td>0.974992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.288600</td>\n",
       "      <td>0.289676</td>\n",
       "      <td>0.977032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.192400</td>\n",
       "      <td>0.279666</td>\n",
       "      <td>0.978799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.169400</td>\n",
       "      <td>0.301271</td>\n",
       "      <td>0.977422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.182000</td>\n",
       "      <td>0.281189</td>\n",
       "      <td>0.979179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.178000</td>\n",
       "      <td>0.273145</td>\n",
       "      <td>0.979447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.162300</td>\n",
       "      <td>0.271591</td>\n",
       "      <td>0.979788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.168600</td>\n",
       "      <td>0.254969</td>\n",
       "      <td>0.980780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.154600</td>\n",
       "      <td>0.280250</td>\n",
       "      <td>0.979317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.182800</td>\n",
       "      <td>0.256172</td>\n",
       "      <td>0.980650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.148400</td>\n",
       "      <td>0.256122</td>\n",
       "      <td>0.981014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-10\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-20\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-30\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_0\\checkpoint-10] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-40\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_0\\checkpoint-20] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-50\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_0\\checkpoint-30] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-60\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_0\\checkpoint-40] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-70\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_0\\checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-80\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_0\\checkpoint-60] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-90\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_0\\checkpoint-70] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-100\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_0\\checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-110\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_0\\checkpoint-90] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-120\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_0\\checkpoint-100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-130\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_0\\checkpoint-110] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-140\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_0\\checkpoint-120] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-150\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_0\\checkpoint-130] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-160\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_0\\checkpoint-140] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-170\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_0\\checkpoint-150] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-180\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_0\\checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-190\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_0\\checkpoint-170] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-200\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_0\\checkpoint-180] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-210\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_0\\checkpoint-190] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-220\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_0\\checkpoint-200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-230\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_0\\checkpoint-210] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-240\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_0\\checkpoint-220] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-250\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_0\\checkpoint-230] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-260\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_0\\checkpoint-250] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-270\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_0\\checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-280\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_0\\checkpoint-260] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-290\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_0\\checkpoint-270] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-300\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_0\\checkpoint-280] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-310\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_0\\checkpoint-290] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-320\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_0\\checkpoint-300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-330\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_0\\checkpoint-310] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-340\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_0\\checkpoint-320] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-350\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_0\\checkpoint-330] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-360\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_0\\checkpoint-340] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-370\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_0\\checkpoint-350] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-380\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_0\\checkpoint-360] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-390\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_0\\checkpoint-370] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-400\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_0\\checkpoint-380] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-410\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_0\\checkpoint-390] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-420\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_0\\checkpoint-400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-430\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_0\\checkpoint-410] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-440\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_0\\checkpoint-420] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-450\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_0\\checkpoint-430] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-460\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_0\\checkpoint-440] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-470\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_0\\checkpoint-450] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-480\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_0\\checkpoint-460] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-490\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_0\\checkpoint-480] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_0\\checkpoint-470] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-510\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_0\\checkpoint-490] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-520\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_0\\checkpoint-500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-530\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_0\\checkpoint-510] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-540\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_0\\checkpoint-530] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_0\\checkpoint-550\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_0\\checkpoint-540] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from fold_0\\checkpoint-520 (score: 0.2549688518047333).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file pytorch_model.bin from cache at C:\\Users\\halo/.cache\\huggingface\\hub\\models--monologg--kobigbird-bert-base\\snapshots\\ceacda477e20abef2c929adfa4a07c6f811323be\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at monologg/kobigbird-bert-base were not used when initializing BigBirdModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BigBirdModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BigBirdModel were initialized from the model checkpoint at monologg/kobigbird-bert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BigBirdModel for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using cuda_amp half precision backend\n",
      "C:\\Users\\halo\\miniconda3\\envs\\ml\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 39228\n",
      "  Num Epochs = 1000\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total optimization steps = 153000\n",
      "  Number of trainable parameters = 113763084\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='580' max='153000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   580/153000 1:56:03 < 510:05:32, 0.08 it/s, Epoch 3/1000]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1-sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.401900</td>\n",
       "      <td>1.883456</td>\n",
       "      <td>0.802804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.603000</td>\n",
       "      <td>1.469941</td>\n",
       "      <td>0.839424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.315300</td>\n",
       "      <td>1.203373</td>\n",
       "      <td>0.885188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.226500</td>\n",
       "      <td>1.054349</td>\n",
       "      <td>0.896700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.091400</td>\n",
       "      <td>1.006316</td>\n",
       "      <td>0.894221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.971300</td>\n",
       "      <td>0.914083</td>\n",
       "      <td>0.917118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.973500</td>\n",
       "      <td>0.856218</td>\n",
       "      <td>0.918015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.883500</td>\n",
       "      <td>0.824235</td>\n",
       "      <td>0.922868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.806300</td>\n",
       "      <td>0.783695</td>\n",
       "      <td>0.927171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.832900</td>\n",
       "      <td>0.783492</td>\n",
       "      <td>0.928489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.800600</td>\n",
       "      <td>0.762964</td>\n",
       "      <td>0.930255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.794400</td>\n",
       "      <td>0.735332</td>\n",
       "      <td>0.930954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.781500</td>\n",
       "      <td>0.733320</td>\n",
       "      <td>0.933141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.742600</td>\n",
       "      <td>0.751838</td>\n",
       "      <td>0.930701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.772500</td>\n",
       "      <td>0.710776</td>\n",
       "      <td>0.934669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.704300</td>\n",
       "      <td>0.689639</td>\n",
       "      <td>0.936257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.656700</td>\n",
       "      <td>0.703302</td>\n",
       "      <td>0.937375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.620400</td>\n",
       "      <td>0.673767</td>\n",
       "      <td>0.938363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.613700</td>\n",
       "      <td>0.644965</td>\n",
       "      <td>0.940702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.604800</td>\n",
       "      <td>0.629437</td>\n",
       "      <td>0.943116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.583600</td>\n",
       "      <td>0.615655</td>\n",
       "      <td>0.944971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.563000</td>\n",
       "      <td>0.616032</td>\n",
       "      <td>0.943491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.581100</td>\n",
       "      <td>0.599219</td>\n",
       "      <td>0.944676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.595900</td>\n",
       "      <td>0.608284</td>\n",
       "      <td>0.945996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.594000</td>\n",
       "      <td>0.543456</td>\n",
       "      <td>0.950377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.510100</td>\n",
       "      <td>0.545408</td>\n",
       "      <td>0.952716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.545500</td>\n",
       "      <td>0.506058</td>\n",
       "      <td>0.955557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.528700</td>\n",
       "      <td>0.505862</td>\n",
       "      <td>0.956200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.492900</td>\n",
       "      <td>0.487212</td>\n",
       "      <td>0.957698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.472300</td>\n",
       "      <td>0.959460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.435500</td>\n",
       "      <td>0.473785</td>\n",
       "      <td>0.960190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.378300</td>\n",
       "      <td>0.467192</td>\n",
       "      <td>0.959478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.353400</td>\n",
       "      <td>0.443383</td>\n",
       "      <td>0.963260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.340300</td>\n",
       "      <td>0.416068</td>\n",
       "      <td>0.963332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.325700</td>\n",
       "      <td>0.423013</td>\n",
       "      <td>0.964990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.327300</td>\n",
       "      <td>0.394837</td>\n",
       "      <td>0.967105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.318500</td>\n",
       "      <td>0.387066</td>\n",
       "      <td>0.968279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.322400</td>\n",
       "      <td>0.384802</td>\n",
       "      <td>0.968862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.299700</td>\n",
       "      <td>0.377802</td>\n",
       "      <td>0.969107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.333700</td>\n",
       "      <td>0.369051</td>\n",
       "      <td>0.970073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.284300</td>\n",
       "      <td>0.346973</td>\n",
       "      <td>0.972184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.314700</td>\n",
       "      <td>0.329603</td>\n",
       "      <td>0.973289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.279300</td>\n",
       "      <td>0.316852</td>\n",
       "      <td>0.975548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.280800</td>\n",
       "      <td>0.319522</td>\n",
       "      <td>0.975416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.277400</td>\n",
       "      <td>0.274609</td>\n",
       "      <td>0.978598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.277717</td>\n",
       "      <td>0.978292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.188100</td>\n",
       "      <td>0.282027</td>\n",
       "      <td>0.978011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.205000</td>\n",
       "      <td>0.264422</td>\n",
       "      <td>0.978601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.192100</td>\n",
       "      <td>0.256334</td>\n",
       "      <td>0.980345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.167400</td>\n",
       "      <td>0.265372</td>\n",
       "      <td>0.979667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.198100</td>\n",
       "      <td>0.273639</td>\n",
       "      <td>0.978828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.186200</td>\n",
       "      <td>0.235646</td>\n",
       "      <td>0.981609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.162600</td>\n",
       "      <td>0.243511</td>\n",
       "      <td>0.981499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.177900</td>\n",
       "      <td>0.246977</td>\n",
       "      <td>0.980231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.164500</td>\n",
       "      <td>0.223586</td>\n",
       "      <td>0.983200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.153900</td>\n",
       "      <td>0.226323</td>\n",
       "      <td>0.982902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.164700</td>\n",
       "      <td>0.237802</td>\n",
       "      <td>0.982208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.146500</td>\n",
       "      <td>0.225556</td>\n",
       "      <td>0.983121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-10\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-20\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-30\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-10] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-40\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-20] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-50\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-30] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-60\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-40] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-70\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-80\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-60] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-90\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-70] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-100\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-110\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-90] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-120\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-130\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-110] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-140\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-120] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-150\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-130] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-160\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-140] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-170\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-150] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-180\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-190\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-170] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-200\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-180] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-210\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-190] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-220\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-230\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-210] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-240\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-220] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-250\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-230] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-260\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-270\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-250] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-280\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-260] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-290\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-270] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-300\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-280] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-310\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-290] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-320\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-330\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-310] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-340\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-320] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-350\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-330] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-360\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-340] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-370\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-350] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-380\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-360] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-390\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-370] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-400\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-380] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-410\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-390] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-420\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-430\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-410] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-440\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-420] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-450\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-430] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-460\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-440] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-470\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-460] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-480\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-450] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-490\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-470] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-480] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-510\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-520\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-490] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-530\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-510] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-540\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-530] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-550\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-520] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-560\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-540] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-570\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-560] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_1\\checkpoint-580\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_1\\checkpoint-570] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from fold_1\\checkpoint-550 (score: 0.22358621656894684).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file pytorch_model.bin from cache at C:\\Users\\halo/.cache\\huggingface\\hub\\models--monologg--kobigbird-bert-base\\snapshots\\ceacda477e20abef2c929adfa4a07c6f811323be\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at monologg/kobigbird-bert-base were not used when initializing BigBirdModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BigBirdModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BigBirdModel were initialized from the model checkpoint at monologg/kobigbird-bert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BigBirdModel for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using cuda_amp half precision backend\n",
      "C:\\Users\\halo\\miniconda3\\envs\\ml\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 39228\n",
      "  Num Epochs = 1000\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total optimization steps = 153000\n",
      "  Number of trainable parameters = 113763084\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='450' max='153000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   450/153000 1:17:59 < 442:36:25, 0.10 it/s, Epoch 2/1000]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1-sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.275900</td>\n",
       "      <td>1.567475</td>\n",
       "      <td>0.830472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.471100</td>\n",
       "      <td>1.363114</td>\n",
       "      <td>0.853140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.260500</td>\n",
       "      <td>1.156775</td>\n",
       "      <td>0.888822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.118700</td>\n",
       "      <td>1.029044</td>\n",
       "      <td>0.898259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.022700</td>\n",
       "      <td>0.916221</td>\n",
       "      <td>0.915706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.920600</td>\n",
       "      <td>0.867384</td>\n",
       "      <td>0.919098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.914500</td>\n",
       "      <td>0.839309</td>\n",
       "      <td>0.918945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.819600</td>\n",
       "      <td>0.846231</td>\n",
       "      <td>0.920309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.812400</td>\n",
       "      <td>0.808943</td>\n",
       "      <td>0.922998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.858700</td>\n",
       "      <td>0.782908</td>\n",
       "      <td>0.927845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.833700</td>\n",
       "      <td>0.757856</td>\n",
       "      <td>0.927703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.811000</td>\n",
       "      <td>0.752281</td>\n",
       "      <td>0.931248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.759400</td>\n",
       "      <td>0.726697</td>\n",
       "      <td>0.931222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.721600</td>\n",
       "      <td>0.706374</td>\n",
       "      <td>0.934506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.722700</td>\n",
       "      <td>0.729344</td>\n",
       "      <td>0.929997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.697500</td>\n",
       "      <td>0.702161</td>\n",
       "      <td>0.934991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.595000</td>\n",
       "      <td>0.693662</td>\n",
       "      <td>0.937951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.612900</td>\n",
       "      <td>0.674829</td>\n",
       "      <td>0.937859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.637600</td>\n",
       "      <td>0.639369</td>\n",
       "      <td>0.941491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.642800</td>\n",
       "      <td>0.647365</td>\n",
       "      <td>0.941002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.589500</td>\n",
       "      <td>0.633698</td>\n",
       "      <td>0.943633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.568800</td>\n",
       "      <td>0.611073</td>\n",
       "      <td>0.945047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.573900</td>\n",
       "      <td>0.589282</td>\n",
       "      <td>0.947358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.535500</td>\n",
       "      <td>0.579264</td>\n",
       "      <td>0.948260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.548000</td>\n",
       "      <td>0.558059</td>\n",
       "      <td>0.950310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.512500</td>\n",
       "      <td>0.576932</td>\n",
       "      <td>0.951003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.530400</td>\n",
       "      <td>0.530692</td>\n",
       "      <td>0.952841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.516500</td>\n",
       "      <td>0.519001</td>\n",
       "      <td>0.954343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.461200</td>\n",
       "      <td>0.496771</td>\n",
       "      <td>0.958330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.474700</td>\n",
       "      <td>0.470074</td>\n",
       "      <td>0.959516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.420200</td>\n",
       "      <td>0.459811</td>\n",
       "      <td>0.961696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.351300</td>\n",
       "      <td>0.458352</td>\n",
       "      <td>0.961604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.337400</td>\n",
       "      <td>0.457772</td>\n",
       "      <td>0.963047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.334800</td>\n",
       "      <td>0.416400</td>\n",
       "      <td>0.965858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.322500</td>\n",
       "      <td>0.431239</td>\n",
       "      <td>0.964834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.287900</td>\n",
       "      <td>0.420083</td>\n",
       "      <td>0.965959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.322800</td>\n",
       "      <td>0.406236</td>\n",
       "      <td>0.966630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.310600</td>\n",
       "      <td>0.378986</td>\n",
       "      <td>0.969445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.337500</td>\n",
       "      <td>0.415344</td>\n",
       "      <td>0.966344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.317700</td>\n",
       "      <td>0.364563</td>\n",
       "      <td>0.970222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.282300</td>\n",
       "      <td>0.360099</td>\n",
       "      <td>0.970452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.295400</td>\n",
       "      <td>0.343658</td>\n",
       "      <td>0.972463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.267600</td>\n",
       "      <td>0.363340</td>\n",
       "      <td>0.972138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.297000</td>\n",
       "      <td>0.371305</td>\n",
       "      <td>0.970680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.294600</td>\n",
       "      <td>0.357181</td>\n",
       "      <td>0.971984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_2\\checkpoint-10\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_2\\checkpoint-20\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_2\\checkpoint-30\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_2\\checkpoint-10] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_2\\checkpoint-40\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_2\\checkpoint-20] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_2\\checkpoint-50\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_2\\checkpoint-30] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_2\\checkpoint-60\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_2\\checkpoint-40] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_2\\checkpoint-70\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_2\\checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_2\\checkpoint-80\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_2\\checkpoint-60] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_2\\checkpoint-90\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_2\\checkpoint-70] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_2\\checkpoint-100\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_2\\checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_2\\checkpoint-110\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_2\\checkpoint-90] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_2\\checkpoint-120\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_2\\checkpoint-100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_2\\checkpoint-130\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_2\\checkpoint-110] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_2\\checkpoint-140\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_2\\checkpoint-120] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_2\\checkpoint-150\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_2\\checkpoint-130] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_2\\checkpoint-160\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_2\\checkpoint-140] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_2\\checkpoint-170\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_2\\checkpoint-150] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_2\\checkpoint-180\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_2\\checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_2\\checkpoint-190\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_2\\checkpoint-170] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_2\\checkpoint-200\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_2\\checkpoint-180] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_2\\checkpoint-210\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_2\\checkpoint-190] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_2\\checkpoint-220\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_2\\checkpoint-200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_2\\checkpoint-230\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_2\\checkpoint-210] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_2\\checkpoint-240\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_2\\checkpoint-220] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_2\\checkpoint-250\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_2\\checkpoint-230] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_2\\checkpoint-260\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_2\\checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_2\\checkpoint-270\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_2\\checkpoint-250] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_2\\checkpoint-280\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_2\\checkpoint-260] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_2\\checkpoint-290\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_2\\checkpoint-270] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_2\\checkpoint-300\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_2\\checkpoint-280] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_2\\checkpoint-310\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_2\\checkpoint-290] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_2\\checkpoint-320\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_2\\checkpoint-300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_2\\checkpoint-330\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_2\\checkpoint-310] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_2\\checkpoint-340\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_2\\checkpoint-320] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_2\\checkpoint-350\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_2\\checkpoint-330] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_2\\checkpoint-360\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_2\\checkpoint-350] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_2\\checkpoint-370\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_2\\checkpoint-340] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_2\\checkpoint-380\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_2\\checkpoint-360] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_2\\checkpoint-390\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_2\\checkpoint-370] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_2\\checkpoint-400\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_2\\checkpoint-380] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_2\\checkpoint-410\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_2\\checkpoint-390] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_2\\checkpoint-420\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_2\\checkpoint-400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_2\\checkpoint-430\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_2\\checkpoint-410] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_2\\checkpoint-440\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_2\\checkpoint-430] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_2\\checkpoint-450\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_2\\checkpoint-440] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from fold_2\\checkpoint-420 (score: 0.34365785121917725).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file pytorch_model.bin from cache at C:\\Users\\halo/.cache\\huggingface\\hub\\models--monologg--kobigbird-bert-base\\snapshots\\ceacda477e20abef2c929adfa4a07c6f811323be\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at monologg/kobigbird-bert-base were not used when initializing BigBirdModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BigBirdModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BigBirdModel were initialized from the model checkpoint at monologg/kobigbird-bert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BigBirdModel for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using cuda_amp half precision backend\n",
      "C:\\Users\\halo\\miniconda3\\envs\\ml\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 39228\n",
      "  Num Epochs = 1000\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total optimization steps = 153000\n",
      "  Number of trainable parameters = 113763084\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='570' max='153000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   570/153000 1:38:43 < 441:36:14, 0.10 it/s, Epoch 3/1000]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1-sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.336300</td>\n",
       "      <td>1.543227</td>\n",
       "      <td>0.830453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.480100</td>\n",
       "      <td>1.256288</td>\n",
       "      <td>0.856805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.252000</td>\n",
       "      <td>1.136982</td>\n",
       "      <td>0.877436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.134100</td>\n",
       "      <td>0.973452</td>\n",
       "      <td>0.903215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.000100</td>\n",
       "      <td>0.931598</td>\n",
       "      <td>0.915122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.918600</td>\n",
       "      <td>0.876954</td>\n",
       "      <td>0.919029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.873100</td>\n",
       "      <td>0.873518</td>\n",
       "      <td>0.920630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.813800</td>\n",
       "      <td>0.807402</td>\n",
       "      <td>0.924356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.836500</td>\n",
       "      <td>0.804482</td>\n",
       "      <td>0.922495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.761300</td>\n",
       "      <td>0.778725</td>\n",
       "      <td>0.922578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.760100</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.924618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.780500</td>\n",
       "      <td>0.738034</td>\n",
       "      <td>0.929512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.774200</td>\n",
       "      <td>0.721886</td>\n",
       "      <td>0.931603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.737700</td>\n",
       "      <td>0.711184</td>\n",
       "      <td>0.932882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.672800</td>\n",
       "      <td>0.699523</td>\n",
       "      <td>0.935266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.640500</td>\n",
       "      <td>0.707621</td>\n",
       "      <td>0.933216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.637000</td>\n",
       "      <td>0.663663</td>\n",
       "      <td>0.939524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.595300</td>\n",
       "      <td>0.674654</td>\n",
       "      <td>0.938418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.602800</td>\n",
       "      <td>0.629571</td>\n",
       "      <td>0.943011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.612900</td>\n",
       "      <td>0.634052</td>\n",
       "      <td>0.942170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.587000</td>\n",
       "      <td>0.631781</td>\n",
       "      <td>0.943491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.602000</td>\n",
       "      <td>0.607715</td>\n",
       "      <td>0.945012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.528400</td>\n",
       "      <td>0.589189</td>\n",
       "      <td>0.946911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.570600</td>\n",
       "      <td>0.573702</td>\n",
       "      <td>0.949104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.559900</td>\n",
       "      <td>0.532138</td>\n",
       "      <td>0.952237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.539800</td>\n",
       "      <td>0.534192</td>\n",
       "      <td>0.951717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.523000</td>\n",
       "      <td>0.529006</td>\n",
       "      <td>0.953524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.544800</td>\n",
       "      <td>0.505028</td>\n",
       "      <td>0.955444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.521900</td>\n",
       "      <td>0.486234</td>\n",
       "      <td>0.956640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.495800</td>\n",
       "      <td>0.486463</td>\n",
       "      <td>0.957284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.451700</td>\n",
       "      <td>0.508469</td>\n",
       "      <td>0.956021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.344000</td>\n",
       "      <td>0.464232</td>\n",
       "      <td>0.961366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.348500</td>\n",
       "      <td>0.461262</td>\n",
       "      <td>0.960667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.325600</td>\n",
       "      <td>0.426388</td>\n",
       "      <td>0.963003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.340300</td>\n",
       "      <td>0.422412</td>\n",
       "      <td>0.964416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.324400</td>\n",
       "      <td>0.419259</td>\n",
       "      <td>0.964299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.328200</td>\n",
       "      <td>0.396421</td>\n",
       "      <td>0.967489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.336900</td>\n",
       "      <td>0.382342</td>\n",
       "      <td>0.967241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.329900</td>\n",
       "      <td>0.370076</td>\n",
       "      <td>0.968870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.306000</td>\n",
       "      <td>0.364832</td>\n",
       "      <td>0.969956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.294500</td>\n",
       "      <td>0.361857</td>\n",
       "      <td>0.970265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.286100</td>\n",
       "      <td>0.330485</td>\n",
       "      <td>0.973337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.265200</td>\n",
       "      <td>0.318629</td>\n",
       "      <td>0.974501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.272800</td>\n",
       "      <td>0.376355</td>\n",
       "      <td>0.970286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.277400</td>\n",
       "      <td>0.315760</td>\n",
       "      <td>0.974518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.290700</td>\n",
       "      <td>0.295740</td>\n",
       "      <td>0.975755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.183800</td>\n",
       "      <td>0.299355</td>\n",
       "      <td>0.974631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.178700</td>\n",
       "      <td>0.303152</td>\n",
       "      <td>0.976602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.172200</td>\n",
       "      <td>0.291514</td>\n",
       "      <td>0.976704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.188200</td>\n",
       "      <td>0.323035</td>\n",
       "      <td>0.975294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.215600</td>\n",
       "      <td>0.298123</td>\n",
       "      <td>0.976184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.196100</td>\n",
       "      <td>0.272130</td>\n",
       "      <td>0.980101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.192000</td>\n",
       "      <td>0.278994</td>\n",
       "      <td>0.980401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.184400</td>\n",
       "      <td>0.247365</td>\n",
       "      <td>0.980322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.168500</td>\n",
       "      <td>0.249948</td>\n",
       "      <td>0.980648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.161600</td>\n",
       "      <td>0.255019</td>\n",
       "      <td>0.981408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.173000</td>\n",
       "      <td>0.254525</td>\n",
       "      <td>0.980746</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-10\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-20\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-30\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-10] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-40\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-20] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-50\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-30] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-60\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-40] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-70\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-80\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-60] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-90\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-70] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-100\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-110\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-90] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-120\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-130\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-110] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-140\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-120] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-150\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-130] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-160\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-140] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-170\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-150] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-180\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-190\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-170] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-200\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-180] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-210\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-220\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-190] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-230\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-210] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-240\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-220] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-250\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-230] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-260\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-270\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-250] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-280\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-260] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-290\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-270] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-300\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-280] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-310\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-320\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-290] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-330\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-310] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-340\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-320] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-350\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-330] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-360\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-340] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-370\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-350] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-380\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-360] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-390\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-370] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-400\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-380] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-410\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-390] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-420\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-430\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-410] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-440\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-420] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-450\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-430] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-460\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-440] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-470\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-450] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-480\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-470] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-490\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-460] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-480] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-510\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-520\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-490] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-530\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-510] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-540\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-520] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-550\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-530] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-560\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-550] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_3\\checkpoint-570\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_3\\checkpoint-560] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from fold_3\\checkpoint-540 (score: 0.24736535549163818).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file pytorch_model.bin from cache at C:\\Users\\halo/.cache\\huggingface\\hub\\models--monologg--kobigbird-bert-base\\snapshots\\ceacda477e20abef2c929adfa4a07c6f811323be\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at monologg/kobigbird-bert-base were not used when initializing BigBirdModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BigBirdModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BigBirdModel were initialized from the model checkpoint at monologg/kobigbird-bert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BigBirdModel for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using cuda_amp half precision backend\n",
      "C:\\Users\\halo\\miniconda3\\envs\\ml\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 39228\n",
      "  Num Epochs = 1000\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total optimization steps = 153000\n",
      "  Number of trainable parameters = 113763084\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='680' max='153000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   680/153000 1:57:20 < 439:21:30, 0.10 it/s, Epoch 4/1000]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1-sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.260300</td>\n",
       "      <td>1.630957</td>\n",
       "      <td>0.818825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.485200</td>\n",
       "      <td>1.327509</td>\n",
       "      <td>0.858768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.277200</td>\n",
       "      <td>1.126277</td>\n",
       "      <td>0.887199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.123500</td>\n",
       "      <td>1.051113</td>\n",
       "      <td>0.889038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.087000</td>\n",
       "      <td>0.993670</td>\n",
       "      <td>0.909648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.000100</td>\n",
       "      <td>0.899894</td>\n",
       "      <td>0.915662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.936700</td>\n",
       "      <td>0.874281</td>\n",
       "      <td>0.920101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.911200</td>\n",
       "      <td>0.821416</td>\n",
       "      <td>0.920061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.835500</td>\n",
       "      <td>0.796901</td>\n",
       "      <td>0.921645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.841100</td>\n",
       "      <td>0.777946</td>\n",
       "      <td>0.926670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.804000</td>\n",
       "      <td>0.781585</td>\n",
       "      <td>0.923867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.791300</td>\n",
       "      <td>0.745562</td>\n",
       "      <td>0.928987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.815800</td>\n",
       "      <td>0.774703</td>\n",
       "      <td>0.927126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.787200</td>\n",
       "      <td>0.721159</td>\n",
       "      <td>0.934767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.752900</td>\n",
       "      <td>0.690804</td>\n",
       "      <td>0.931600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.696200</td>\n",
       "      <td>0.682382</td>\n",
       "      <td>0.935595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.653800</td>\n",
       "      <td>0.653845</td>\n",
       "      <td>0.938005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.630500</td>\n",
       "      <td>0.677808</td>\n",
       "      <td>0.937484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.589100</td>\n",
       "      <td>0.649612</td>\n",
       "      <td>0.937812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.604600</td>\n",
       "      <td>0.630563</td>\n",
       "      <td>0.942164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.594100</td>\n",
       "      <td>0.599063</td>\n",
       "      <td>0.945546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.568600</td>\n",
       "      <td>0.612833</td>\n",
       "      <td>0.944083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.587200</td>\n",
       "      <td>0.573038</td>\n",
       "      <td>0.947327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.551400</td>\n",
       "      <td>0.548037</td>\n",
       "      <td>0.950746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.509100</td>\n",
       "      <td>0.531814</td>\n",
       "      <td>0.952754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.497600</td>\n",
       "      <td>0.523001</td>\n",
       "      <td>0.953143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.523800</td>\n",
       "      <td>0.512967</td>\n",
       "      <td>0.955158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.527200</td>\n",
       "      <td>0.514099</td>\n",
       "      <td>0.954253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.508300</td>\n",
       "      <td>0.488594</td>\n",
       "      <td>0.955974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.523100</td>\n",
       "      <td>0.484855</td>\n",
       "      <td>0.957582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.460800</td>\n",
       "      <td>0.479563</td>\n",
       "      <td>0.957406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.394500</td>\n",
       "      <td>0.440098</td>\n",
       "      <td>0.961737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.317800</td>\n",
       "      <td>0.431786</td>\n",
       "      <td>0.962134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.354000</td>\n",
       "      <td>0.416839</td>\n",
       "      <td>0.964633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.332200</td>\n",
       "      <td>0.400130</td>\n",
       "      <td>0.966336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.303700</td>\n",
       "      <td>0.422667</td>\n",
       "      <td>0.964503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.333000</td>\n",
       "      <td>0.394852</td>\n",
       "      <td>0.967504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.331700</td>\n",
       "      <td>0.394040</td>\n",
       "      <td>0.966236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.322100</td>\n",
       "      <td>0.364192</td>\n",
       "      <td>0.968979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.304500</td>\n",
       "      <td>0.362525</td>\n",
       "      <td>0.968326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.307000</td>\n",
       "      <td>0.349942</td>\n",
       "      <td>0.971815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.325500</td>\n",
       "      <td>0.420330</td>\n",
       "      <td>0.965798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>0.333373</td>\n",
       "      <td>0.974096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.237900</td>\n",
       "      <td>0.320862</td>\n",
       "      <td>0.974161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.275900</td>\n",
       "      <td>0.328294</td>\n",
       "      <td>0.973588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.299400</td>\n",
       "      <td>0.291783</td>\n",
       "      <td>0.976292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.203200</td>\n",
       "      <td>0.301838</td>\n",
       "      <td>0.975496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.185300</td>\n",
       "      <td>0.323998</td>\n",
       "      <td>0.974065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.196100</td>\n",
       "      <td>0.280445</td>\n",
       "      <td>0.977131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.189100</td>\n",
       "      <td>0.299668</td>\n",
       "      <td>0.976767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.187300</td>\n",
       "      <td>0.265768</td>\n",
       "      <td>0.978878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.159000</td>\n",
       "      <td>0.255295</td>\n",
       "      <td>0.979953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.190600</td>\n",
       "      <td>0.254856</td>\n",
       "      <td>0.980387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.192200</td>\n",
       "      <td>0.260475</td>\n",
       "      <td>0.979515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.171400</td>\n",
       "      <td>0.244453</td>\n",
       "      <td>0.981006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.168600</td>\n",
       "      <td>0.264149</td>\n",
       "      <td>0.979194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.173700</td>\n",
       "      <td>0.255122</td>\n",
       "      <td>0.980838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.158800</td>\n",
       "      <td>0.230360</td>\n",
       "      <td>0.981962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.168400</td>\n",
       "      <td>0.232664</td>\n",
       "      <td>0.982913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.178300</td>\n",
       "      <td>0.242654</td>\n",
       "      <td>0.981445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.170100</td>\n",
       "      <td>0.228116</td>\n",
       "      <td>0.983169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.143700</td>\n",
       "      <td>0.234992</td>\n",
       "      <td>0.982441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.107000</td>\n",
       "      <td>0.220830</td>\n",
       "      <td>0.983905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.128200</td>\n",
       "      <td>0.192219</td>\n",
       "      <td>0.986709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.106100</td>\n",
       "      <td>0.185595</td>\n",
       "      <td>0.987119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.106100</td>\n",
       "      <td>0.197066</td>\n",
       "      <td>0.985104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.114800</td>\n",
       "      <td>0.187062</td>\n",
       "      <td>0.986752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.108100</td>\n",
       "      <td>0.203382</td>\n",
       "      <td>0.985259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-10\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-20\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-30\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-10] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-40\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-20] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-50\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-30] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-60\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-40] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-70\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-50] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-80\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-60] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-90\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-70] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-100\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-80] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-110\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-90] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-120\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-130\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-110] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-140\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-120] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-150\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-130] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-160\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-140] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-170\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-150] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-180\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-160] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-190\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-170] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-200\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-180] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-210\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-190] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-220\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-230\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-210] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-240\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-220] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-250\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-230] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-260\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-240] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-270\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-250] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-280\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-260] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-290\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-270] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-300\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-280] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-310\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-290] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-320\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-330\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-310] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-340\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-320] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-350\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-330] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-360\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-340] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-370\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-350] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-380\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-360] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-390\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-370] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-400\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-380] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-410\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-390] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-420\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-430\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-410] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-440\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-420] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-450\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-430] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-460\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-440] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-470\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-450] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-480\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-470] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-490\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-460] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-480] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-510\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-490] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-520\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-530\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-510] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-540\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-520] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-550\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-530] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-560\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-540] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-570\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-560] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-580\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-550] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-590\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-570] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-600\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-590] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-610\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-580] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-620\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-630\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-610] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-640\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-620] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-650\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-630] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-660\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-640] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-670\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-660] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9807\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to fold_4\\checkpoint-680\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [fold_4\\checkpoint-670] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from fold_4\\checkpoint-650 (score: 0.185594841837883).\n"
     ]
    }
   ],
   "source": [
    "config=AutoConfig.from_pretrained(model_path)\n",
    "config._name_or_path = 'kr.kim'\n",
    "print(f'hidden_layers : {config.num_hidden_layers}')\n",
    "# config.num_hidden_layers = 17\n",
    "\n",
    "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "for i, (train_index, test_index) in enumerate(kf.split(train)):\n",
    "    print(f'Round {i}')\n",
    "    X_train, X_val = train.loc[train_index, :], train.loc[test_index, :]\n",
    "    train_labels, val_labels = encoding(X_train, X_val)\n",
    "    token_train, token_val = tokenizer(X_train.문장.tolist(), padding=True, truncation=True, max_length=length), tokenizer(X_val.문장.tolist(), padding=True, truncation=True, max_length=length)\n",
    "    train_dataset, val_dataset = CustomDataset(token_train, train_labels), CustomDataset(token_val, val_labels)\n",
    "    model = CustomModel()\n",
    "    model.to(device)\n",
    "    args = TrainingArguments(run_name = f'fold_{i}',\n",
    "                             output_dir= f\"fold_{i}\",                                   # 모델저장경로\n",
    "                             evaluation_strategy=\"steps\",                           # 모델의 평가를 언제 진행할지\n",
    "                             eval_steps=10,                                        # 500 스텝 마다 모델 평가\n",
    "                             logging_steps=10,\n",
    "                             per_device_train_batch_size=batch,                        # GPU에 학습데이터를 몇개씩 올려서 학습할지\n",
    "                             per_device_eval_batch_size=batch,                         # GPU에 학습데이터를 몇개씩 올려서 평가할지\n",
    "                             gradient_accumulation_steps=16,\n",
    "                             num_train_epochs=epoch,                                  # 전체 학습 진행 횟수\n",
    "                             learning_rate=lr,                                      # 학습률 정의 \n",
    "                             seed=42,                                                 \n",
    "                             load_best_model_at_end=True,                           # 평가기준 스코어가 좋은 모델만 저장할지 여부\n",
    "                             fp16=True,\n",
    "                             do_train=True,\n",
    "                             do_eval=True,\n",
    "                             save_steps=10,\n",
    "                             save_total_limit = 2,                                  # 저장할 모델의 갯수\n",
    "                             # metric_for_best_model\n",
    "                             # greater_is_better = True,\n",
    "    )\n",
    "    trainer = CustomTrainer(model=model,\n",
    "                            args=args,\n",
    "                            train_dataset=train_dataset,                      # 학습데이터\n",
    "                            eval_dataset=val_dataset,                        # validation 데이터\n",
    "                            compute_metrics=compute_metrics,                       # 모델 평가 방식\n",
    "                            callbacks=[EarlyStoppingCallback(early_stopping_patience=stop)],)\n",
    "    trainer.train()\n",
    "    del model\n",
    "    del trainer\n",
    "    gc.collect() # python 자원 관리 \n",
    "    torch.cuda.empty_cache() # gpu 자원관리   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec38659-4ebc-4649-a747-b24b5270269c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38f059d9-e265-4f7c-8bba-a484968574ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recent_file(path):\n",
    "    file_name_and_time_lst = []\n",
    "    # 해당 경로에 있는 파일들의 생성시간을 함께 리스트로 넣어줌. \n",
    "    for f_name in os.listdir(f\"{path}\"):\n",
    "        written_time = os.path.getctime(f\"{path}/{f_name}\")\n",
    "        file_name_and_time_lst.append((f_name, written_time))\n",
    "    # 생성시간 역순으로 정렬하고, \n",
    "    sorted_file_lst = sorted(file_name_and_time_lst, key=lambda x: x[1], reverse=True)\n",
    "    # 가장 앞에 이는 놈을 넣어준다.\n",
    "    recent_file = sorted_file_lst[0]\n",
    "    recent_file_name = recent_file[0]\n",
    "    return f\"{path}/{recent_file_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "617bb5a0-0aad-4869-a10b-aba9acd9223c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at C:\\Users\\halo/.cache\\huggingface\\hub\\models--monologg--kobigbird-bert-base\\snapshots\\ceacda477e20abef2c929adfa4a07c6f811323be\\vocab.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\halo/.cache\\huggingface\\hub\\models--monologg--kobigbird-bert-base\\snapshots\\ceacda477e20abef2c929adfa4a07c6f811323be\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\halo/.cache\\huggingface\\hub\\models--monologg--kobigbird-bert-base\\snapshots\\ceacda477e20abef2c929adfa4a07c6f811323be\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\halo/.cache\\huggingface\\hub\\models--monologg--kobigbird-bert-base\\snapshots\\ceacda477e20abef2c929adfa4a07c6f811323be\\tokenizer_config.json\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\halo/.cache\\huggingface\\hub\\models--monologg--kobigbird-bert-base\\snapshots\\ceacda477e20abef2c929adfa4a07c6f811323be\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/kobigbird-bert-base were not used when initializing BigBirdModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BigBirdModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BigBirdModel were initialized from the model checkpoint at monologg/kobigbird-bert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BigBirdModel for predictions without further training.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 7090\n",
      "  Batch size = 512\n",
      "C:\\Users\\halo\\AppData\\Local\\Temp\\ipykernel_3536\\1912981868.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file pytorch_model.bin from cache at C:\\Users\\halo/.cache\\huggingface\\hub\\models--monologg--kobigbird-bert-base\\snapshots\\ceacda477e20abef2c929adfa4a07c6f811323be\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/kobigbird-bert-base were not used when initializing BigBirdModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BigBirdModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BigBirdModel were initialized from the model checkpoint at monologg/kobigbird-bert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BigBirdModel for predictions without further training.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 7090\n",
      "  Batch size = 512\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file pytorch_model.bin from cache at C:\\Users\\halo/.cache\\huggingface\\hub\\models--monologg--kobigbird-bert-base\\snapshots\\ceacda477e20abef2c929adfa4a07c6f811323be\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/kobigbird-bert-base were not used when initializing BigBirdModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BigBirdModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BigBirdModel were initialized from the model checkpoint at monologg/kobigbird-bert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BigBirdModel for predictions without further training.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 7090\n",
      "  Batch size = 512\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file pytorch_model.bin from cache at C:\\Users\\halo/.cache\\huggingface\\hub\\models--monologg--kobigbird-bert-base\\snapshots\\ceacda477e20abef2c929adfa4a07c6f811323be\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/kobigbird-bert-base were not used when initializing BigBirdModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BigBirdModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BigBirdModel were initialized from the model checkpoint at monologg/kobigbird-bert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BigBirdModel for predictions without further training.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 7090\n",
      "  Batch size = 512\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file pytorch_model.bin from cache at C:\\Users\\halo/.cache\\huggingface\\hub\\models--monologg--kobigbird-bert-base\\snapshots\\ceacda477e20abef2c929adfa4a07c6f811323be\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/kobigbird-bert-base were not used when initializing BigBirdModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BigBirdModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BigBirdModel were initialized from the model checkpoint at monologg/kobigbird-bert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BigBirdModel for predictions without further training.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 7090\n",
      "  Batch size = 512\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'int' and 'PredictionOutput'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m model\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m trainer\n\u001b[1;32m---> 34\u001b[0m test_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_results\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(test_results)\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'PredictionOutput'"
     ]
    }
   ],
   "source": [
    "gc.collect() # python 자원 관리 \n",
    "torch.cuda.empty_cache() # gpu 자원관리\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenized = tokenizer(test.문장.tolist(), padding=True, truncation=True, max_length=length, return_tensors=\"pt\")\n",
    "test_dataset = CustomDataset(tokenized, None)\n",
    "test_args = TrainingArguments(\n",
    "    output_dir = './',\n",
    "    do_train = False,\n",
    "    do_predict = True,\n",
    "    per_device_eval_batch_size = 512,   \n",
    "    dataloader_drop_last = False    \n",
    ")\n",
    "\n",
    "tmp = 0\n",
    "while os.path.isdir(f'fold_{tmp}'):\n",
    "    tmp += 1\n",
    "\n",
    "test_results = []\n",
    "for i in range(tmp):\n",
    "    print(f'Round {i}')\n",
    "    # model = AutoModel.from_pretrained(recent_file('custom_model'), config=config)\n",
    "    model = CustomModel().to(device)\n",
    "    model.load_state_dict(torch.load(f\"{recent_file(f'fold_{i}')}/pytorch_model.bin\"))\n",
    "    trainer = CustomTrainer(\n",
    "                  model = model, \n",
    "                  args = test_args, \n",
    "                  compute_metrics = compute_metrics)\n",
    "    test_results.append(trainer.predict(test_dataset))\n",
    "    gc.collect() # python 자원 관리 \n",
    "    torch.cuda.empty_cache() # gpu 자원관리\n",
    "    del model\n",
    "    del trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b9558dc5-3db7-4275-8d2c-18ba58783256",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test['유형'] = list(map(lambda x : 유형.inverse_transform([np.argmax(x)]), sum(list(map(lambda x: x.predictions[0], test_results)))/len(test_results)))\n",
    "test['극성'] = list(map(lambda x : 극성.inverse_transform([np.argmax(x)]), sum(list(map(lambda x: x.predictions[1], test_results)))/len(test_results)))\n",
    "test['시제'] = list(map(lambda x : 시제.inverse_transform([np.argmax(x)]), sum(list(map(lambda x: x.predictions[2], test_results)))/len(test_results)))\n",
    "test['확실성'] = list(map(lambda x : 확실성.inverse_transform([np.argmax(x)]), sum(list(map(lambda x: x.predictions[3], test_results)))/len(test_results)))\n",
    "\n",
    "test['유형'] = list(map(lambda x : x[0], test['유형']))\n",
    "test['극성'] = list(map(lambda x : x[0], test['극성']))\n",
    "test['시제'] = list(map(lambda x : x[0], test['시제']))\n",
    "test['확실성'] = list(map(lambda x : x[0], test['확실성']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "be6cee5e-145f-43c7-b1bf-bf966124577a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>문장</th>\n",
       "      <th>유형</th>\n",
       "      <th>극성</th>\n",
       "      <th>시제</th>\n",
       "      <th>확실성</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_0000</td>\n",
       "      <td>장욱진의 가족은 허물 없는 가족애를 처음 공개되는 정약용의 정효자전과 정부인전은 강...</td>\n",
       "      <td>사실형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>현재</td>\n",
       "      <td>확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_0001</td>\n",
       "      <td>조지 W 부시 버락 오바마 전 대통령도 전쟁 위험 때문에 버린 카드다</td>\n",
       "      <td>사실형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>현재</td>\n",
       "      <td>확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_0002</td>\n",
       "      <td>지난해 1분기 128억원이었던 영업이익이 올해 1분기 505억원으로 급증했다</td>\n",
       "      <td>사실형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>과거</td>\n",
       "      <td>확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_0003</td>\n",
       "      <td>수상 작가와 맺으려던 계약서 내용 가운데 일부가 독소 조항으로 해석돼 수정을 요청받...</td>\n",
       "      <td>사실형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>현재</td>\n",
       "      <td>확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_0004</td>\n",
       "      <td>결국 최근 KDB산업은행은 대규모 손실 위기에 닥친 에어부산에 140억원 금융지원을...</td>\n",
       "      <td>사실형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>과거</td>\n",
       "      <td>확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7085</th>\n",
       "      <td>TEST_7085</td>\n",
       "      <td>2020 세계국가편람 모바일 앱은 세계 216개국의 국가개황과 주요 경제지표 사회개...</td>\n",
       "      <td>사실형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>현재</td>\n",
       "      <td>확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7086</th>\n",
       "      <td>TEST_7086</td>\n",
       "      <td>탈세계화 징후들이 반갑지 않은 이유다</td>\n",
       "      <td>추론형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>현재</td>\n",
       "      <td>확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7087</th>\n",
       "      <td>TEST_7087</td>\n",
       "      <td>틱톡은 6월 인터넷 안전의 달을 맞아 올바른 개인정보 보호 관리 방법 앱 내 유용한...</td>\n",
       "      <td>사실형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>현재</td>\n",
       "      <td>확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7088</th>\n",
       "      <td>TEST_7088</td>\n",
       "      <td>만약 3개월 간 채굴자들의 투표를 거쳐 23 이상의 해시파워가 채굴세 도입에 찬성한...</td>\n",
       "      <td>추론형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>미래</td>\n",
       "      <td>확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7089</th>\n",
       "      <td>TEST_7089</td>\n",
       "      <td>아버지 홍언필이 인기척에 깨 그 광경을 지켜봤다</td>\n",
       "      <td>사실형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>과거</td>\n",
       "      <td>확실</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7090 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID                                                 문장   유형  극성  \\\n",
       "0     TEST_0000  장욱진의 가족은 허물 없는 가족애를 처음 공개되는 정약용의 정효자전과 정부인전은 강...  사실형  긍정   \n",
       "1     TEST_0001             조지 W 부시 버락 오바마 전 대통령도 전쟁 위험 때문에 버린 카드다  사실형  긍정   \n",
       "2     TEST_0002         지난해 1분기 128억원이었던 영업이익이 올해 1분기 505억원으로 급증했다  사실형  긍정   \n",
       "3     TEST_0003  수상 작가와 맺으려던 계약서 내용 가운데 일부가 독소 조항으로 해석돼 수정을 요청받...  사실형  긍정   \n",
       "4     TEST_0004  결국 최근 KDB산업은행은 대규모 손실 위기에 닥친 에어부산에 140억원 금융지원을...  사실형  긍정   \n",
       "...         ...                                                ...  ...  ..   \n",
       "7085  TEST_7085  2020 세계국가편람 모바일 앱은 세계 216개국의 국가개황과 주요 경제지표 사회개...  사실형  긍정   \n",
       "7086  TEST_7086                               탈세계화 징후들이 반갑지 않은 이유다  추론형  긍정   \n",
       "7087  TEST_7087  틱톡은 6월 인터넷 안전의 달을 맞아 올바른 개인정보 보호 관리 방법 앱 내 유용한...  사실형  긍정   \n",
       "7088  TEST_7088  만약 3개월 간 채굴자들의 투표를 거쳐 23 이상의 해시파워가 채굴세 도입에 찬성한...  추론형  긍정   \n",
       "7089  TEST_7089                         아버지 홍언필이 인기척에 깨 그 광경을 지켜봤다  사실형  긍정   \n",
       "\n",
       "      시제 확실성  \n",
       "0     현재  확실  \n",
       "1     현재  확실  \n",
       "2     과거  확실  \n",
       "3     현재  확실  \n",
       "4     과거  확실  \n",
       "...   ..  ..  \n",
       "7085  현재  확실  \n",
       "7086  현재  확실  \n",
       "7087  현재  확실  \n",
       "7088  미래  확실  \n",
       "7089  과거  확실  \n",
       "\n",
       "[7090 rows x 6 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "db41568b-37eb-4e1f-878b-ffa7df37e6f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>문장</th>\n",
       "      <th>유형</th>\n",
       "      <th>극성</th>\n",
       "      <th>시제</th>\n",
       "      <th>확실성</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_0000</td>\n",
       "      <td>장욱진의 가족은 허물 없는 가족애를 처음 공개되는 정약용의 정효자전과 정부인전은 강...</td>\n",
       "      <td>사실형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>현재</td>\n",
       "      <td>확실</td>\n",
       "      <td>사실형-긍정-현재-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_0001</td>\n",
       "      <td>조지 W 부시 버락 오바마 전 대통령도 전쟁 위험 때문에 버린 카드다</td>\n",
       "      <td>사실형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>현재</td>\n",
       "      <td>확실</td>\n",
       "      <td>사실형-긍정-현재-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_0002</td>\n",
       "      <td>지난해 1분기 128억원이었던 영업이익이 올해 1분기 505억원으로 급증했다</td>\n",
       "      <td>사실형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>과거</td>\n",
       "      <td>확실</td>\n",
       "      <td>사실형-긍정-과거-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_0003</td>\n",
       "      <td>수상 작가와 맺으려던 계약서 내용 가운데 일부가 독소 조항으로 해석돼 수정을 요청받...</td>\n",
       "      <td>사실형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>현재</td>\n",
       "      <td>확실</td>\n",
       "      <td>사실형-긍정-현재-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_0004</td>\n",
       "      <td>결국 최근 KDB산업은행은 대규모 손실 위기에 닥친 에어부산에 140억원 금융지원을...</td>\n",
       "      <td>사실형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>과거</td>\n",
       "      <td>확실</td>\n",
       "      <td>사실형-긍정-과거-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7085</th>\n",
       "      <td>TEST_7085</td>\n",
       "      <td>2020 세계국가편람 모바일 앱은 세계 216개국의 국가개황과 주요 경제지표 사회개...</td>\n",
       "      <td>사실형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>현재</td>\n",
       "      <td>확실</td>\n",
       "      <td>사실형-긍정-현재-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7086</th>\n",
       "      <td>TEST_7086</td>\n",
       "      <td>탈세계화 징후들이 반갑지 않은 이유다</td>\n",
       "      <td>추론형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>현재</td>\n",
       "      <td>확실</td>\n",
       "      <td>추론형-긍정-현재-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7087</th>\n",
       "      <td>TEST_7087</td>\n",
       "      <td>틱톡은 6월 인터넷 안전의 달을 맞아 올바른 개인정보 보호 관리 방법 앱 내 유용한...</td>\n",
       "      <td>사실형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>현재</td>\n",
       "      <td>확실</td>\n",
       "      <td>사실형-긍정-현재-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7088</th>\n",
       "      <td>TEST_7088</td>\n",
       "      <td>만약 3개월 간 채굴자들의 투표를 거쳐 23 이상의 해시파워가 채굴세 도입에 찬성한...</td>\n",
       "      <td>추론형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>미래</td>\n",
       "      <td>확실</td>\n",
       "      <td>추론형-긍정-미래-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7089</th>\n",
       "      <td>TEST_7089</td>\n",
       "      <td>아버지 홍언필이 인기척에 깨 그 광경을 지켜봤다</td>\n",
       "      <td>사실형</td>\n",
       "      <td>긍정</td>\n",
       "      <td>과거</td>\n",
       "      <td>확실</td>\n",
       "      <td>사실형-긍정-과거-확실</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7090 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID                                                 문장   유형  극성  \\\n",
       "0     TEST_0000  장욱진의 가족은 허물 없는 가족애를 처음 공개되는 정약용의 정효자전과 정부인전은 강...  사실형  긍정   \n",
       "1     TEST_0001             조지 W 부시 버락 오바마 전 대통령도 전쟁 위험 때문에 버린 카드다  사실형  긍정   \n",
       "2     TEST_0002         지난해 1분기 128억원이었던 영업이익이 올해 1분기 505억원으로 급증했다  사실형  긍정   \n",
       "3     TEST_0003  수상 작가와 맺으려던 계약서 내용 가운데 일부가 독소 조항으로 해석돼 수정을 요청받...  사실형  긍정   \n",
       "4     TEST_0004  결국 최근 KDB산업은행은 대규모 손실 위기에 닥친 에어부산에 140억원 금융지원을...  사실형  긍정   \n",
       "...         ...                                                ...  ...  ..   \n",
       "7085  TEST_7085  2020 세계국가편람 모바일 앱은 세계 216개국의 국가개황과 주요 경제지표 사회개...  사실형  긍정   \n",
       "7086  TEST_7086                               탈세계화 징후들이 반갑지 않은 이유다  추론형  긍정   \n",
       "7087  TEST_7087  틱톡은 6월 인터넷 안전의 달을 맞아 올바른 개인정보 보호 관리 방법 앱 내 유용한...  사실형  긍정   \n",
       "7088  TEST_7088  만약 3개월 간 채굴자들의 투표를 거쳐 23 이상의 해시파워가 채굴세 도입에 찬성한...  추론형  긍정   \n",
       "7089  TEST_7089                         아버지 홍언필이 인기척에 깨 그 광경을 지켜봤다  사실형  긍정   \n",
       "\n",
       "      시제 확실성         label  \n",
       "0     현재  확실  사실형-긍정-현재-확실  \n",
       "1     현재  확실  사실형-긍정-현재-확실  \n",
       "2     과거  확실  사실형-긍정-과거-확실  \n",
       "3     현재  확실  사실형-긍정-현재-확실  \n",
       "4     과거  확실  사실형-긍정-과거-확실  \n",
       "...   ..  ..           ...  \n",
       "7085  현재  확실  사실형-긍정-현재-확실  \n",
       "7086  현재  확실  추론형-긍정-현재-확실  \n",
       "7087  현재  확실  사실형-긍정-현재-확실  \n",
       "7088  미래  확실  추론형-긍정-미래-확실  \n",
       "7089  과거  확실  사실형-긍정-과거-확실  \n",
       "\n",
       "[7090 rows x 7 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['label'] = test['유형'] + '-' + test['극성'] + '-' + test['시제'] + '-' + test['확실성']\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d8a5530a-a3cb-4576-ba2c-87a39e813e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('sample_submission.csv')\n",
    "sub['label'] = test['label']\n",
    "tmp = 0\n",
    "while os.path.exists(f'제출{tmp}.csv'):\n",
    "    tmp += 1\n",
    "sub.to_csv(f'제출{tmp}.csv', index=False, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e80db0-04a0-4544-907c-ea37c92d079f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
